0 50 100 200 300 5 10 15 20 25 TV Sales 0 10 20 30 40 50 5 10 15 20 25 Radio Sales 0 20 40 60 80 100 5 10 15 20 25 Newspaper Sales 
Sales ≈ f(TV, Radio, Newspaper) 1 / 30 Notation Here Sales is a response or target that we wish to predict. 
We generically refer to the response as Y . 
TV is a feature, or input, or predictor; we name it X 1 . 
Likewise name Radio as X 2 , and so on. We can refer to the input vector collectively as X =    X 1 X 2 X 3    Now we write our model as Y = f(X) +  where  captures measurement errors and other discrepancies. 
2 / 30 What is f(X) good for? 
• With a good f we can make predictions of Y at new points X = x. •We can understand which components of X = (X 1 , X 2 , . 
. 
. 
, X p ) are important in explaining Y , and which are irrelevant. 
e.g. 
Seniority and Years of Education have a big impact on Income, but Marital Status typically does not. 
•Depending on the complexity of f, we may be able to understand how each component X j of X affects Y . 
3 / 30 ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ●●● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 1 2 3 4 5 6 7 −2 0 2 4 6 x y ● Is there an ideal f(X)? 
In particular, what is a good value for f(X) at any selected value of X, say X = 4? 
There can be many Y values at X = 4. A good value is f(4) = E(Y |X = 4) E(Y |X = 4) means expected value (average) of Y given X = 4. This ideal f(x) = E(Y |X = x) is called the regression function. 
4 / 30 The regression function f(x) •Is also defined for vector X; e.g. f(x) = f(x 1 , x 2 , x 3 ) = E(Y |X 1 = x 1 , X 2 = x 2 , X 3 = x 3 ) •Is the ideal or optimal predictor of Y with regard to mean-squared prediction error: f(x) = E(Y |X = x) is the function that minimizes E[(Y − g(X)) 2 |X = x] over all functions g at all points X = x. • = Y − f(x) is the irreducible error — i.e. even if we knew f(x), we would still make errors in prediction, since at each X = x there is typically a distribution of possible Y values. 
•For any estimate ˆ f(x) of f(x), we have E[(Y − ˆ f(X)) 2 |X = x] = [f(x) − ˆ f(x)] 2    Reducible + Var()    Irreducible 5 / 30 The regression function f(x) •Is also defined for vector X; e.g. f(x) = f(x 1 , x 2 , x 3 ) = E(Y |X 1 = x 1 , X 2 = x 2 , X 3 = x 3 ) •Is the ideal or optimal predictor of Y with regard to mean-squared prediction error: f(x) = E(Y |X = x) is the function that minimizes E[(Y − g(X)) 2 |X = x] over all functions g at all points X = x. • = Y − f(x) is the irreducible error — i.e. even if we knew f(x), we would still make errors in prediction, since at each X = x there is typically a distribution of possible Y values. 
•For any estimate ˆ f(x) of f(x), we have E[(Y − ˆ f(X)) 2 |X = x] = [f(x) − ˆ f(x)] 2    Reducible + Var()    Irreducible 5 / 30 The regression function f(x) •Is also defined for vector X; e.g. f(x) = f(x 1 , x 2 , x 3 ) = E(Y |X 1 = x 1 , X 2 = x 2 , X 3 = x 3 ) •Is the ideal or optimal predictor of Y with regard to mean-squared prediction error: f(x) = E(Y |X = x) is the function that minimizes E[(Y − g(X)) 2 |X = x] over all functions g at all points X = x. • = Y − f(x) is the irreducible error — i.e. even if we knew f(x), we would still make errors in prediction, since at each X = x there is typically a distribution of possible Y values. 
•For any estimate ˆ f(x) of f(x), we have E[(Y − ˆ f(X)) 2 |X = x] = [f(x) − ˆ f(x)] 2    Reducible + Var()    Irreducible 5 / 30 The regression function f(x) •Is also defined for vector X; e.g. f(x) = f(x 1 , x 2 , x 3 ) = E(Y |X 1 = x 1 , X 2 = x 2 , X 3 = x 3 ) •Is the ideal or optimal predictor of Y with regard to mean-squared prediction error: f(x) = E(Y |X = x) is the function that minimizes E[(Y − g(X)) 2 |X = x] over all functions g at all points X = x. • = Y − f(x) is the irreducible error — i.e. even if we knew f(x), we would still make errors in prediction, since at each X = x there is typically a distribution of possible Y values. 
• For any estimate ˆ f(x) of f(x), we have E[(Y − ˆ f(X)) 2 |X = x] = [f(x) − ˆ f(x)] 2    Reducible + Var()    Irreducible 5 / 30 How to estimate f •Typically we have few if any data points with X = 4 exactly. 
•So we cannot compute E(Y |X = x)! 
•Relax the definition and let ˆ f(x) = Ave(Y |X ∈ N(x)) where N(x) is some neighborhood of x. ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● 1 2 3 4 5 6 −2 −1 0 1 2 3 x y ● 6 / 30 •Nearest neighbor averaging can be pretty good for small p — i.e. p ≤ 4 and large-ish N . 
•We will discuss smoother versions, such as kernel and spline smoothing later in the course. 
•Nearest neighbor methods can be lousy when p is large. 
Reason: the curse of dimensionality. 
Nearest neighbors tend to be far away in high dimensions. 
•We need to get a reasonable fraction of the N values of y i to average to bring the variance down—e.g. 10%. 
•A 10% neighborhood in high dimensions need no longer be local, so we lose the spirit of estimating E(Y |X = x) by local averaging. 
7 / 30 •Nearest neighbor averaging can be pretty good for small p — i.e. p ≤ 4 and large-ish N . 
•We will discuss smoother versions, such as kernel and spline smoothing later in the course. 
•Nearest neighbor methods can be lousy when p is large. 
Reason: the curse of dimensionality. 
Nearest neighbors tend to be far away in high dimensions. 
•We need to get a reasonable fraction of the N values of y i to average to bring the variance down—e.g. 10%. 
• A 10% neighborhood in high dimensions need no longer be local, so we lose the spirit of estimating E(Y |X = x) by local averaging. 
7 / 30 The curse of dimensionality ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● −1.0 −0.5 0.0 0.5 1.0 −1.0 −0.5 0.0 0.5 1.0 x1 x2 10% Neighborhood ● 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.0 0.5 1.0 1.5 Fraction of Volume Radius p= 1 p= 2 p= 3 p= 5 p= 10 8 / 30 Parametric and structured models The linear model is an important example of a parametric model: f L (X) = β 0 + β 1 X 1 + β 2 X 2 + . 
. 
. 
β p X p . 
•A linear model is specified in terms of p + 1 parameters β 0 , β 1 , . 
. 
. 
, β p . 
• We estimate the parameters by fitting the model to training data. 
• Although it is almost never correct, a linear model often serves as a good and interpretable approximation to the unknown true function f(X). 
9 / 30 A linear model ˆ f L (X) = ˆ β 0 + ˆ β 1 X gives a reasonable fit here ● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 1 2 3 4 5 6 −2 −1 0 1 2 3 x y ● A quadratic model ˆ f Q (X) = ˆ β 0 + ˆ β 1 X + ˆ β 2 X 2 fits slightly better. 
● ● ● ● ● ● ● ● ● ● ● ● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 1 2 3 4 5 6 −2 −1 0 1 2 3 x y ● 10 / 30 Years of Education Seniority Income Simulated example. 
Red points are simulated values for income from the model income = f(education, seniority) +  f is the blue surface. 
11 / 30 Years of Education Seniority Income Linear regression model fit to the simulated data. 
ˆ f L (education, seniority) = ˆ β 0 + ˆ β 1 ×education+ ˆ β 2 ×seniority 12 / 30 Years of Education Seniority Income More flexible regression model ˆ f S (education, seniority) fit to the simulated data. 
Here we use a technique called a thin-plate spline to fit a flexible surface. 
We control the roughness of the fit (chapter 7). 13 / 30 Years of Education Seniority Income Even more flexible spline regression model ˆ f S (education, seniority) fit to the simulated data. 
Here the fitted model makes no errors on the training data! 
Also known as overfitting. 
14 / 30 Some trade-offs • Prediction accuracy versus interpretability. 
— Linear models are easy to interpret; thin-plate splines are not. 
•Good fit versus over-fit or under-fit. 
— How do we know when the fit is just right? 
•Parsimony versus black-box. 
— We often prefer a simpler model involving fewer variables over a black-box predictor involving them all. 
15 / 30 Some trade-offs • Prediction accuracy versus interpretability. 
— Linear models are easy to interpret; thin-plate splines are not. 
•Good fit versus over-fit or under-fit. 
— How do we know when the fit is just right? 
•Parsimony versus black-box. 
— We often prefer a simpler model involving fewer variables over a black-box predictor involving them all. 
15 / 30 Some trade-offs • Prediction accuracy versus interpretability. 
— Linear models are easy to interpret; thin-plate splines are not. 
•Good fit versus over-fit or under-fit. 
— How do we know when the fit is just right? 
• Parsimony versus black-box. 
— We often prefer a simpler model involving fewer variables over a black-box predictor involving them all. 
15 / 30 2.1 What Is Statistical Learning? 
25 Flexibility Interpretability Low High Low High Subset Selection Lasso Least Squares Generalized Additive Models Trees Bagging, Boosting Support Vector Machines FIGURE 2.7. 
A representation of the tradeoff between flexibility and interpretability, using different statistical learning methods. 
In general, as the flexibility of a method increases, its interpretability decreases. 
more interpreta ble . 
For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to under stand the relationship between Y and X 1 , X 2 , . 
. 
. 
, X p . 
In contrast, very flexible approa ches, such as the s plines disc ussed in Chapter 7 and displayed in Figures 2.5 and 2.6, and the boo sting methods discussed in Chapter 8, can lead to such complicated estimates of f that it is difficult to understand how any individual predictor is associated with the response. 
Figure 2.7 provides an illustration of the trade-off b e tween flexibility and interpretability for some o f the methods that we cover in this book. 
Lea st squares linear regression, discussed in Chapter 3, is relatively inflexible but is quite interpretable. 
The lasso, discussed in Chapter 6, relies upon the lasso linear model (2.4) but uses an alternative fitting pr ocedure fo r estimating the coefficients β 0 , β 1 , . 
. 
. 
, β p . 
The new procedure is more restrictive in estimating the coefficients, and sets a number of them to exactly zero. 
Hence in this sense the lasso is a less flexible approach than linear regression. 
It is also more interpreta ble than linear regression, b e cause in the final model the response variable will only be related to a small subset of the predictors — namely, those with nonzero coefficient estimates. 
Generalized additive models (GAMs), discusse d in C hapter 7, instead extend the lingeneralized additive model ear model (2.4) to allow for certain non-linear relationships. 
Co nsequently, GAMs are more flexible than linear regression. 
They are also somewhat less interpretable than linear regression, because the relationship between each predictor and the respo nse is now modeled using a curve. 
Finally, fully 16 / 30 Assessing Model Accuracy Suppose we fit a model ˆ f(x) to some training data Tr = {x i , y i } N 1 , and we wish to see how well it performs. 
•We could compute the average squared prediction error over Tr: MSE Tr = Ave i∈Tr [y i − ˆ f(x i )] 2 This may be biased toward more overfit models. 
•Instead we should, if possible, compute it using fresh test data Te = {x i , y i } M 1 : MSE Te = Ave i∈Te [y i − ˆ f(x i )] 2 17 / 30 2.2 Assessing Model Accuracy 31 0 20 40 60 80 100 2 4 6 8 10 12 X Y 2 5 10 20 0.0 0.5 1.0 1.5 2.0 2.5 Flexibility Mean Squared Error FIGURE 2.9. 
Left: Data simulated from f, shown in black. 
Three estimates of f are shown: the linear regression line (orange curve), and two smoothing spline fits (blue and green curves). 
Right: Traini ng MSE (grey curve), test MSE (red curve), and minimum possible test MSE over all methods (dashed line). 
Squares represent the training and test MSEs for the three fits shown in the left-hand panel. 
statistical methods specifically estimate coefficients so as to minimize the training set MSE. 
For these methods, the tra ining set MSE c an be quite small, but the test MSE is often much larger. 
Figure 2.9 illustrates this pheno menon on a simple example. 
In the lefthand panel of Figure 2.9, we have generated observations from (2.1) with the true f given by the black curve. 
The orange, blue and green curves illustrate three possible estimates for f obtained us ing methods with increasing levels of flexibility. 
The or ange line is the linear regression fit, which is relatively inflexible. 
The blue and green curves were produced using smoothing splines, discussed in Chapter 7, with different levels of smoothness. 
It is smo othing spline clear tha t as the level of flexibility increases, the curves fit the observed data more closely. 
The green curve is the most flexible and matches the data very well; however, we observe that it fits the true f (shown in black) poorly because it is too wiggly. 
By adjusting the level of flexibility of the smoothing spline fit, we can produce many different fits to this data. 
We now move on to the right-hand panel of Figure 2.9. 
The grey curve displays the average training MSE as a function of flexibility, or more formally the degrees of freedom, for a number of smoo thing splines. 
The dedegrees of freedom grees of freedom is a quantity that summarizes the flexibility of a curve; it is discussed more fully in Chapter 7. The orange, blue and gr een squares Black curve is truth. 
Red curve on right is MSE Te , grey curve is MSE Tr . 
Orange, blue and green curves/squares correspond to fits of different flexibility. 
18 / 30 2.2 Assessing Model Accuracy 33 0 20 40 60 80 100 2 4 6 8 10 12 X Y 2 5 10 20 0.0 0.5 1.0 1.5 2.0 2.5 Flexibility Mean Squared Error FIGURE 2.10. 
Details are as in Figure 2.9, using a different true f that is much closer to linear. 
In this setting, linear regression provides a very good fit to the data. 
0 20 40 60 80 100 −10 0 10 20 X Y 2 5 10 20 0 5 10 15 20 Flexibility Mean Squared Error FIGURE 2.11. 
Details are as in Figure 2.9, using a different f that is far from linear. 
In this setting, linear regression provides a very poor fit to the data. 
Here the truth is smoother, so the smoother fit and linear model do really well. 
19 / 30 2.2 Assessing Model Accuracy 33 0 20 40 60 80 100 2 4 6 8 10 12 X Y 2 5 10 20 0.0 0.5 1.0 1.5 2.0 2.5 Flexibility Mean Squared Error FIGURE 2.10. 
Details are as in Figure 2.9, using a different true f that is much closer to linear. 
In this setting, linear regression provides a very good fit to the data. 
0 20 40 60 80 100 −10 0 10 20 X Y 2 5 10 20 0 5 10 15 20 Flexibility Mean Squared Error FIGURE 2.11. 
Details are as in Figure 2.9, using a different f that is far from linear. 
In this setting, linear regression provides a very poor fit to the data. 
Here the truth is wiggly and the noise is low, so the more flexible fits do the best. 20 / 30 Bias-Variance Trade-off Suppose we have fit a model ˆ f(x) to some training data Tr, and let (x 0 , y 0 ) be a test observation drawn from the population. 
If the true model is Y = f(X) +  (with f(x) = E(Y |X = x)), then E  y 0 − ˆ f(x 0 )  2 = Var( ˆ f(x 0 )) + [Bias( ˆ f(x 0 ))] 2 + Var(). 
The expectation averages over the variability of y 0 as well as the variability in Tr. Note that Bias( ˆ f(x 0 ))] = E[ ˆ f(x 0 )] −f (x 0 ). Typically as the flexibility of ˆ f increases, its variance increases, and its bias decreases. 
So choosing the flexibility based on average test error amounts to a bias-variance trade-off. 
21 / 30 Bias-variance trade-off for the three examples 36 2. Statistical Learning 2 5 10 20 0.0 0.5 1.0 1.5 2.0 2.5 Flexibility 2 5 10 20 0.0 0.5 1.0 1.5 2.0 2.5 Flexibility 2 5 10 20 0 5 10 15 20 Flexibility MSE Bias Var FIGURE 2.12. 
Squared bias (blue curve), variance (orange curve), Var( ǫ) (dashed line), and test MSE (red curve) for the three data sets in Figures 2.9–2.11. 
The vertical dashed line indicates the flexibility level corresponding to the smallest test MSE. 
ibility increases, and the test MSE only declines slightly before increasing rapidly as the variance increases. 
Finally, in the right-hand panel of Figure 2.12, as flexibility increases, there is a dramatic decline in bias because the true f is very non-linear. 
There is also very little increase in variance as flexibility increases. 
Consequently, the test MSE declines substantially before experiencing a small increase as model flexibility increases. 
The relationship between bias, variance, and test set MSE given in Equation 2.7 and displayed in Figure 2 .12 is referred to as the bias-variance trade-off. 
Goo d test set performance of a statistical learning method re- bias-variance trade-off quires low variance as well as low squared bias. 
This is referred to a s a trade-off because it is easy to o btain a method w ith extremely low bias but high variance (for instance, by drawing a curve that passes through every single training obser vation) or a method with very low variance but high bias (by fitting a horizontal line to the data). 
The challenge lies in finding a method for which bo th the variance and the squared bia s are low. 
This trade-off is one of the most important recurring themes in this book. 
In a real-life situation in w hich f is uno bserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method. 
Nevertheless, one should always keep the bias-variance trade-off in mind. 
In this book we explore methods that are extremely flexible and hence can essentially eliminate bias. 
However, this does not guarantee that they will outperform a much simpler method such as linear regression. 
To take an extreme example, suppose that the true f is linear. 
In this situation linear regression will have no bias, making it very hard for a more flexible method to compete. 
In contrast, if the true f is highly non-linear and we have an ample number of training observations, then we may do better using a highly flexible approach, as in Figure 2.11. 
In 22 / 30 Classification Problems Here the response variable Y is qualitative — e.g. email is one of C = (spam, ham) (ham=good email), digit class is one of C = {0, 1, . 
. 
. 
, 9}. Our goals are to: •Build a classifier C(X) that assigns a class label from C to a future unlabeled observation X. •Assess the uncertainty in each classification •Understand the roles of the different predictors among X = (X 1 , X 2 , . 
. 
. 
, X p ). 23 / 30 | || | | | || | | || || | | | | || | | | | | | || || | | | | | | | | | || || | | | | | || | | || || | ||| | | | | || | ||| | ||| | | || || || ||| | | || ||| | | | | | | | | | | ||| | | | | | | || ||| | | | ||| | | ||| | | | | ||| || | | ||| | | | | | || || | | | || | | | | || | || | | || | | | | | || || | | | | | | ||| | || || | || | || || | | | || | || | | | | | || |||| | ||| | | | | || || | || | || || | | ||| | ||| | || || || | | | | | ||| | | || || || ||| | | | || || | | | | | | | || || || | | || ||| | | | || | | | || | | | | | | | || | | | | | | | || | | || | | | || ||| || | | | | || | || || || | ||| | || || | | || | | | || | || || | | | | | | || || || | || | | | || | | || | ||| | | | | | | | | || | || || | | || | | ||| | | | | | | || | | || | | | | | || || | | | | | || | | | | | | | | | | | | || | | | | | || | | | || | | | || | || | | | | | | | | | | || | | || || || | | | | | | | | | | | | | | || | | || | | | | | | | | | | | | | | || || | | || || || | || | | | | | | | || | | | | || || | | | | | ||| | | || | | | || | | | | ||| || | | | || | | | | || | || | | | | || | || ||| ||| | | | | || | | || | || || | |||| | || || | || | | | | | | | || | | || | |||| | | | | ||| || | || | | | || | | | || ||| | | || | || || | | || | | ||| || | | || | | | || | || | | || | | | | | | | || | | | | | || | |||| || ||| | | | | | || | | | ||| | | | || | | || | | | | |||||| | || || | | | || ||| | || | || | || ||| | | || | | | | | | | || | | | | | | | | | | | | | || | || ||| || || | | | | | | | | || | | | | || | | | | | | ||||| || | | || || || | | ||| | | | | || | | | | || | | | | | | | || | | | | || | || | || | | | | | || | | | | | | || | | | | | | | || | | | | ||| | | | || || | || | | | ||| | | || | || || | | || || || || | | || | | || | | | | | || | || | || | | | || || | | | | | | | | | || | || | | ||| | | || || | | | | | || | | || || | | | | || | | | | | ||| | | || | | || | | | ||| | | || | | | || | ||| | | | ||| | || || || | || | | | | || | | ||| | ||| | | | | ||| | | | | | | ||| ||| | | | | | | | | || | | | | | || | | | | | | | | | | | || | | || | || || | | | | | | | || | || | || | ||| || | | | | | | | || | | | | | | | | || || | | | | | | | | | | ||| || | | | | | | | | | | | || || || | | || ||| | | | | | | | | | || || | || | || | | | | | ||| || ||| || || | || | | | ||| | | || | | | | || | | | | | || | || || | | | | || | || | ||| | | | || | | | | | | | ||| | | | | | | | | | || ||| | || || | | | | | | || || | || | | | | ||| | | | || | | | | | |||| || || | | | || | | | || | | | | | || | | || | | | | | | || | || | || | | | || || | | || || |||| | | | | | | || | | | | | | | | || | | | || | | | | || | | | | | | | | | | | | | | || || | || | || || || | | | | | | || | | | || || | | | | | | | | || | || | | | || | | | | | || || | || | | || || | | || | | || | | | ||| | | | || | | || | | || | | | | || || | | | | | | | | || | ||| | || || || | | | | || | ||| || | || | | | | | | | ||| | | | ||| | || || | | || | || | || | || || | || || | || | || | | | || | | | | | | | | || | | | | || | || | | | || | ||| | || | | | | | | || | | | | || || | || | || | | | | | || | | | || ||| || | | | | || || | || | | | || | | | || | | | | | ||| | | | || || | | | | | || | ||| || | | | | || | | ||| | | | | | | | | | || | | | | | | || | || | || | | | || || | || | || ||| || || | || || | ||| | ||| | || | | | | ||| || | | | || | | | | || | | | | | | | || | | || | | || | | | | | | | | | | ||| | | | | | | || | | | | | | | | | || | || | || || || | | | | | || || | | | || | | 1 2 3 4 5 6 7 0.0 0.2 0.4 0.6 0.8 1.0 x y Is there an ideal C(X)? 
Suppose the K elements in C are numbered 1, 2, . 
. 
. 
, K. Let p k (x) = Pr(Y = k|X = x), k = 1, 2, . 
. 
. 
, K. These are the conditional class probabilities at x; e.g. see little barplot at x = 5. Then the Bayes optimal classifier at x is C(x) = j if p j (x) = max{p 1 (x), p 2 (x), . 
. 
. 
, p K (x)} 24 / 30 | || | | | | | || | | | | | | | || | | | || || | | || || | | | | | | | || | | || | || | | | | | | | | | ||| | | | | | | | ||| | | | | | | || || ||||| | | | | || ||| | | | | || 2 3 4 5 6 0.0 0.2 0.4 0.6 0.8 1.0 x y Nearest-neighbor averaging can be used as before. 
Also breaks down as dimension grows. 
However, the impact on ˆ C(x) is less than on ˆp k (x), k = 1, . 
. 
. 
, K. 25 / 30 Classification: some details •Typically we measure the performance of ˆ C(x) using the misclassification error rate: Err Te = Ave i∈Te I[y i = ˆ C(x i )] •The Bayes classifier (using the true p k (x)) has smallest error (in the population). 
• Support-vector machines build structured models for C(x). 
•We will also build structured models for representing the p k (x). 
e.g. 
Logistic regression, generalized additive models. 
26 / 30 Classification: some details •Typically we measure the performance of ˆ C(x) using the misclassification error rate: Err Te = Ave i∈Te I[y i = ˆ C(x i )] •The Bayes classifier (using the true p k (x)) has smallest error (in the population). 
•Support-vector machines build structured models for C(x). 
•We will also build structured models for representing the p k (x). 
e.g. 
Logistic regression, generalized additive models. 
26 / 30 Example: K-nearest neighbors in two dimensions 38 2. Statistical Learning o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o oo o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o o X 1 X 2 FIGURE 2.13. 
A simulated data set consisting of 100 observations in each of two groups, indicated in blue and in orange. 
The purple dashed line represents the Bayes decision boundary. 
The orange background grid indicates the region in which a test observation will be assigned to the orange class, and the blue background grid indicates the region in which a test observation will be assigned to the blue class. 
only two possible respo nse values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if Pr(Y = 1|X = x 0 ) > 0.5, and class two otherwis e. Figure 2.13 provides an example using a simulated data set in a twodimensional space consisting of predictors X 1 and X 2 . 
The orange and blue circles co rrespond to training observations that belong to two different classes . 
For each value of X 1 and X 2 , there is a different probability of the response being orange or blue. 
Since this is simulated data, we know how the data were generated and we can calculate the conditional pro babilities for each value of X1and X2. The orange shaded region reflects the set of points for which Pr(Y = orange|X) is greater than 50%, while the blue shaded reg ion indicates the set of points for which the probability is below 50%. 
The purple dashed line represents the points where the probability is exactly 50%. 
This is called the Bayes decision boundary. 
The Bayes Bayes decision boundaryclassifier’s prediction is determined by the Bayes decision boundary; an observation that falls on the orange side of the boundary will be assigne d to the orange class, and similarly an observation on the blue side of the boundary will be assigned to the blue class. 
The Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. 
Since the Bayes classifier will always choose the class Bayes error rate for which (2.10) is largest, the erro r rate at X = x0will be 1−maxjPr(Y = 27 / 30 2.2 Assessing Model Accuracy 41 o o o oo oo o o o o oo oo o o oo oo oooo o oo o oo oo oo o oo oo o oo o o oo o ooooo ooo o o o o oo oo oo ooo oo o oo o oo ooo ooooo ooo oooo ooo o o oo oo oo o oo o oo oo o oooo o o oo oo oo oo o oo o oooo o o oo oo oo o oo o oo oooo o oo ooo ooo o oooo o oooo o o ooo ooo ooo ooo oo oo o o o oo oooo ooKNN: K=10 X1 X2 FIGURE 2.15. 
The black curve indicates the KNN decision boundary on the data from Figure 2.13, using K = 10. The Bayes decision boundary is shown as a purple dashed line. 
The KNN and Bayes decision boundaries are very similar. 
o o o oo oo o o oo oo oo oo oo oo oooo o oo o oo oo oo o oo oo o oo o ooo o ooooo ooo o o o o oo oo oo ooo oo o oo o oo ooo ooooo ooo oooo ooo oo oo oo oo o oo o oo oo o oooo o o oooo oo oo o oo o oooo oo oo oo oo o oo o oo oooo o oo ooo ooo o oooo o oooo o o ooo ooo ooo ooo oo oo o oo oo oooo oo o o o oo oo o o oo oo oo oo oo oo oooo o oo o oo oo oo o oo oo o oo o ooo o ooooo ooo o o o o oo oo oo ooo oo o oo o oo ooo ooooo ooo oooo ooo oo oo oo oo o oo o oo oo o oooo o o oooo oo oo o oo o oooo oo oo oo oo o oo o oo oooo o oo ooo ooo o oooo o oooo o o ooo ooo ooo ooo oo oo o oo oo oooo ooKNN: K=1 KNN: K=100 FIGURE 2.16. 
A comparison of the KNN decision boundaries (solid black curves) obtained using K = 1 and K = 100 on the data from Figure 2.13. 
With K = 1, the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. 
T he Bayes decision boundary is shown as a purple dashed line. 
28 / 30 2.2 Assessing Model Accuracy 41 o o o oo oo o o o o oo oo o o oo oo oooo o oo o oo oo oo o oo oo o oo o o oo o ooooo ooo o o o o oo oo oo ooo oo o oo o oo ooo ooooo ooo oooo ooo o o oo oo oo o oo o oo oo o oooo o o oo oo oo oo o oo o oooo o o oo oo oo o oo o oo oooo o oo ooo ooo o oooo o oooo o o ooo ooo ooo ooo oo oo o o o oo oooo oo KNN: K=10 X1 X2 FIGURE 2.15. 
The black curve indicates the KNN decision boundary on the data from Figure 2.13, using K = 10. The Bayes decision boundary is shown as a purple dashed line. 
T he KNN and Bayes decision boundaries are very similar. 
o o o oo oo o o oo oo oo oo oo oo oooo o oo o oo oo oo o oo oo o oo o ooo o ooooo ooo o o o o oo oo oo ooo oo o oo o oo ooo ooooo ooo oooo ooo oo oo oo oo o oo o oo oo o oooo o o oooo oo oo o oo o oooo oo oo oo oo o oo o oo oooo o oo ooo ooo o oooo o oooo o o ooo ooo ooo ooo oo oo o oo oo oooo oo o o o oo oo o o oo oo oo oo oo oo oooo o oo o oo oo oo o oo oo o oo o ooo o ooooo ooo o o o o oo oo oo ooo oo o oo o oo ooo ooooo ooo oooo ooo oo oo oo oo o oo o oo oo o oooo o o oooo oo oo o oo o oooo oo oo oo oo o oo o oo oooo o oo ooo ooo o oooo o oooo o o ooo ooo ooo ooo oo oo o oo oo oooo ooKNN: K=1 KNN: K=100 FIGURE 2.16. 
A com parison of the KNN decision boundaries (solid black curves) obtained using K = 1 and K = 100 on the data from Figure 2.13. 
With K = 1, the decision boundary is overly flexible, while with K = 100 it is not sufficiently flexible. 
The Bayes decision boundary is shown as a purple dashed line.29 / 30 42 2. Statistical Learning 0.01 0.02 0.05 0.10 0.20 0.50 1.000.00 0.05 0.10 0.15 0.20 1/K Error Rate Training Errors Test Errors FIGURE 2.17. 
The KNN training error rate (blue, 200 observations) and test error rate (orange, 5000 observations) on the data from Figure 2.13, as the level of flexibility (assessed using 1/K) increases, or equivalently as the number of neighbors K decreases. 
The black dashed line indicates the Bayes error rate. 
The jumpiness of the curves is due to the small size of the training data set. 
In both the regression and c lassification settings, choosing the correct level of flexibility is c ritical to the success of any statistical learning method. 
The bias-variance tradeoff, a nd the resulting U-shape in the test error, can make this a difficult task. 
In Chapter 5, we re turn to this to pic and discuss various methods for estima ting test error r ates and thereby choosing the optimal level of flexibility for a given statistical learning method. 
2.3 Lab: Introductio n t o R In this lab, we will introduce some simple R commands. 
The best way to learn a new la nguage is to try out the commands. 
R can be downloaded from http://cran .r-project.org/ 2.3.1 Basic Commands R uses functions to perform operations. 
To run a function called funcname, function we type funcname(input1, input2), where the inputs (or arguments) input1 argument 30 / 30 