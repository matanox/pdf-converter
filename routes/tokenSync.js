// Generated by CoffeeScript 1.6.3
var css, extract, logging, riak, storage, util;

util = require('../util');

logging = require('../logging');

css = require('../css');

storage = require('../storage');

require('stream');

riak = require('riak-js').getClient({
  host: "localhost",
  port: "8098"
});

extract = require('./extract');

exports.go = function(req, res) {
  var docLogger, filtered, paragraphOpeningDelimitation, removed, t, tokenSequence, tokenSequenceSerialized, tokens, x, _i, _j, _len, _ref;
  util.timelog("handling client ajax request for " + req.session.name);
  if (Object.keys(req.body).length === 0) {
    if (req.query.regenerate != null) {
      docLogger = util.initDocLogger(baseFileName);
      generateFromHtml(req, req.session.name, res, docLogger);
    }
    if (req.session.tokenSequenceSerialized != null) {
      console.log('serving tokens from cache');
      console.log(req.session.tokenSequenceSerialized.length);
      console.log(typeof req.session.tokenSequenceSerialized);
      return res.end(req.session.tokenSequenceSerialized);
    } else {
      console.log('serving newly created tokens');
      tokens = req.session.tokens;
      tokenSequence = [];
      paragraphOpeningDelimitation = {
        metaType: 'paragraphBreak'
      };
      for (_i = 0, _len = tokens.length; _i < _len; _i++) {
        x = tokens[_i];
        if (x.metaType === 'regular') {
          if (x.paragraph === 'opener') {
            tokenSequence.push(paragraphOpeningDelimitation);
          }
        }
        tokenSequence.push(x);
      }
      util.timelog('pickling');
      tokenSequenceSerialized = JSON.stringify(tokenSequence);
      util.timelog('pickling');
      console.log("" + tokens.length + " tokens pickled into " + tokenSequenceSerialized.length + " long bytes stream");
      console.log("pickled size to tokens ratio: " + (parseFloat(tokenSequenceSerialized.length) / tokens.length));
      res.end(tokenSequenceSerialized);
      util.timelog("handling client ajax request for " + req.session.name);
      console.log('saving tokens to data store');
      storage.store('tokens', req.session.name, tokenSequenceSerialized, req.session.docLogger);
      return req.session.tokenSequenceSerialized = tokenSequenceSerialized;
    }
  } else {
    console.log("ajax request body length is " + req.body.length);
    tokenSequence = req.body;
    console.log('received updated tokens from client');
    util.timelog('saving updated tokens to data store');
    removed = 0;
    filtered = [];
    for (t = _j = 0, _ref = tokenSequence.length - 1; 0 <= _ref ? _j <= _ref : _j >= _ref; t = 0 <= _ref ? ++_j : --_j) {
      if (tokenSequence[t].remove) {
        removed += 1;
      } else {
        filtered.push(tokenSequence[t]);
      }
    }
    tokenSequence = filtered;
    console.log("removed " + removed + " tokens marked for removal by client request");
    util.timelog('pickling');
    tokenSequenceSerialized = JSON.stringify(tokenSequence);
    util.timelog('pickling');
    storage.store('tokens', req.session.name, tokenSequenceSerialized, req.session.docLogger);
    util.timelog('saving updated tokens to data store');
    req.session.tokenSequenceSerialized = tokenSequenceSerialized;
    req.session.tokens = tokenSequence;
    return res.end('success');
  }
};
