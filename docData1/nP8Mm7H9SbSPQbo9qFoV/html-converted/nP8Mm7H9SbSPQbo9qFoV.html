<!DOCTYPE html>
<!-- Created by pdf2htmlEX (https://github.com/coolwanglu/pdf2htmlex) -->
<html>
<head>
<meta charset="utf-8">
<meta name="generator" content="pdf2htmlEX">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<link rel="stylesheet" href="base.min.css"/>
<link rel="stylesheet" href="fancy.min.css"/>
<link rel="stylesheet" href="nP8Mm7H9SbSPQbo9qFoV.css"/>
<script src="compatibility.min.js"></script>
<script src="pdf2htmlEX.min.js"></script>
<script>
try{
  pdf2htmlEX.defaultViewer = new pdf2htmlEX.Viewer({});
}catch(e){}
</script>
<title></title>
</head>
<body>
<div id="sidebar">
<div id="outline">
<ul><li><a class="l" href="#pf1" data-dest-detail='[1,"XYZ",108,347.662,null]'>1 Introduction</a><ul><li><a class="l" href="#pf1" data-dest-detail='[1,"XYZ",108,119.42,null]'>1.1 Goals of the Paper</a></li><li><a class="l" href="#pf2" data-dest-detail='[2,"XYZ",108,493.132,null]'>1.2 Previous Work</a></li></ul></li><li><a class="l" href="#pf2" data-dest-detail='[2,"XYZ",108,255.053,null]'>2 Model Architectures</a><ul><li><a class="l" href="#pf3" data-dest-detail='[3,"XYZ",108,622.969,null]'>2.1 Feedforward Neural Net Language Model (NNLM)</a></li><li><a class="l" href="#pf3" data-dest-detail='[3,"XYZ",108,258.126,null]'>2.2 Recurrent Neural Net Language Model (RNNLM)</a></li><li><a class="l" href="#pf4" data-dest-detail='[4,"XYZ",108,710.138,null]'>2.3 Parallel Training of Neural Networks</a></li></ul></li><li><a class="l" href="#pf4" data-dest-detail='[4,"XYZ",108,594.147,null]'>3 New Log-linear Models</a><ul><li><a class="l" href="#pf4" data-dest-detail='[4,"XYZ",108,414.497,null]'>3.1 Continuous Bag-of-Words Model</a></li><li><a class="l" href="#pf4" data-dest-detail='[4,"XYZ",108,234.721,null]'>3.2 Continuous Skip-gram Model</a></li></ul></li><li><a class="l" href="#pf5" data-dest-detail='[5,"XYZ",108,347.441,null]'>4 Results</a><ul><li><a class="l" href="#pf6" data-dest-detail='[6,"XYZ",108,437.327,null]'>4.1 Task Description</a></li><li><a class="l" href="#pf6" data-dest-detail='[6,"XYZ",108,224.475,null]'>4.2 Maximization of Accuracy</a></li><li><a class="l" href="#pf7" data-dest-detail='[7,"XYZ",108,347.746,null]'>4.3 Comparison of Model Architectures</a></li><li><a class="l" href="#pf8" data-dest-detail='[8,"XYZ",108,130.948,null]'>4.4 Large Scale Parallel Training of Models</a></li><li><a class="l" href="#pf9" data-dest-detail='[9,"XYZ",108,372.12,null]'>4.5 Microsoft Research Sentence Completion Challenge</a></li></ul></li><li><a class="l" href="#pf9" data-dest-detail='[9,"XYZ",108,137.03,null]'>5 Examples of the Learned Relationships</a></li><li><a class="l" href="#pfa" data-dest-detail='[10,"XYZ",108,343.246,null]'>6 Conclusion</a></li><li><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,710.138,null]'>7 Follow-Up Work</a></li></ul></div>
</div>
<div id="page-container">
<div id="pf1" class="pf w0 h0" data-page-no="1"><div class="pc pc1 w0 h0"><img class="bi x0 y0 w1 h1" alt="" src="bg1.png"/><div class="t m0 x1 h2 y1 ff1 fs0 fc0 sc0 ls0 ws0">Efﬁcient Estimation of W<span class="_ _0"></span>ord Repr<span class="_ _1"></span>esentations in</div><div class="t m0 x2 h2 y2 ff1 fs0 fc0 sc0 ls0 ws0">V<span class="_ _0"></span>ector<span class="_ _2"> </span>Space</div><div class="t m0 x3 h3 y3 ff1 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>omas Mikolov</div><div class="t m0 x4 h3 y4 ff2 fs1 fc0 sc0 ls0 ws0">Google Inc., Mountain V<span class="_ _1"></span>ie<span class="_ _1"></span>w<span class="_ _3"></span>, CA</div><div class="t m0 x5 h4 y5 ff3 fs1 fc0 sc0 ls0 ws0">tmikolov@google.com</div><div class="t m0 x6 h3 y3 ff1 fs1 fc0 sc0 ls0 ws0">Kai Chen</div><div class="t m0 x7 h3 y4 ff2 fs1 fc0 sc0 ls0 ws0">Google Inc., Mountain V<span class="_ _1"></span>ie<span class="_ _1"></span>w<span class="_ _3"></span>, CA</div><div class="t m0 x8 h4 y5 ff3 fs1 fc0 sc0 ls0 ws0">kaichen@google.com</div><div class="t m0 x9 h3 y6 ff1 fs1 fc0 sc0 ls0 ws0">Greg Corrado</div><div class="t m0 x4 h3 y7 ff2 fs1 fc0 sc0 ls0 ws0">Google Inc., Mountain V<span class="_ _1"></span>ie<span class="_ _1"></span>w<span class="_ _3"></span>, CA</div><div class="t m0 x5 h4 y8 ff3 fs1 fc0 sc0 ls0 ws0">gcorrado@google.com</div><div class="t m0 xa h3 y6 ff1 fs1 fc0 sc0 ls0 ws0">Jeffr<span class="_ _1"></span>ey Dean</div><div class="t m0 x7 h3 y7 ff2 fs1 fc0 sc0 ls0 ws0">Google Inc., Mountain V<span class="_ _1"></span>ie<span class="_ _1"></span>w<span class="_ _3"></span>, CA</div><div class="t m0 xb h4 y8 ff3 fs1 fc0 sc0 ls0 ws0">jeff@google.com</div><div class="t m0 xc h5 y9 ff1 fs2 fc0 sc0 ls0 ws0">Abstract</div><div class="t m0 xd h3 ya ff2 fs1 fc0 sc0 ls0 ws0">W<span class="_ _3"></span>e<span class="_ _4"> </span>propose<span class="_ _4"> </span>two<span class="_ _4"> </span>no<span class="_ _1"></span>v<span class="_ _1"></span>el<span class="_ _4"> </span>model<span class="_ _4"> </span>architectures<span class="_ _4"> </span>for<span class="_ _4"> </span>computing<span class="_ _5"> </span>continuous<span class="_ _4"> </span>v<span class="_ _1"></span>ector<span class="_ _4"> </span>repre-</div><div class="t m0 xd h3 yb ff2 fs1 fc0 sc0 ls0 ws0">sentations of<span class="_ _4"> </span>words from<span class="_ _4"> </span>very<span class="_ _4"> </span>large<span class="_ _4"> </span>data sets.<span class="_ _6"> </span>The quality<span class="_ _4"> </span>of these representations</div><div class="t m0 xd h3 yc ff2 fs1 fc0 sc0 ls0 ws0">is<span class="_ _6"> </span>measured<span class="_ _7"> </span>in<span class="_ _6"> </span>a<span class="_ _7"> </span>word<span class="_ _6"> </span>similarity<span class="_ _7"> </span>task,<span class="_ _6"> </span>and<span class="_ _7"> </span>the<span class="_ _6"> </span>results<span class="_ _7"> </span>are<span class="_ _6"> </span>compared<span class="_ _7"> </span>to<span class="_ _6"> </span>the<span class="_ _7"> </span>previ-</div><div class="t m0 xd h3 yd ff2 fs1 fc0 sc0 ls0 ws0">ously best<span class="_ _4"> </span>performing techniques<span class="_ _4"> </span>based on<span class="_ _4"> </span>dif<span class="_ _1"></span>ferent types<span class="_ _4"> </span>of neural<span class="_ _4"> </span>networks.<span class="_ _6"> </span>W<span class="_ _3"></span>e</div><div class="t m0 xd h3 ye ff2 fs1 fc0 sc0 ls0 ws0">observe lar<span class="_ _1"></span>ge improvements in accuracy at much lower computational cost, i.e.<span class="_ _7"> </span>it</div><div class="t m0 xd h3 yf ff2 fs1 fc0 sc0 ls0 ws0">takes<span class="_ _6"> </span>less<span class="_ _6"> </span>than<span class="_ _6"> </span>a<span class="_ _7"> </span>day<span class="_ _6"> </span>to<span class="_ _6"> </span>learn<span class="_ _7"> </span>high<span class="_ _6"> </span>quality<span class="_ _6"> </span>word<span class="_ _6"> </span>vectors<span class="_ _6"> </span>from<span class="_ _6"> </span>a<span class="_ _7"> </span>1.6<span class="_ _6"> </span>billion<span class="_ _6"> </span>words</div><div class="t m0 xd h3 y10 ff2 fs1 fc0 sc0 ls0 ws0">data set.<span class="_ _2"> </span>Furthermore,<span class="_ _6"> </span>we<span class="_ _6"> </span>sho<span class="_ _1"></span>w<span class="_ _6"> </span>that these<span class="_ _6"> </span>vectors provide state-of-the-art<span class="_ _6"> </span>perfor-</div><div class="t m0 xd h3 y11 ff2 fs1 fc0 sc0 ls0 ws0">mance on our test set for measuring syntactic and semantic word similarities.</div><div class="t m0 x0 h5 y12 ff1 fs2 fc0 sc0 ls0 ws0">1<span class="_ _8"> </span>Introduction</div><div class="t m0 x0 h3 y13 ff2 fs1 fc0 sc0 ls0 ws0">Many<span class="_ _4"> </span>current<span class="_ _4"> </span>NLP<span class="_ _4"> </span>systems<span class="_ _5"> </span>and techniques<span class="_ _4"> </span>treat<span class="_ _4"> </span>w<span class="_ _1"></span>ords<span class="_ _4"> </span>as<span class="_ _4"> </span>atomic<span class="_ _4"> </span>units<span class="_ _4"> </span>-<span class="_ _5"> </span>there is<span class="_ _4"> </span>no<span class="_ _4"> </span>notion<span class="_ _4"> </span>of<span class="_ _5"> </span>similar-</div><div class="t m0 x0 h3 y14 ff2 fs1 fc0 sc0 ls0 ws0">ity between words,<span class="_ _4"> </span>as these are represented as<span class="_ _4"> </span>indices in a v<span class="_ _1"></span>ocabulary<span class="_ _3"></span>.<span class="_ _7"> </span>This choice has<span class="_ _4"> </span>sev<span class="_ _1"></span>eral good</div><div class="t m0 x0 h3 y15 ff2 fs1 fc0 sc0 ls0 ws0">reasons -<span class="_ _6"> </span>simplicity<span class="_ _3"></span>,<span class="_ _6"> </span>robustness and the<span class="_ _6"> </span>observ<span class="_ _1"></span>ation that<span class="_ _6"> </span>simple models<span class="_ _6"> </span>trained on huge<span class="_ _6"> </span>amounts of</div><div class="t m0 x0 h3 y16 ff2 fs1 fc0 sc0 ls0 ws0">data<span class="_ _7"> </span>outperform<span class="_ _7"> </span>comple<span class="_ _1"></span>x<span class="_ _7"> </span>systems<span class="_ _7"> </span>trained<span class="_ _6"> </span>on<span class="_ _7"> </span>less<span class="_ _7"> </span>data.<span class="_ _9"> </span>An<span class="_ _7"> </span>example<span class="_ _6"> </span>is<span class="_ _7"> </span>the<span class="_ _7"> </span>popular<span class="_ _7"> </span>N-gram<span class="_ _6"> </span>model</div><div class="t m0 x0 h3 y17 ff2 fs1 fc0 sc0 ls0 ws0">used<span class="_ _4"> </span>for<span class="_ _4"> </span>statistical<span class="_ _5"> </span>language<span class="_ _4"> </span>modeling<span class="_ _4"> </span>-<span class="_ _4"> </span>today<span class="_ _3"></span>,<span class="_ _4"> </span>it<span class="_ _4"> </span>is<span class="_ _5"> </span>possible<span class="_ _4"> </span>to<span class="_ _4"> </span>train<span class="_ _4"> </span>N-grams<span class="_ _5"> </span>on<span class="_ _4"> </span>virtually<span class="_ _4"> </span>all<span class="_ _4"> </span>a<span class="_ _1"></span>v<span class="_ _1"></span>ailable</div><div class="t m0 x0 h3 y18 ff2 fs1 fc0 sc0 ls0 ws0">data (trillions of words [3]).</div><div class="t m0 x0 h3 y19 ff2 fs1 fc0 sc0 ls0 ws0">Howe<span class="_ _1"></span>v<span class="_ _1"></span>er<span class="_ _1"></span>,<span class="_ _a"> </span>the<span class="_ _2"> </span>simple<span class="_ _2"> </span>techniques<span class="_ _a"> </span>are<span class="_ _2"> </span>at<span class="_ _2"> </span>their<span class="_ _a"> </span>limits<span class="_ _2"> </span>in<span class="_ _2"> </span>many<span class="_ _2"> </span>tasks.<span class="_ _b"> </span>For<span class="_ _2"> </span>example,<span class="_ _a"> </span>the<span class="_ _2"> </span>amount<span class="_ _2"> </span>of</div><div class="t m0 x0 h3 y1a ff2 fs1 fc0 sc0 ls0 ws0">relev<span class="_ _1"></span>ant<span class="_ _7"> </span>in-domain<span class="_ _2"> </span>data<span class="_ _2"> </span>for<span class="_ _7"> </span>automatic<span class="_ _2"> </span>speech<span class="_ _2"> </span>recognition<span class="_ _7"> </span>is<span class="_ _2"> </span>limited<span class="_ _2"> </span>-<span class="_ _7"> </span>the<span class="_ _2"> </span>performance<span class="_ _7"> </span>is<span class="_ _2"> </span>usually</div><div class="t m0 x0 h3 y1b ff2 fs1 fc0 sc0 ls0 ws0">dominated<span class="_ _2"> </span>by<span class="_ _7"> </span>the<span class="_ _2"> </span>size<span class="_ _2"> </span>of<span class="_ _7"> </span>high<span class="_ _2"> </span>quality<span class="_ _7"> </span>transcribed<span class="_ _2"> </span>speech<span class="_ _2"> </span>data<span class="_ _7"> </span>(often<span class="_ _2"> </span>just<span class="_ _7"> </span>millions<span class="_ _2"> </span>of<span class="_ _2"> </span>w<span class="_ _1"></span>ords).<span class="_ _c"> </span>In</div><div class="t m0 x0 h3 y1c ff2 fs1 fc0 sc0 ls0 ws0">machine<span class="_ _6"> </span>translation,<span class="_ _7"> </span>the<span class="_ _6"> </span>existing<span class="_ _6"> </span>corpora<span class="_ _6"> </span>for<span class="_ _7"> </span>many languages<span class="_ _7"> </span>contain<span class="_ _6"> </span>only<span class="_ _6"> </span>a<span class="_ _7"> </span>fe<span class="_ _1"></span>w<span class="_ _6"> </span>billions<span class="_ _7"> </span>of<span class="_ _6"> </span>words</div><div class="t m0 x0 h3 y1d ff2 fs1 fc0 sc0 ls0 ws0">or less.<span class="_ _2"> </span>Thus,<span class="_ _6"> </span>there are<span class="_ _6"> </span>situations where<span class="_ _6"> </span>simple scaling<span class="_ _6"> </span>up of<span class="_ _6"> </span>the<span class="_ _6"> </span>basic techniques<span class="_ _6"> </span>will not<span class="_ _6"> </span>result in</div><div class="t m0 x0 h3 y1e ff2 fs1 fc0 sc0 ls0 ws0">any signiﬁcant progress, and we ha<span class="_ _1"></span>ve to focus on more adv<span class="_ _1"></span>anced techniques.</div><div class="t m0 x0 h3 y1f ff2 fs1 fc0 sc0 ls0 ws0">W<span class="_ _1"></span>ith progress<span class="_ _6"> </span>of machine<span class="_ _6"> </span>learning<span class="_ _6"> </span>techniques in<span class="_ _6"> </span>recent<span class="_ _6"> </span>years, it<span class="_ _6"> </span>has<span class="_ _6"> </span>become possible<span class="_ _6"> </span>to<span class="_ _6"> </span>train more</div><div class="t m0 x0 h3 y20 ff2 fs1 fc0 sc0 ls0 ws0">complex<span class="_ _4"> </span>models<span class="_ _4"> </span>on<span class="_ _4"> </span>much<span class="_ _5"> </span>larger<span class="_ _4"> </span>data<span class="_ _4"> </span>set,<span class="_ _4"> </span>and<span class="_ _4"> </span>the<span class="_ _1"></span>y<span class="_ _4"> </span>typically<span class="_ _4"> </span>outperform<span class="_ _4"> </span>the<span class="_ _4"> </span>simple<span class="_ _4"> </span>models. Probably</div><div class="t m0 x0 h3 y21 ff2 fs1 fc0 sc0 ls0 ws0">the most successful concept is to use distributed representations of words [10].<span class="_ _6"> </span>For example, neural</div><div class="t m0 x0 h3 y22 ff2 fs1 fc0 sc0 ls0 ws0">network based language models signiﬁcantly outperform N-gram models [1, 27, 17].</div><div class="t m0 x0 h3 y23 ff1 fs1 fc0 sc0 ls0 ws0">1.1<span class="_ _d"> </span>Goals of the Paper</div><div class="t m0 x0 h3 y24 ff2 fs1 fc0 sc0 ls0 ws0">The<span class="_ _4"> </span>main<span class="_ _4"> </span>goal<span class="_ _5"> </span>of<span class="_ _4"> </span>this<span class="_ _4"> </span>paper<span class="_ _5"> </span>is<span class="_ _4"> </span>to<span class="_ _4"> </span>introduce<span class="_ _5"> </span>techniques<span class="_ _4"> </span>that<span class="_ _4"> </span>can<span class="_ _4"> </span>be<span class="_ _5"> </span>used<span class="_ _4"> </span>for<span class="_ _4"> </span>learning<span class="_ _5"> </span>high-quality<span class="_ _4"> </span>word</div><div class="t m0 x0 h3 y25 ff2 fs1 fc0 sc0 ls0 ws0">vectors<span class="_ _4"> </span>from huge<span class="_ _4"> </span>data sets<span class="_ _4"> </span>with billions<span class="_ _4"> </span>of w<span class="_ _1"></span>ords, and<span class="_ _4"> </span>with millions<span class="_ _4"> </span>of words<span class="_ _4"> </span>in the<span class="_ _4"> </span>v<span class="_ _1"></span>ocabulary<span class="_ _3"></span>.<span class="_ _7"> </span>As</div><div class="t m0 x0 h3 y26 ff2 fs1 fc0 sc0 ls0 ws0">far<span class="_ _4"> </span>as<span class="_ _4"> </span>we<span class="_ _4"> </span>know<span class="_ _3"></span>, none<span class="_ _4"> </span>of<span class="_ _4"> </span>the<span class="_ _4"> </span>pre<span class="_ _1"></span>viously<span class="_ _4"> </span>proposed<span class="_ _4"> </span>architectures has<span class="_ _4"> </span>been<span class="_ _4"> </span>successfully<span class="_ _4"> </span>trained<span class="_ _4"> </span>on<span class="_ _4"> </span>more</div><div class="t m0 xe h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">1</div><div class="t m1 xf h6 y28 ff4 fs3 fc1 sc0 ls0 ws0">arXiv:1301.3781v3  [cs.CL]  7 Sep 2013</div><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,531.365,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:203.250000px;bottom:249.709000px;width:5.974000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,314.025,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:403.878000px;bottom:139.124000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,585.091,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:403.577000px;bottom:128.305000px;width:5.974000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,466.886,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:413.539000px;bottom:128.230000px;width:10.956000px;height:7.862000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,104.915,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:428.483000px;bottom:128.230000px;width:10.956000px;height:7.862000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf2" class="pf w0 h0" data-page-no="2"><div class="pc pc2 w0 h0"><img class="bi x0 y29 w2 h7" alt="" src="bg2.png"/><div class="t m0 x0 h3 y2a ff2 fs1 fc0 sc0 ls0 ws0">than a fe<span class="_ _1"></span>w hundred<span class="_ _4"> </span>of millions of<span class="_ _4"> </span>words, with<span class="_ _4"> </span>a modest dimensionality<span class="_ _4"> </span>of the w<span class="_ _1"></span>ord v<span class="_ _1"></span>ectors between</div><div class="t m0 x0 h3 y2b ff2 fs1 fc0 sc0 ls0 ws0">50 - 100.</div><div class="t m0 x0 h3 y2c ff2 fs1 fc0 sc0 ls0 ws0">W<span class="_ _3"></span>e<span class="_ _7"> </span>use<span class="_ _7"> </span>recently<span class="_ _7"> </span>proposed<span class="_ _6"> </span>techniques<span class="_ _7"> </span>for<span class="_ _7"> </span>measuring<span class="_ _7"> </span>the<span class="_ _7"> </span>quality<span class="_ _7"> </span>of<span class="_ _6"> </span>the<span class="_ _7"> </span>resulting<span class="_ _7"> </span>vector<span class="_ _7"> </span>representa-</div><div class="t m0 x0 h3 y2d ff2 fs1 fc0 sc0 ls0 ws0">tions,<span class="_ _7"> </span>with<span class="_ _6"> </span>the<span class="_ _7"> </span>expectation<span class="_ _6"> </span>that<span class="_ _6"> </span>not<span class="_ _7"> </span>only<span class="_ _6"> </span>will<span class="_ _7"> </span>similar<span class="_ _6"> </span>words<span class="_ _6"> </span>tend<span class="_ _7"> </span>to<span class="_ _6"> </span>be<span class="_ _7"> </span>close<span class="_ _6"> </span>to<span class="_ _7"> </span>each<span class="_ _6"> </span>other<span class="_ _1"></span>,<span class="_ _7"> </span>but<span class="_ _6"> </span>that</div><div class="t m0 x0 h3 y2e ff2 fs1 fc0 sc0 ls0 ws0">words can<span class="_ _6"> </span>have <span class="ff1">multiple<span class="_ _6"> </span>degrees<span class="_ _6"> </span>of<span class="_ _6"> </span>similarity<span class="_ _6"> </span></span>[20].<span class="_ _2"> </span>This<span class="_ _6"> </span>has<span class="_ _6"> </span>been<span class="_ _6"> </span>observed earlier<span class="_ _6"> </span>in<span class="_ _6"> </span>the<span class="_ _6"> </span>context</div><div class="t m0 x0 h3 y2f ff2 fs1 fc0 sc0 ls0 ws0">of inﬂectional languages - for example, nouns can ha<span class="_ _1"></span>v<span class="_ _1"></span>e multiple word endings, and if we search for</div><div class="t m0 x0 h3 y30 ff2 fs1 fc0 sc0 ls0 ws0">similar w<span class="_ _1"></span>ords in<span class="_ _4"> </span>a subspace<span class="_ _4"> </span>of<span class="_ _4"> </span>the original<span class="_ _4"> </span>vector<span class="_ _4"> </span>space, it<span class="_ _4"> </span>is possible<span class="_ _4"> </span>to<span class="_ _4"> </span>ﬁnd w<span class="_ _1"></span>ords that<span class="_ _4"> </span>hav<span class="_ _1"></span>e similar</div><div class="t m0 x0 h3 y31 ff2 fs1 fc0 sc0 ls0 ws0">endings [13, 14].</div><div class="t m0 x0 h3 y32 ff2 fs1 fc0 sc0 ls0 ws0">Somewhat<span class="_ _7"> </span>surprisingly<span class="_ _1"></span>,<span class="_ _2"> </span>it<span class="_ _2"> </span>w<span class="_ _1"></span>as<span class="_ _2"> </span>found<span class="_ _7"> </span>that<span class="_ _2"> </span>similarity<span class="_ _2"> </span>of<span class="_ _2"> </span>w<span class="_ _1"></span>ord<span class="_ _2"> </span>representations<span class="_ _7"> </span>goes<span class="_ _2"> </span>beyond<span class="_ _7"> </span>simple</div><div class="t m0 x0 h3 y33 ff2 fs1 fc0 sc0 ls0 ws0">syntactic<span class="_ _2"> </span>re<span class="_ _1"></span>gularities.<span class="_ _c"> </span>Using<span class="_ _2"> </span>a<span class="_ _2"> </span>w<span class="_ _1"></span>ord<span class="_ _2"> </span>of<span class="_ _1"></span>fset<span class="_ _7"> </span>technique<span class="_ _2"> </span>where<span class="_ _2"> </span>simple<span class="_ _7"> </span>algebraic<span class="_ _2"> </span>operations<span class="_ _2"> </span>are<span class="_ _7"> </span>per-</div><div class="t m0 x0 h3 y4 ff2 fs1 fc0 sc0 ls0 ws0">formed on the word v<span class="_ _1"></span>ectors, it was sho<span class="_ _1"></span>wn for example that <span class="ff5">vector(”King”) - vector(”Man”) + vec-</span></div><div class="t m0 x0 h3 y34 ff5 fs1 fc0 sc0 ls0 ws0">tor(”W<span class="_ _3"></span>oman”) <span class="ff2">results in<span class="_ _4"> </span>a vector<span class="_ _4"> </span>that is closest to<span class="_ _4"> </span>the v<span class="_ _1"></span>ector representation of the<span class="_ _4"> </span>word <span class="ff5">Queen<span class="_ _4"> </span></span>[20].</span></div><div class="t m0 x0 h3 y35 ff2 fs1 fc0 sc0 ls0 ws0">In<span class="_ _2"> </span>this<span class="_ _7"> </span>paper<span class="_ _1"></span>,<span class="_ _2"> </span>we<span class="_ _2"> </span>try<span class="_ _7"> </span>to<span class="_ _2"> </span>maximize<span class="_ _7"> </span>accuracy<span class="_ _2"> </span>of<span class="_ _7"> </span>these<span class="_ _2"> </span>v<span class="_ _1"></span>ector<span class="_ _7"> </span>operations<span class="_ _2"> </span>by<span class="_ _2"> </span>de<span class="_ _1"></span>v<span class="_ _1"></span>eloping<span class="_ _7"> </span>new<span class="_ _2"> </span>model</div><div class="t m0 x0 h3 y36 ff2 fs1 fc0 sc0 ls0 ws0">architectures<span class="_ _4"> </span>that preserv<span class="_ _1"></span>e<span class="_ _4"> </span>the<span class="_ _4"> </span>linear<span class="_ _4"> </span>regularities<span class="_ _4"> </span>among<span class="_ _4"> </span>words.<span class="_ _6"> </span>W<span class="_ _3"></span>e design<span class="_ _4"> </span>a<span class="_ _4"> </span>ne<span class="_ _1"></span>w<span class="_ _4"> </span>comprehensi<span class="_ _1"></span>ve<span class="_ _4"> </span>test</div><div class="t m0 x0 h3 y37 ff2 fs1 fc0 sc0 ls0 ws0">set<span class="_ _7"> </span>for<span class="_ _7"> </span>measuring<span class="_ _7"> </span>both<span class="_ _7"> </span>syntactic<span class="_ _7"> </span>and<span class="_ _7"> </span>semantic<span class="_ _2"> </span>re<span class="_ _1"></span>gularities</div><div class="t m0 x8 h8 y38 ff2 fs4 fc0 sc0 ls0 ws0">1</div><div class="t m0 x10 h3 y37 ff2 fs1 fc0 sc0 ls0 ws0">,<span class="_ _7"> </span>and<span class="_ _2"> </span>sho<span class="_ _1"></span>w<span class="_ _6"> </span>that<span class="_ _7"> </span>many<span class="_ _7"> </span>such<span class="_ _7"> </span>regularities</div><div class="t m0 x0 h3 y39 ff2 fs1 fc0 sc0 ls0 ws0">can<span class="_ _6"> </span>be<span class="_ _6"> </span>learned<span class="_ _6"> </span>with<span class="_ _6"> </span>high<span class="_ _6"> </span>accuracy<span class="_ _3"></span>.<span class="_ _a"> </span>Moreover<span class="_ _1"></span>,<span class="_ _6"> </span>we<span class="_ _6"> </span>discuss<span class="_ _6"> </span>ho<span class="_ _1"></span>w<span class="_ _6"> </span>training<span class="_ _6"> </span>time<span class="_ _6"> </span>and<span class="_ _6"> </span>accuracy<span class="_ _6"> </span>depends</div><div class="t m0 x0 h3 y8 ff2 fs1 fc0 sc0 ls0 ws0">on the dimensionality of the word vectors and on the amount of the training data.</div><div class="t m0 x0 h3 y3a ff1 fs1 fc0 sc0 ls0 ws0">1.2<span class="_ _d"> </span>Pre<span class="_ _1"></span>vious W<span class="_ _3"></span>ork</div><div class="t m0 x0 h3 y3b ff2 fs1 fc0 sc0 ls0 ws0">Representation of words as continuous vectors has a long history [10, 26, 8].<span class="_ _7"> </span>A very popular model</div><div class="t m0 x0 h3 y3c ff2 fs1 fc0 sc0 ls0 ws0">architecture<span class="_ _6"> </span>for<span class="_ _7"> </span>estimating<span class="_ _6"> </span>neural<span class="_ _6"> </span>network<span class="_ _6"> </span>language<span class="_ _7"> </span>model<span class="_ _6"> </span>(NNLM)<span class="_ _7"> </span>was<span class="_ _6"> </span>proposed<span class="_ _6"> </span>in<span class="_ _6"> </span>[1],<span class="_ _7"> </span>where<span class="_ _7"> </span>a</div><div class="t m0 x0 h3 y3d ff2 fs1 fc0 sc0 ls0 ws0">feedforward neural netw<span class="_ _1"></span>ork with a linear projection layer<span class="_ _4"> </span>and a non-linear hidden layer w<span class="_ _1"></span>as used to</div><div class="t m0 x0 h3 y3e ff2 fs1 fc0 sc0 ls0 ws0">learn<span class="_ _7"> </span>jointly<span class="_ _7"> </span>the<span class="_ _6"> </span>word<span class="_ _7"> </span>vector<span class="_ _6"> </span>representation<span class="_ _7"> </span>and<span class="_ _7"> </span>a<span class="_ _7"> </span>statistical<span class="_ _7"> </span>language<span class="_ _7"> </span>model.<span class="_ _9"> </span>This<span class="_ _6"> </span>work<span class="_ _7"> </span>has<span class="_ _7"> </span>been</div><div class="t m0 x0 h3 y3f ff2 fs1 fc0 sc0 ls0 ws0">followed by man<span class="_ _1"></span>y others.</div><div class="t m0 x0 h3 y40 ff2 fs1 fc0 sc0 ls0 ws0">Another<span class="_ _7"> </span>interesting<span class="_ _7"> </span>architecture<span class="_ _7"> </span>of<span class="_ _7"> </span>NNLM<span class="_ _2"> </span>w<span class="_ _1"></span>as<span class="_ _7"> </span>presented<span class="_ _7"> </span>in<span class="_ _7"> </span>[13,<span class="_ _7"> </span>14],<span class="_ _2"> </span>where<span class="_ _6"> </span>the<span class="_ _7"> </span>word<span class="_ _7"> </span>vectors<span class="_ _7"> </span>are</div><div class="t m0 x0 h3 y41 ff2 fs1 fc0 sc0 ls0 ws0">ﬁrst learned<span class="_ _4"> </span>using<span class="_ _4"> </span>neural<span class="_ _4"> </span>network<span class="_ _4"> </span>with<span class="_ _4"> </span>a single<span class="_ _4"> </span>hidden<span class="_ _4"> </span>layer<span class="_ _1"></span>. The word<span class="_ _4"> </span>vectors<span class="_ _4"> </span>are<span class="_ _4"> </span>then<span class="_ _4"> </span>used<span class="_ _4"> </span>to train</div><div class="t m0 x0 h3 y42 ff2 fs1 fc0 sc0 ls0 ws0">the<span class="_ _7"> </span>NNLM.<span class="_ _7"> </span>Thus,<span class="_ _7"> </span>the<span class="_ _7"> </span>word<span class="_ _7"> </span>vectors<span class="_ _6"> </span>are<span class="_ _7"> </span>learned<span class="_ _7"> </span>even<span class="_ _6"> </span>without<span class="_ _7"> </span>constructing<span class="_ _7"> </span>the<span class="_ _7"> </span>full<span class="_ _7"> </span>NNLM.<span class="_ _7"> </span>In<span class="_ _7"> </span>this</div><div class="t m0 x0 h3 y43 ff2 fs1 fc0 sc0 ls0 ws0">work,<span class="_ _4"> </span>we directly<span class="_ _4"> </span>extend<span class="_ _4"> </span>this<span class="_ _4"> </span>architecture, and<span class="_ _4"> </span>focus<span class="_ _4"> </span>just on<span class="_ _4"> </span>the<span class="_ _4"> </span>ﬁrst step<span class="_ _4"> </span>where<span class="_ _4"> </span>the w<span class="_ _1"></span>ord v<span class="_ _1"></span>ectors are</div><div class="t m0 x0 h3 y44 ff2 fs1 fc0 sc0 ls0 ws0">learned using a simple model.</div><div class="t m0 x0 h3 y45 ff2 fs1 fc0 sc0 ls0 ws0">It<span class="_ _7"> </span>was<span class="_ _7"> </span>later<span class="_ _2"> </span>sho<span class="_ _1"></span>wn<span class="_ _7"> </span>that<span class="_ _7"> </span>the<span class="_ _7"> </span>word<span class="_ _7"> </span>vectors<span class="_ _7"> </span>can<span class="_ _2"> </span>be<span class="_ _6"> </span>used<span class="_ _2"> </span>to<span class="_ _6"> </span>signiﬁcantly<span class="_ _2"> </span>impro<span class="_ _1"></span>ve<span class="_ _7"> </span>and<span class="_ _7"> </span>simplify<span class="_ _7"> </span>many</div><div class="t m0 x0 h3 y46 ff2 fs1 fc0 sc0 ls0 ws0">NLP<span class="_ _7"> </span>applications<span class="_ _2"> </span>[4,<span class="_ _6"> </span>5,<span class="_ _2"> </span>29].<span class="_ _e"> </span>Estimation<span class="_ _7"> </span>of<span class="_ _2"> </span>the<span class="_ _6"> </span>word<span class="_ _2"> </span>v<span class="_ _1"></span>ectors<span class="_ _7"> </span>itself<span class="_ _7"> </span>was<span class="_ _7"> </span>performed<span class="_ _2"> </span>using<span class="_ _7"> </span>dif<span class="_ _1"></span>ferent</div><div class="t m0 x0 h3 y47 ff2 fs1 fc0 sc0 ls0 ws0">model architectures<span class="_ _4"> </span>and<span class="_ _4"> </span>trained<span class="_ _4"> </span>on v<span class="_ _1"></span>arious<span class="_ _4"> </span>corpora<span class="_ _4"> </span>[4, 29,<span class="_ _4"> </span>23,<span class="_ _4"> </span>19,<span class="_ _4"> </span>9], and<span class="_ _4"> </span>some<span class="_ _4"> </span>of the<span class="_ _4"> </span>resulting<span class="_ _4"> </span>word</div><div class="t m0 x0 h3 y48 ff2 fs1 fc0 sc0 ls0 ws0">vectors<span class="_ _4"> </span>were<span class="_ _4"> </span>made<span class="_ _4"> </span>a<span class="_ _1"></span>v<span class="_ _1"></span>ailable<span class="_ _4"> </span>for<span class="_ _4"> </span>future<span class="_ _4"> </span>research<span class="_ _4"> </span>and<span class="_ _4"> </span>comparison</div><div class="t m0 x11 h8 y13 ff2 fs4 fc0 sc0 ls0 ws0">2</div><div class="t m0 x12 h3 y48 ff2 fs1 fc0 sc0 ls0 ws0">.<span class="_ _6"> </span>Howe<span class="_ _1"></span>ver<span class="_ _1"></span>,<span class="_ _4"> </span>as<span class="_ _4"> </span>f<span class="_ _1"></span>ar<span class="_ _4"> </span>as<span class="_ _4"> </span>we<span class="_ _4"> </span>kno<span class="_ _1"></span>w<span class="_ _1"></span>,<span class="_ _4"> </span>these</div><div class="t m0 x0 h3 y49 ff2 fs1 fc0 sc0 ls0 ws0">architectures were<span class="_ _6"> </span>signiﬁcantly more<span class="_ _6"> </span>computationally expensi<span class="_ _1"></span>ve for training<span class="_ _6"> </span>than the one<span class="_ _6"> </span>proposed</div><div class="t m0 x0 h3 y4a ff2 fs1 fc0 sc0 ls0 ws0">in [13],<span class="_ _6"> </span>with the exception of<span class="_ _6"> </span>certain version of<span class="_ _6"> </span>log-bilinear model where<span class="_ _6"> </span>diagonal weight<span class="_ _6"> </span>matrices</div><div class="t m0 x0 h3 y4b ff2 fs1 fc0 sc0 ls0 ws0">are used [23].</div><div class="t m0 x0 h5 y4c ff1 fs2 fc0 sc0 ls0 ws0">2<span class="_ _8"> </span>Model Architectur<span class="_ _1"></span>es</div><div class="t m0 x0 h3 y4d ff2 fs1 fc0 sc0 ls0 ws0">Many different types<span class="_ _6"> </span>of models<span class="_ _6"> </span>were<span class="_ _6"> </span>proposed for<span class="_ _6"> </span>estimating continuous<span class="_ _6"> </span>representations<span class="_ _6"> </span>of words,</div><div class="t m0 x0 h3 y4e ff2 fs1 fc0 sc0 ls0 ws0">including the<span class="_ _6"> </span>well-known Latent Semantic<span class="_ _6"> </span>Analysis (LSA)<span class="_ _6"> </span>and Latent<span class="_ _6"> </span>Dirichlet Allocation<span class="_ _6"> </span>(LD<span class="_ _1"></span>A).</div><div class="t m0 x0 h3 y4f ff2 fs1 fc0 sc0 ls0 ws0">In this paper<span class="_ _1"></span>,<span class="_ _4"> </span>we focus<span class="_ _4"> </span>on distrib<span class="_ _1"></span>uted representations of w<span class="_ _1"></span>ords learned by<span class="_ _4"> </span>neural networks,<span class="_ _4"> </span>as it w<span class="_ _1"></span>as</div><div class="t m0 x0 h3 y50 ff2 fs1 fc0 sc0 ls0 ws0">previously shown<span class="_ _6"> </span>that<span class="_ _6"> </span>they<span class="_ _6"> </span>perform<span class="_ _6"> </span>signiﬁcantly<span class="_ _7"> </span>better<span class="_ _6"> </span>than<span class="_ _6"> </span>LSA<span class="_ _7"> </span>for<span class="_ _6"> </span>preserving<span class="_ _6"> </span>linear<span class="_ _6"> </span>regularities</div><div class="t m0 x0 h3 y51 ff2 fs1 fc0 sc0 ls0 ws0">among words [20, 31]; LD<span class="_ _1"></span>A moreo<span class="_ _1"></span>ver becomes computationally v<span class="_ _1"></span>ery expensi<span class="_ _1"></span>v<span class="_ _1"></span>e on large data sets.</div><div class="t m0 x0 h3 y52 ff2 fs1 fc0 sc0 ls0 ws0">Similar to<span class="_ _4"> </span>[18],<span class="_ _4"> </span>to compare<span class="_ _4"> </span>dif<span class="_ _1"></span>ferent model<span class="_ _4"> </span>architectures<span class="_ _4"> </span>we deﬁne<span class="_ _4"> </span>ﬁrst<span class="_ _4"> </span>the computational<span class="_ _4"> </span>comple<span class="_ _1"></span>x-</div><div class="t m0 x0 h3 y53 ff2 fs1 fc0 sc0 ls0 ws0">ity of<span class="_ _6"> </span>a<span class="_ _6"> </span>model<span class="_ _6"> </span>as the<span class="_ _6"> </span>number<span class="_ _6"> </span>of parameters<span class="_ _6"> </span>that<span class="_ _6"> </span>need to<span class="_ _6"> </span>be<span class="_ _6"> </span>accessed to<span class="_ _6"> </span>fully<span class="_ _6"> </span>train the<span class="_ _6"> </span>model.<span class="_ _2"> </span>Next,</div><div class="t m0 x0 h3 y54 ff2 fs1 fc0 sc0 ls0 ws0">we will try to maximize the accuracy<span class="_ _3"></span>, while minimizing the computational complexity<span class="_ _3"></span>.</div><div class="t m0 x13 h9 y55 ff2 fs5 fc0 sc0 ls0 ws0">1</div><div class="t m0 x14 ha y56 ff2 fs6 fc0 sc0 ls0 ws0">The test set is av<span class="_ _1"></span>ailable at <span class="ff3">www.fit.vutbr.cz/</span></div><div class="t m0 x15 ha y57 ff3 fs6 fc0 sc0 ls0 ws0">˜</div><div class="t m0 x16 ha y56 ff3 fs6 fc0 sc0 ls0 ws0">imikolov/rnnlm/word-<span class="_ _f"></span>test.v1.txt</div><div class="t m0 x13 h9 y58 ff2 fs5 fc0 sc0 ls0 ws0">2</div><div class="t m0 x14 ha y59 ff3 fs6 fc0 sc0 ls0 ws0">http://ronan.collobert.com/senna/</div><div class="t m0 x0 ha y5a ff3 fs6 fc0 sc0 ls0 ws0">http://metaoptimize.com/projects/wordreprs/</div><div class="t m0 x0 ha y25 ff3 fs6 fc0 sc0 ls0 ws0">http://www.fit.vutbr.cz/</div><div class="t m0 x17 ha y5b ff3 fs6 fc0 sc0 ls0 ws0">˜</div><div class="t m0 x18 ha y25 ff3 fs6 fc0 sc0 ls0 ws0">imikolov/rnnlm/</div><div class="t m0 x0 ha y26 ff3 fs6 fc0 sc0 ls0 ws0">http://ai.stanford.edu/</div><div class="t m0 x19 ha y5c ff3 fs6 fc0 sc0 ls0 ws0">˜</div><div class="t m0 x17 ha y26 ff3 fs6 fc0 sc0 ls0 ws0">ehhuang/</div><div class="t m0 xe h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">2</div><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,648.206,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:302.714000px;bottom:648.727000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,213.374,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:143.305000px;bottom:615.850000px;width:10.956000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,187.13,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:158.249000px;bottom:615.989000px;width:10.956000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,648.206,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:486.733000px;bottom:566.037000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf2" data-dest-detail='[2,"XYZ",124.139,112.863,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:341.184000px;bottom:525.165000px;width:4.977000px;height:11.521000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,314.025,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:371.000000px;bottom:454.777000px;width:10.956000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,494.806,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:385.966000px;bottom:454.777000px;width:10.956000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,366.514,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:400.932000px;bottom:454.777000px;width:5.974000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,585.091,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:456.841000px;bottom:443.958000px;width:5.974000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,213.374,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:356.734000px;bottom:394.005000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,187.13,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:372.467000px;bottom:394.144000px;width:10.955000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,493.409,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:183.574000px;bottom:322.413000px;width:5.974000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,456.958,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:194.439000px;bottom:322.274000px;width:5.974000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,415.08,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:205.304000px;bottom:322.194000px;width:10.955000px;height:8.006000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,493.409,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:313.024000px;bottom:311.455000px;width:5.974000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,415.08,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:322.764000px;bottom:311.235000px;width:10.955000px;height:8.007000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,572.514,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:337.486000px;bottom:311.315000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,674.108,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:352.207000px;bottom:311.235000px;width:10.955000px;height:8.007000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,351.228,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:366.929000px;bottom:311.235000px;width:5.973000px;height:8.007000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf2" data-dest-detail='[2,"XYZ",124.139,101.989,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:358.495000px;bottom:298.339000px;width:4.978000px;height:11.521000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,213.374,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:120.205000px;bottom:278.438000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,572.514,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:145.228000px;bottom:267.480000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,648.206,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:166.170000px;bottom:165.424000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,363.274,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:181.114000px;bottom:165.424000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,710.138,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:151.488000px;bottom:148.488000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt"><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,255);position:absolute;left:216.869000px;bottom:98.416000px;width:265.102000px;height:11.867000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="http://ronan.collobert.com/senna/"><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,255);position:absolute;left:122.643000px;bottom:89.122000px;width:178.527000px;height:10.286000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="http://metaoptimize.com/projects/wordreprs/"><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,255);position:absolute;left:106.504000px;bottom:79.160000px;width:232.324000px;height:8.344000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="http://www.fit.vutbr.cz/~imikolov/rnnlm/"><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,255);position:absolute;left:106.504000px;bottom:68.886000px;width:216.185000px;height:8.656000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="http://ai.stanford.edu/~ehhuang/"><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,255);position:absolute;left:106.504000px;bottom:57.653000px;width:173.146000px;height:9.926000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf3" class="pf w0 h0" data-page-no="3"><div class="pc pc3 w0 h0"><img class="bi x1a y5d w3 h7" alt="" src="bg3.png"/><div class="t m0 x0 h3 y2a ff2 fs1 fc0 sc0 ls0 ws0">For all the follo<span class="_ _1"></span>wing models, the training complexity is proportional to</div><div class="t m0 x1b hb y5e ff6 fs1 fc0 sc0 ls0 ws0">O<span class="_ _6"> </span><span class="ff7">=<span class="_ _6"> </span></span>E<span class="_ _6"> </span><span class="ff8">×<span class="_ _4"> </span></span>T<span class="_ _2"> </span><span class="ff8">×<span class="_ _4"> </span></span>Q,<span class="_ _10"> </span><span class="ff2">(1)</span></div><div class="t m0 x0 hc y5f ff2 fs1 fc0 sc0 ls0 ws0">where <span class="ff6">E<span class="_ _7"> </span></span>is number of the training epochs, <span class="ff6">T<span class="_ _2"> </span></span>is the number of the<span class="_ _6"> </span>words in the training set and <span class="ff6">Q<span class="_ _6"> </span></span>is</div><div class="t m0 x0 hb y60 ff2 fs1 fc0 sc0 ls0 ws0">deﬁned further<span class="_ _4"> </span>for each model<span class="_ _4"> </span>architecture.<span class="_ _6"> </span>Common choice is<span class="_ _4"> </span><span class="ff6">E<span class="_ _7"> </span><span class="ff7">=<span class="_ _6"> </span>3<span class="_ _5"> </span><span class="ff8">−<span class="_ _4"> </span></span>50<span class="_ _4"> </span></span></span>and<span class="_ _4"> </span><span class="ff6">T<span class="_ _2"> </span></span>up<span class="_ _4"> </span>to one billion.</div><div class="t m0 x0 h3 y61 ff2 fs1 fc0 sc0 ls0 ws0">All models are trained using stochastic gradient descent and backpropagation [26].</div><div class="t m0 x0 h3 y62 ff1 fs1 fc0 sc0 ls0 ws0">2.1<span class="_ _d"> </span>Feedf<span class="_ _1"></span>orward Neural Net Language Model (NNLM)</div><div class="t m0 x0 h3 y63 ff2 fs1 fc0 sc0 ls0 ws0">The probabilistic<span class="_ _6"> </span>feedforward neural<span class="_ _6"> </span>network language model<span class="_ _6"> </span>has been<span class="_ _6"> </span>proposed in<span class="_ _6"> </span>[1].<span class="_ _2"> </span>It consists</div><div class="t m0 x0 hc y64 ff2 fs1 fc0 sc0 ls0 ws0">of<span class="_ _7"> </span>input,<span class="_ _7"> </span>projection,<span class="_ _2"> </span>hidden<span class="_ _6"> </span>and<span class="_ _7"> </span>output<span class="_ _7"> </span>layers.<span class="_ _e"> </span>At<span class="_ _7"> </span>the<span class="_ _7"> </span>input<span class="_ _7"> </span>layer<span class="_ _1"></span>,<span class="_ _7"> </span><span class="ff6">N<span class="_ _a"> </span></span>previous<span class="_ _7"> </span>words<span class="_ _6"> </span>are<span class="_ _7"> </span>encoded</div><div class="t m0 x0 hc y5 ff2 fs1 fc0 sc0 ls0 ws0">using<span class="_ _2"> </span>1-of-<span class="ff6">V<span class="_ _11"> </span></span>coding,<span class="_ _2"> </span>where<span class="_ _2"> </span><span class="ff6">V<span class="_ _11"> </span></span>is<span class="_ _2"> </span>size<span class="_ _2"> </span>of<span class="_ _2"> </span>the<span class="_ _2"> </span>v<span class="_ _1"></span>ocab<span class="_ _1"></span>ulary<span class="_ _3"></span>.<span class="_ _12"> </span>The<span class="_ _2"> </span>input<span class="_ _2"> </span>layer<span class="_ _2"> </span>is<span class="_ _7"> </span>then<span class="_ _2"> </span>projected<span class="_ _2"> </span>to<span class="_ _2"> </span>a</div><div class="t m0 x0 hc y65 ff2 fs1 fc0 sc0 ls0 ws0">projection<span class="_ _7"> </span>layer<span class="_ _7"> </span><span class="ff6">P<span class="_ _9"> </span></span>that<span class="_ _6"> </span>has<span class="_ _7"> </span>dimensionality<span class="_ _7"> </span><span class="ff6">N<span class="_ _2"> </span><span class="ff8">×<span class="_ _6"> </span></span>D<span class="_ _f"></span></span>,<span class="_ _7"> </span>using<span class="_ _7"> </span>a<span class="_ _7"> </span>shared<span class="_ _7"> </span>projection<span class="_ _7"> </span>matrix.<span class="_ _e"> </span>As<span class="_ _7"> </span>only<span class="_ _7"> </span><span class="ff6">N</span></div><div class="t m0 x0 h3 y66 ff2 fs1 fc0 sc0 ls0 ws0">inputs<span class="_ _4"> </span>are<span class="_ _5"> </span>activ<span class="_ _1"></span>e<span class="_ _4"> </span>at<span class="_ _5"> </span>any<span class="_ _4"> </span>gi<span class="_ _1"></span>v<span class="_ _1"></span>en<span class="_ _4"> </span>time,<span class="_ _5"> </span>composition<span class="_ _4"> </span>of<span class="_ _5"> </span>the<span class="_ _4"> </span>projection<span class="_ _4"> </span>layer<span class="_ _5"> </span>is<span class="_ _4"> </span>a<span class="_ _5"> </span>relativ<span class="_ _1"></span>ely<span class="_ _4"> </span>cheap<span class="_ _5"> </span>operation.</div><div class="t m0 x0 h3 y67 ff2 fs1 fc0 sc0 ls0 ws0">The<span class="_ _6"> </span>NNLM<span class="_ _6"> </span>architecture<span class="_ _6"> </span>becomes<span class="_ _6"> </span>complex for<span class="_ _6"> </span>computation<span class="_ _6"> </span>between<span class="_ _6"> </span>the<span class="_ _7"> </span>projection and<span class="_ _6"> </span>the<span class="_ _7"> </span>hidden</div><div class="t m0 x0 hb y68 ff2 fs1 fc0 sc0 ls0 ws0">layer<span class="_ _1"></span>,<span class="_ _6"> </span>as values<span class="_ _6"> </span>in the<span class="_ _6"> </span>projection<span class="_ _6"> </span>layer<span class="_ _6"> </span>are<span class="_ _6"> </span>dense.<span class="_ _2"> </span>For a<span class="_ _6"> </span>common<span class="_ _6"> </span>choice<span class="_ _6"> </span>of<span class="_ _6"> </span><span class="ff6">N<span class="_ _a"> </span><span class="ff7">=<span class="_ _2"> </span>10<span class="_ _1"></span><span class="ff2">,<span class="_ _6"> </span>the size<span class="_ _6"> </span>of<span class="_ _6"> </span>the</span></span></span></div><div class="t m0 x0 hc y69 ff2 fs1 fc0 sc0 ls0 ws0">projection<span class="_ _6"> </span>layer<span class="_ _6"> </span>(<span class="ff6">P<span class="_ _5"> </span></span>)<span class="_ _6"> </span>might<span class="_ _7"> </span>be<span class="_ _6"> </span>500<span class="_ _6"> </span>to<span class="_ _6"> </span>2000,<span class="_ _7"> </span>while<span class="_ _6"> </span>the<span class="_ _7"> </span>hidden<span class="_ _6"> </span>layer<span class="_ _6"> </span>size<span class="_ _7"> </span><span class="ff6">H<span class="_ _2"> </span></span>is typically<span class="_ _7"> </span>500<span class="_ _6"> </span>to<span class="_ _6"> </span>1000</div><div class="t m0 x0 h3 y6a ff2 fs1 fc0 sc0 ls0 ws0">units.<span class="_ _6"> </span>Moreover<span class="_ _1"></span>,<span class="_ _4"> </span>the<span class="_ _5"> </span>hidden<span class="_ _4"> </span>layer<span class="_ _4"> </span>is<span class="_ _4"> </span>used<span class="_ _5"> </span>to<span class="_ _4"> </span>compute<span class="_ _4"> </span>probability<span class="_ _4"> </span>distrib<span class="_ _1"></span>ution<span class="_ _4"> </span>o<span class="_ _1"></span>v<span class="_ _1"></span>er<span class="_ _4"> </span>all<span class="_ _4"> </span>the<span class="_ _5"> </span>words<span class="_ _4"> </span>in<span class="_ _4"> </span>the</div><div class="t m0 x0 hc y6b ff2 fs1 fc0 sc0 ls0 ws0">vocab<span class="_ _1"></span>ulary<span class="_ _3"></span>, resulting in an<span class="_ _4"> </span>output layer with<span class="_ _4"> </span>dimensionality <span class="ff6">V<span class="_ _4"> </span></span>.<span class="_ _6"> </span>Thus, the computational<span class="_ _4"> </span>complexity</div><div class="t m0 x0 h3 y6c ff2 fs1 fc0 sc0 ls0 ws0">per each training example is</div><div class="t m0 x1c hb y6d ff6 fs1 fc0 sc0 ls0 ws0">Q<span class="_ _6"> </span><span class="ff7">=<span class="_ _6"> </span></span>N<span class="_ _7"> </span><span class="ff8">×<span class="_ _4"> </span></span>D<span class="_ _4"> </span><span class="ff7">+<span class="_ _6"> </span></span>N<span class="_ _6"> </span><span class="ff8">×<span class="_ _4"> </span></span>D<span class="_ _6"> </span><span class="ff8">×<span class="_ _4"> </span></span>H<span class="_ _6"> </span><span class="ff7">+<span class="_ _4"> </span></span>H<span class="_ _6"> </span><span class="ff8">×<span class="_ _4"> </span></span>V<span class="_ _f"></span>,<span class="_ _13"> </span><span class="ff2">(2)</span></div><div class="t m0 x0 hc yb ff2 fs1 fc0 sc0 ls0 ws0">where<span class="_ _2"> </span>the<span class="_ _2"> </span>dominating<span class="_ _a"> </span>term<span class="_ _2"> </span>is<span class="_ _2"> </span><span class="ff6">H<span class="_ _a"> </span><span class="ff8">×<span class="_ _7"> </span></span>V<span class="_ _4"> </span></span>.<span class="_ _b"> </span>Howe<span class="_ _1"></span>v<span class="_ _1"></span>er<span class="_ _1"></span>,<span class="_ _a"> </span>sev<span class="_ _1"></span>eral<span class="_ _2"> </span>practical<span class="_ _2"> </span>solutions<span class="_ _2"> </span>were<span class="_ _a"> </span>proposed<span class="_ _2"> </span>for</div><div class="t m0 x0 h3 yc ff2 fs1 fc0 sc0 ls0 ws0">av<span class="_ _1"></span>oiding<span class="_ _6"> </span>it;<span class="_ _2"> </span>either<span class="_ _6"> </span>using<span class="_ _7"> </span>hierarchical<span class="_ _7"> </span>versions<span class="_ _6"> </span>of<span class="_ _7"> </span>the<span class="_ _7"> </span>softmax<span class="_ _7"> </span>[25,<span class="_ _7"> </span>23,<span class="_ _6"> </span>18],<span class="_ _2"> </span>or<span class="_ _6"> </span>av<span class="_ _1"></span>oiding<span class="_ _6"> </span>normalized</div><div class="t m0 x0 h3 yd ff2 fs1 fc0 sc0 ls0 ws0">models completely by<span class="_ _6"> </span>using models that are<span class="_ _6"> </span>not normalized during training [4,<span class="_ _6"> </span>9].<span class="_ _7"> </span>W<span class="_ _1"></span>ith binary tree</div><div class="t m0 x0 h3 ye ff2 fs1 fc0 sc0 ls0 ws0">representations of the v<span class="_ _1"></span>ocabulary<span class="_ _3"></span>, the number of output units that need to be e<span class="_ _1"></span>v<span class="_ _1"></span>aluated can go do<span class="_ _1"></span>wn</div><div class="t m0 x0 hc yf ff2 fs1 fc0 sc0 ls0 ws0">to around <span class="ff6">log</span></div><div class="t m0 x1d h8 y41 ff9 fs4 fc0 sc0 ls0 ws0">2</div><div class="t m0 x1e hb yf ff7 fs1 fc0 sc0 ls0 ws0">(<span class="ff6">V<span class="_ _4"> </span></span>)<span class="ff2">.<span class="_ _7"> </span>Thus, most of the comple<span class="_ _1"></span>xity is caused by the term <span class="ff6">N<span class="_ _7"> </span><span class="ff8">×<span class="_ _4"> </span></span>D<span class="_ _6"> </span><span class="ff8">×<span class="_ _4"> </span></span>H<span class="_ _f"></span></span>.</span></div><div class="t m0 x0 h3 y6e ff2 fs1 fc0 sc0 ls0 ws0">In<span class="_ _4"> </span>our<span class="_ _4"> </span>models,<span class="_ _4"> </span>we<span class="_ _4"> </span>use<span class="_ _4"> </span>hierarchical<span class="_ _4"> </span>softmax<span class="_ _4"> </span>where<span class="_ _4"> </span>the<span class="_ _4"> </span>v<span class="_ _1"></span>ocab<span class="_ _1"></span>ulary<span class="_ _4"> </span>is<span class="_ _4"> </span>represented<span class="_ _4"> </span>as<span class="_ _4"> </span>a<span class="_ _4"> </span>Huf<span class="_ _1"></span>fman<span class="_ _4"> </span>binary</div><div class="t m0 x0 h3 y6f ff2 fs1 fc0 sc0 ls0 ws0">tree.<span class="_ _6"> </span>This<span class="_ _4"> </span>follo<span class="_ _1"></span>ws<span class="_ _4"> </span>pre<span class="_ _1"></span>vious<span class="_ _4"> </span>observ<span class="_ _1"></span>ations<span class="_ _4"> </span>that<span class="_ _5"> </span>the<span class="_ _4"> </span>frequency<span class="_ _5"> </span>of w<span class="_ _1"></span>ords<span class="_ _4"> </span>w<span class="_ _1"></span>orks<span class="_ _4"> </span>well<span class="_ _4"> </span>for<span class="_ _5"> </span>obtaining<span class="_ _4"> </span>classes</div><div class="t m0 x0 h3 y70 ff2 fs1 fc0 sc0 ls0 ws0">in neural net language models [16].<span class="_ _6"> </span>Huffman trees assign short binary codes to frequent w<span class="_ _1"></span>ords, and</div><div class="t m0 x0 h3 y71 ff2 fs1 fc0 sc0 ls0 ws0">this further reduces<span class="_ _4"> </span>the number of output<span class="_ _4"> </span>units that need<span class="_ _4"> </span>to be e<span class="_ _1"></span>v<span class="_ _1"></span>aluated:<span class="_ _6"> </span>while balanced binary tree</div><div class="t m0 x0 hc y72 ff2 fs1 fc0 sc0 ls0 ws0">would<span class="_ _4"> </span>require<span class="_ _5"> </span><span class="ff6">l<span class="_ _f"></span>og</span></div><div class="t m0 x1f h8 y46 ff9 fs4 fc0 sc0 ls0 ws0">2</div><div class="t m0 x20 hb y72 ff7 fs1 fc0 sc0 ls0 ws0">(<span class="ff6">V<span class="_ _4"> </span></span>)<span class="_ _4"> </span><span class="ff2">outputs<span class="_ _4"> </span>to<span class="_ _5"> </span>be<span class="_ _4"> </span>ev<span class="_ _1"></span>aluated,<span class="_ _4"> </span>the<span class="_ _5"> </span>Huffman<span class="_ _4"> </span>tree<span class="_ _4"> </span>based<span class="_ _5"> </span>hierarchical<span class="_ _4"> </span>softmax<span class="_ _4"> </span>requires</span></div><div class="t m0 x0 hc y5d ff2 fs1 fc0 sc0 ls0 ws0">only<span class="_ _6"> </span>about<span class="_ _6"> </span><span class="ff6">log</span></div><div class="t m0 x1e h8 y47 ff9 fs4 fc0 sc0 ls0 ws0">2</div><div class="t m0 x21 hb y5d ff7 fs1 fc0 sc0 ls0 ws0">(<span class="ff6">U<span class="_ _5"></span>nigram<span class="_ _2"> </span>perpl<span class="_ _f"></span>exity</span>(<span class="ff6">V<span class="_ _4"> </span></span>))<span class="ff2">.<span class="_ _a"> </span>For<span class="_ _6"> </span>example when<span class="_ _6"> </span>the<span class="_ _6"> </span>vocabulary size<span class="_ _6"> </span>is<span class="_ _6"> </span>one<span class="_ _6"> </span>million</span></div><div class="t m0 x0 h3 y73 ff2 fs1 fc0 sc0 ls0 ws0">words,<span class="_ _6"> </span>this<span class="_ _7"> </span>results<span class="_ _6"> </span>in<span class="_ _6"> </span>about<span class="_ _6"> </span>two<span class="_ _6"> </span>times<span class="_ _7"> </span>speedup<span class="_ _6"> </span>in<span class="_ _6"> </span>ev<span class="_ _1"></span>aluation.<span class="_ _14"> </span>While<span class="_ _6"> </span>this<span class="_ _6"> </span>is<span class="_ _7"> </span>not<span class="_ _6"> </span>crucial<span class="_ _6"> </span>speedup<span class="_ _7"> </span>for</div><div class="t m0 x0 hc y74 ff2 fs1 fc0 sc0 ls0 ws0">neural<span class="_ _4"> </span>network<span class="_ _4"> </span>LMs<span class="_ _4"> </span>as<span class="_ _5"> </span>the computational<span class="_ _4"> </span>bottleneck<span class="_ _5"> </span>is in<span class="_ _4"> </span>the<span class="_ _4"> </span><span class="ff6">N<span class="_ _5"> </span><span class="ff8">×<span class="_ _15"> </span></span>D<span class="_ _16"></span><span class="ff8">×<span class="_ _15"> </span></span>H<span class="_ _6"> </span></span>term, we<span class="_ _4"> </span>will<span class="_ _5"> </span>later propose</div><div class="t m0 x0 h3 y75 ff2 fs1 fc0 sc0 ls0 ws0">architectures<span class="_ _4"> </span>that do<span class="_ _4"> </span>not<span class="_ _4"> </span>ha<span class="_ _1"></span>ve<span class="_ _4"> </span>hidden<span class="_ _4"> </span>layers<span class="_ _4"> </span>and<span class="_ _4"> </span>thus<span class="_ _4"> </span>depend<span class="_ _4"> </span>heavily<span class="_ _4"> </span>on<span class="_ _4"> </span>the<span class="_ _4"> </span>ef<span class="_ _1"></span>ﬁciency<span class="_ _4"> </span>of<span class="_ _4"> </span>the<span class="_ _4"> </span>softmax</div><div class="t m0 x0 h3 y76 ff2 fs1 fc0 sc0 ls0 ws0">normalization.</div><div class="t m0 x0 h3 y28 ff1 fs1 fc0 sc0 ls0 ws0">2.2<span class="_ _d"> </span>Recurrent Neural Net Language Model (RNNLM)</div><div class="t m0 x0 h3 y77 ff2 fs1 fc0 sc0 ls0 ws0">Recurrent neural network based language model<span class="_ _6"> </span>has been proposed to overcome certain limitations</div><div class="t m0 x0 hc y78 ff2 fs1 fc0 sc0 ls0 ws0">of<span class="_ _4"> </span>the<span class="_ _4"> </span>feedforw<span class="_ _1"></span>ard<span class="_ _4"> </span>NNLM,<span class="_ _4"> </span>such<span class="_ _4"> </span>as<span class="_ _4"> </span>the<span class="_ _4"> </span>need<span class="_ _5"> </span>to specify<span class="_ _4"> </span>the<span class="_ _4"> </span>cont<span class="_ _1"></span>ext<span class="_ _4"> </span>length<span class="_ _4"> </span>(the<span class="_ _5"> </span>order of<span class="_ _4"> </span>the<span class="_ _4"> </span>model<span class="_ _5"> </span><span class="ff6">N<span class="_ _16"> </span></span>),</div><div class="t m0 x0 h3 y79 ff2 fs1 fc0 sc0 ls0 ws0">and<span class="_ _7"> </span>because<span class="_ _7"> </span>theoretically<span class="_ _7"> </span>RNNs<span class="_ _7"> </span>can<span class="_ _7"> </span>ef<span class="_ _1"></span>ﬁciently<span class="_ _7"> </span>represent<span class="_ _7"> </span>more<span class="_ _7"> </span>complex<span class="_ _6"> </span>patterns<span class="_ _7"> </span>than<span class="_ _7"> </span>the<span class="_ _7"> </span>shallow</div><div class="t m0 x0 h3 y7a ff2 fs1 fc0 sc0 ls0 ws0">neural<span class="_ _6"> </span>networks<span class="_ _6"> </span>[15,<span class="_ _6"> </span>2].<span class="_ _a"> </span>The<span class="_ _6"> </span>RNN<span class="_ _6"> </span>model<span class="_ _6"> </span>does<span class="_ _6"> </span>not<span class="_ _6"> </span>hav<span class="_ _1"></span>e<span class="_ _6"> </span>a<span class="_ _6"> </span>projection<span class="_ _6"> </span>layer;<span class="_ _7"> </span>only<span class="_ _6"> </span>input,<span class="_ _6"> </span>hidden<span class="_ _6"> </span>and</div><div class="t m0 x0 h3 y7b ff2 fs1 fc0 sc0 ls0 ws0">output<span class="_ _2"> </span>layer<span class="_ _3"></span>.<span class="_ _c"> </span>What<span class="_ _7"> </span>is<span class="_ _2"> </span>special<span class="_ _7"> </span>for<span class="_ _7"> </span>this<span class="_ _2"> </span>type<span class="_ _7"> </span>of<span class="_ _2"> </span>model<span class="_ _7"> </span>is<span class="_ _2"> </span>the<span class="_ _7"> </span>recurrent<span class="_ _7"> </span>matrix<span class="_ _2"> </span>that<span class="_ _7"> </span>connects<span class="_ _2"> </span>hidden</div><div class="t m0 x0 h3 y7c ff2 fs1 fc0 sc0 ls0 ws0">layer<span class="_ _6"> </span>to itself,<span class="_ _6"> </span>using<span class="_ _6"> </span>time-delayed<span class="_ _6"> </span>connections.<span class="_ _2"> </span>This<span class="_ _6"> </span>allo<span class="_ _1"></span>ws<span class="_ _6"> </span>the recurrent<span class="_ _6"> </span>model<span class="_ _6"> </span>to<span class="_ _6"> </span>form some<span class="_ _6"> </span>kind</div><div class="t m0 x0 h3 y7d ff2 fs1 fc0 sc0 ls0 ws0">of short<span class="_ _4"> </span>term memory<span class="_ _3"></span>, as information from<span class="_ _4"> </span>the past<span class="_ _4"> </span>can be<span class="_ _4"> </span>represented by<span class="_ _4"> </span>the hidden<span class="_ _4"> </span>layer state<span class="_ _4"> </span>that</div><div class="t m0 x0 h3 y7e ff2 fs1 fc0 sc0 ls0 ws0">gets updated based on the current input and the state of the hidden layer in the previous time step.</div><div class="t m0 x0 h3 y7f ff2 fs1 fc0 sc0 ls0 ws0">The complexity per training e<span class="_ _1"></span>xample of the RNN model is</div><div class="t m0 x2 hb y80 ff6 fs1 fc0 sc0 ls0 ws0">Q<span class="_ _6"> </span><span class="ff7">=<span class="_ _6"> </span></span>H<span class="_ _6"> </span><span class="ff8">×<span class="_ _4"> </span></span>H<span class="_ _6"> </span><span class="ff7">+<span class="_ _4"> </span></span>H<span class="_ _7"> </span><span class="ff8">×<span class="_ _4"> </span></span>V<span class="_ _f"></span>,<span class="_ _17"> </span><span class="ff2">(3)</span></div><div class="t m0 x0 hc y24 ff2 fs1 fc0 sc0 ls0 ws0">where the<span class="_ _6"> </span>word representations<span class="_ _6"> </span><span class="ff6">D<span class="_ _6"> </span></span>have the<span class="_ _6"> </span>same dimensionality<span class="_ _6"> </span>as<span class="_ _6"> </span>the hidden<span class="_ _6"> </span>layer<span class="_ _6"> </span><span class="ff6">H<span class="_ _f"></span></span>.<span class="_ _2"> </span>Again,<span class="_ _6"> </span>the</div><div class="t m0 x0 hc y25 ff2 fs1 fc0 sc0 ls0 ws0">term <span class="ff6">H<span class="_ _7"> </span><span class="ff8">×<span class="_ _4"> </span></span>V<span class="_ _9"> </span></span>can<span class="_ _4"> </span>be efﬁciently reduced to <span class="ff6">H<span class="_ _7"> </span><span class="ff8">×<span class="_ _4"> </span></span>l<span class="_ _f"></span>og</span></div><div class="t m0 x15 h8 y81 ff9 fs4 fc0 sc0 ls0 ws0">2</div><div class="t m0 x16 hb y25 ff7 fs1 fc0 sc0 ls0 ws0">(<span class="ff6">V<span class="_ _4"> </span></span>)<span class="_ _6"> </span><span class="ff2">by using hierarchical softmax.<span class="_ _7"> </span>Most<span class="_ _6"> </span>of the</span></div><div class="t m0 x0 hc y26 ff2 fs1 fc0 sc0 ls0 ws0">complexity then comes from <span class="ff6">H<span class="_ _6"> </span><span class="ff8">×<span class="_ _4"> </span></span>H<span class="_ _15"></span></span>.</div><div class="t m0 xe h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">3</div><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,494.806,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:420.214000px;bottom:634.870000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,585.091,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:447.772000px;bottom:584.845000px;width:5.974000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,519.443,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:358.279000px;bottom:416.429000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,572.514,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:373.890000px;bottom:416.429000px;width:10.956000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,710.138,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:389.502000px;bottom:416.429000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,493.409,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:413.240000px;bottom:405.609000px;width:5.974000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,351.228,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:423.291000px;bottom:405.390000px;width:5.973000px;height:8.006000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,132.32,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:231.779000px;bottom:344.698000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,159.725,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:176.878000px;bottom:184.944000px;width:10.956000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,556.857,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:192.201000px;bottom:185.084000px;width:5.973000px;height:7.786000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf4" class="pf w0 h0" data-page-no="4"><div class="pc pc4 w0 h0"><div class="t m0 x0 h3 y2a ff1 fs1 fc0 sc0 ls0 ws0">2.3<span class="_ _d"> </span>Parallel T<span class="_ _3"></span>raining of Neural Networks</div><div class="t m0 x0 h3 y82 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>o<span class="_ _2"> </span>train<span class="_ _2"> </span>models<span class="_ _7"> </span>on<span class="_ _2"> </span>huge<span class="_ _7"> </span>data<span class="_ _2"> </span>sets,<span class="_ _2"> </span>we<span class="_ _2"> </span>ha<span class="_ _1"></span>v<span class="_ _1"></span>e<span class="_ _2"> </span>implemented<span class="_ _7"> </span>sev<span class="_ _1"></span>eral<span class="_ _2"> </span>models<span class="_ _7"> </span>on<span class="_ _2"> </span>top<span class="_ _2"> </span>of<span class="_ _7"> </span>a<span class="_ _2"> </span>lar<span class="_ _1"></span>ge-scale</div><div class="t m0 x0 h3 y1 ff2 fs1 fc0 sc0 ls0 ws0">distributed frame<span class="_ _1"></span>work called<span class="_ _6"> </span>DistBelief [6],<span class="_ _6"> </span>including the<span class="_ _6"> </span>feedforward NNLM and<span class="_ _6"> </span>the new models</div><div class="t m0 x0 h3 y5f ff2 fs1 fc0 sc0 ls0 ws0">proposed<span class="_ _2"> </span>in<span class="_ _7"> </span>this<span class="_ _2"> </span>paper<span class="_ _1"></span>.<span class="_ _c"> </span>The<span class="_ _7"> </span>framework<span class="_ _2"> </span>allo<span class="_ _1"></span>ws<span class="_ _7"> </span>us<span class="_ _2"> </span>to<span class="_ _2"> </span>run<span class="_ _7"> </span>multiple<span class="_ _2"> </span>replicas<span class="_ _7"> </span>of<span class="_ _2"> </span>the<span class="_ _2"> </span>same<span class="_ _7"> </span>model<span class="_ _2"> </span>in</div><div class="t m0 x0 h3 y60 ff2 fs1 fc0 sc0 ls0 ws0">parallel,<span class="_ _6"> </span>and<span class="_ _7"> </span>each<span class="_ _6"> </span>replica<span class="_ _6"> </span>synchronizes<span class="_ _7"> </span>its<span class="_ _6"> </span>gradient<span class="_ _6"> </span>updates<span class="_ _6"> </span>through<span class="_ _7"> </span>a<span class="_ _6"> </span>centralized<span class="_ _6"> </span>server<span class="_ _6"> </span>that<span class="_ _6"> </span>keeps</div><div class="t m0 x0 h3 y61 ff2 fs1 fc0 sc0 ls0 ws0">all the parameters.<span class="_ _7"> </span>For this parallel training, we use mini-batch asynchronous gradient descent with</div><div class="t m0 x0 h3 y83 ff2 fs1 fc0 sc0 ls0 ws0">an adaptiv<span class="_ _1"></span>e learning<span class="_ _6"> </span>rate procedure<span class="_ _6"> </span>called Adagrad [7].<span class="_ _2"> </span>Under this framework, it is common<span class="_ _6"> </span>to use</div><div class="t m0 x0 h3 y84 ff2 fs1 fc0 sc0 ls0 ws0">one<span class="_ _7"> </span>hundred<span class="_ _7"> </span>or<span class="_ _7"> </span>more<span class="_ _7"> </span>model<span class="_ _7"> </span>replicas,<span class="_ _7"> </span>each<span class="_ _7"> </span>using<span class="_ _7"> </span>many<span class="_ _7"> </span>CPU<span class="_ _7"> </span>cores<span class="_ _7"> </span>at<span class="_ _7"> </span>different<span class="_ _7"> </span>machines<span class="_ _7"> </span>in<span class="_ _7"> </span>a<span class="_ _7"> </span>data</div><div class="t m0 x0 h3 y85 ff2 fs1 fc0 sc0 ls0 ws0">center<span class="_ _1"></span>.</div><div class="t m0 x0 h5 y86 ff1 fs2 fc0 sc0 ls0 ws0">3<span class="_ _8"> </span>New Log-linear Models</div><div class="t m0 x0 h3 y87 ff2 fs1 fc0 sc0 ls0 ws0">In<span class="_ _2"> </span>this<span class="_ _2"> </span>section,<span class="_ _2"> </span>we<span class="_ _2"> </span>propose<span class="_ _2"> </span>tw<span class="_ _1"></span>o<span class="_ _2"> </span>ne<span class="_ _1"></span>w<span class="_ _2"> </span>model<span class="_ _2"> </span>architectures<span class="_ _7"> </span>for<span class="_ _2"> </span>learning<span class="_ _2"> </span>distributed<span class="_ _2"> </span>representations</div><div class="t m0 x0 h3 y88 ff2 fs1 fc0 sc0 ls0 ws0">of<span class="_ _6"> </span>words<span class="_ _7"> </span>that<span class="_ _6"> </span>try<span class="_ _7"> </span>to<span class="_ _6"> </span>minimize<span class="_ _7"> </span>computational<span class="_ _6"> </span>complexity<span class="_ _3"></span>.<span class="_ _9"> </span>The<span class="_ _6"> </span>main<span class="_ _6"> </span>observation<span class="_ _6"> </span>from<span class="_ _7"> </span>the<span class="_ _6"> </span>previous</div><div class="t m0 x0 h3 y89 ff2 fs1 fc0 sc0 ls0 ws0">section<span class="_ _4"> </span>was<span class="_ _4"> </span>that<span class="_ _4"> </span>most<span class="_ _4"> </span>of<span class="_ _4"> </span>the<span class="_ _4"> </span>comple<span class="_ _1"></span>xity<span class="_ _4"> </span>is<span class="_ _4"> </span>caused<span class="_ _4"> </span>by<span class="_ _4"> </span>the<span class="_ _5"> </span>non-linear hidden<span class="_ _4"> </span>layer<span class="_ _4"> </span>in<span class="_ _4"> </span>the<span class="_ _4"> </span>model. While</div><div class="t m0 x0 h3 y8a ff2 fs1 fc0 sc0 ls0 ws0">this<span class="_ _6"> </span>is<span class="_ _6"> </span>what<span class="_ _6"> </span>makes<span class="_ _6"> </span>neural<span class="_ _7"> </span>networks so<span class="_ _7"> </span>attracti<span class="_ _1"></span>ve,<span class="_ _6"> </span>we<span class="_ _6"> </span>decided<span class="_ _6"> </span>to<span class="_ _6"> </span>explore<span class="_ _6"> </span>simpler<span class="_ _6"> </span>models<span class="_ _7"> </span>that<span class="_ _6"> </span>might</div><div class="t m0 x0 h3 y8 ff2 fs1 fc0 sc0 ls0 ws0">not<span class="_ _4"> </span>be<span class="_ _4"> </span>able<span class="_ _4"> </span>to<span class="_ _4"> </span>represent<span class="_ _4"> </span>the<span class="_ _4"> </span>data<span class="_ _4"> </span>as<span class="_ _4"> </span>precisely<span class="_ _4"> </span>as<span class="_ _4"> </span>neural<span class="_ _4"> </span>netw<span class="_ _1"></span>orks,<span class="_ _4"> </span>but<span class="_ _4"> </span>can<span class="_ _4"> </span>possibly<span class="_ _4"> </span>be<span class="_ _4"> </span>trained<span class="_ _4"> </span>on<span class="_ _4"> </span>much</div><div class="t m0 x0 h3 y8b ff2 fs1 fc0 sc0 ls0 ws0">more data efﬁciently<span class="_ _3"></span>.</div><div class="t m0 x0 h3 y8c ff2 fs1 fc0 sc0 ls0 ws0">The<span class="_ _2"> </span>ne<span class="_ _1"></span>w<span class="_ _2"> </span>architectures<span class="_ _7"> </span>directly<span class="_ _2"> </span>follow<span class="_ _7"> </span>those<span class="_ _2"> </span>proposed<span class="_ _2"> </span>in<span class="_ _2"> </span>our<span class="_ _7"> </span>earlier<span class="_ _2"> </span>work<span class="_ _2"> </span>[13,<span class="_ _7"> </span>14],<span class="_ _a"> </span>where<span class="_ _2"> </span>it<span class="_ _2"> </span>w<span class="_ _1"></span>as</div><div class="t m0 x0 h3 y8d ff2 fs1 fc0 sc0 ls0 ws0">found that<span class="_ _4"> </span>neural<span class="_ _4"> </span>network<span class="_ _4"> </span>language model<span class="_ _4"> </span>can<span class="_ _4"> </span>be successfully<span class="_ _4"> </span>trained in<span class="_ _4"> </span>two<span class="_ _4"> </span>steps:<span class="_ _6"> </span>ﬁrst, continuous</div><div class="t m0 x0 h3 y8e ff2 fs1 fc0 sc0 ls0 ws0">word v<span class="_ _1"></span>ectors are<span class="_ _4"> </span>learned using<span class="_ _4"> </span>simple model, and<span class="_ _4"> </span>then the<span class="_ _4"> </span>N-gram NNLM<span class="_ _4"> </span>is trained on<span class="_ _4"> </span>top of<span class="_ _4"> </span>these</div><div class="t m0 x0 h3 y8f ff2 fs1 fc0 sc0 ls0 ws0">distributed<span class="_ _7"> </span>representations<span class="_ _2"> </span>of<span class="_ _7"> </span>words.<span class="_ _11"> </span>While<span class="_ _2"> </span>there<span class="_ _7"> </span>has<span class="_ _2"> </span>been<span class="_ _7"> </span>later<span class="_ _7"> </span>substantial<span class="_ _2"> </span>amount<span class="_ _7"> </span>of<span class="_ _2"> </span>w<span class="_ _1"></span>ork<span class="_ _7"> </span>that</div><div class="t m0 x0 h3 y90 ff2 fs1 fc0 sc0 ls0 ws0">focuses on<span class="_ _4"> </span>learning word<span class="_ _4"> </span>vectors,<span class="_ _4"> </span>we consider<span class="_ _4"> </span>the approach<span class="_ _4"> </span>proposed in [13]<span class="_ _4"> </span>to be<span class="_ _4"> </span>the simplest<span class="_ _4"> </span>one.</div><div class="t m0 x0 h3 y91 ff2 fs1 fc0 sc0 ls0 ws0">Note that related models hav<span class="_ _1"></span>e been proposed also much earlier [26, 8].</div><div class="t m0 x0 h3 y92 ff1 fs1 fc0 sc0 ls0 ws0">3.1<span class="_ _d"> </span>Continuous Bag-of-W<span class="_ _3"></span>ords Model</div><div class="t m0 x0 h3 y93 ff2 fs1 fc0 sc0 ls0 ws0">The<span class="_ _7"> </span>ﬁrst<span class="_ _7"> </span>proposed<span class="_ _7"> </span>architecture<span class="_ _7"> </span>is<span class="_ _7"> </span>similar<span class="_ _7"> </span>to<span class="_ _7"> </span>the<span class="_ _7"> </span>feedforward<span class="_ _7"> </span>NNLM,<span class="_ _7"> </span>where<span class="_ _7"> </span>the<span class="_ _7"> </span>non-linear<span class="_ _7"> </span>hidden</div><div class="t m0 x0 h3 y6e ff2 fs1 fc0 sc0 ls0 ws0">layer<span class="_ _7"> </span>is<span class="_ _2"> </span>remo<span class="_ _1"></span>v<span class="_ _1"></span>ed<span class="_ _7"> </span>and<span class="_ _2"> </span>the<span class="_ _7"> </span>projection<span class="_ _7"> </span>layer<span class="_ _7"> </span>is<span class="_ _2"> </span>shared<span class="_ _7"> </span>for<span class="_ _7"> </span>all<span class="_ _2"> </span>w<span class="_ _1"></span>ords<span class="_ _7"> </span>(not<span class="_ _7"> </span>just<span class="_ _2"> </span>the<span class="_ _7"> </span>projection<span class="_ _7"> </span>matrix);</div><div class="t m0 x0 h3 y6f ff2 fs1 fc0 sc0 ls0 ws0">thus,<span class="_ _6"> </span>all words get<span class="_ _6"> </span>projected<span class="_ _6"> </span>into the<span class="_ _6"> </span>same position<span class="_ _6"> </span>(their<span class="_ _6"> </span>vectors are averaged).<span class="_ _2"> </span>W<span class="_ _0"></span>e<span class="_ _6"> </span>call<span class="_ _6"> </span>this archi-</div><div class="t m0 x0 h3 y70 ff2 fs1 fc0 sc0 ls0 ws0">tecture a bag-of-words<span class="_ _6"> </span>model as the<span class="_ _6"> </span>order of words in<span class="_ _6"> </span>the history does<span class="_ _6"> </span>not inﬂuence the<span class="_ _6"> </span>projection.</div><div class="t m0 x0 h3 y71 ff2 fs1 fc0 sc0 ls0 ws0">Furthermore, we also use words<span class="_ _4"> </span>from the future; we ha<span class="_ _1"></span>ve obtained the<span class="_ _4"> </span>best performance on the task</div><div class="t m0 x0 h3 y72 ff2 fs1 fc0 sc0 ls0 ws0">introduced<span class="_ _7"> </span>in<span class="_ _7"> </span>the<span class="_ _2"> </span>ne<span class="_ _1"></span>xt<span class="_ _7"> </span>section<span class="_ _7"> </span>by<span class="_ _7"> </span>building<span class="_ _7"> </span>a<span class="_ _7"> </span>log-linear<span class="_ _7"> </span>classiﬁer<span class="_ _2"> </span>with<span class="_ _6"> </span>four<span class="_ _2"> </span>future<span class="_ _6"> </span>and<span class="_ _7"> </span>four<span class="_ _2"> </span>history</div><div class="t m0 x0 h3 y5d ff2 fs1 fc0 sc0 ls0 ws0">words<span class="_ _7"> </span>at<span class="_ _7"> </span>the<span class="_ _7"> </span>input,<span class="_ _2"> </span>where<span class="_ _7"> </span>the<span class="_ _7"> </span>training<span class="_ _2"> </span>criterion<span class="_ _6"> </span>is<span class="_ _7"> </span>to<span class="_ _2"> </span>correctly<span class="_ _6"> </span>classify<span class="_ _2"> </span>the<span class="_ _6"> </span>current<span class="_ _2"> </span>(middle)<span class="_ _6"> </span>word.</div><div class="t m0 x0 h3 y73 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _1"></span>raining complexity is then</div><div class="t m0 x22 hb y74 ff6 fs1 fc0 sc0 ls0 ws0">Q<span class="_ _6"> </span><span class="ff7">=<span class="_ _6"> </span></span>N<span class="_ _7"> </span><span class="ff8">×<span class="_ _4"> </span></span>D<span class="_ _4"> </span><span class="ff7">+<span class="_ _6"> </span></span>D<span class="_ _4"> </span><span class="ff8">×<span class="_ _4"> </span></span>log</div><div class="t m0 x8 h8 y94 ff9 fs4 fc0 sc0 ls0 ws0">2</div><div class="t m0 x10 hb y74 ff7 fs1 fc0 sc0 ls0 ws0">(<span class="ff6">V<span class="_ _4"> </span></span>)<span class="ff6">.<span class="_ _18"> </span><span class="ff2">(4)</span></span></div><div class="t m0 x0 h3 y95 ff2 fs1 fc0 sc0 ls0 ws0">W<span class="_ _3"></span>e denote<span class="_ _6"> </span>this model further<span class="_ _6"> </span>as CBO<span class="_ _1"></span>W<span class="_ _3"></span>, as unlike standard<span class="_ _6"> </span>bag-of-words model, it uses<span class="_ _6"> </span>continuous</div><div class="t m0 x0 h3 y96 ff2 fs1 fc0 sc0 ls0 ws0">distributed representation<span class="_ _4"> </span>of the context.<span class="_ _6"> </span>The model architecture is sho<span class="_ _1"></span>wn at Figure 1.<span class="_ _6"> </span>Note that the</div><div class="t m0 x0 h3 y97 ff2 fs1 fc0 sc0 ls0 ws0">weight matrix<span class="_ _4"> </span>between<span class="_ _4"> </span>the input<span class="_ _4"> </span>and the<span class="_ _4"> </span>projection<span class="_ _4"> </span>layer is<span class="_ _4"> </span>shared<span class="_ _4"> </span>for all<span class="_ _4"> </span>word<span class="_ _4"> </span>positions in<span class="_ _4"> </span>the<span class="_ _4"> </span>same</div><div class="t m0 x0 h3 y98 ff2 fs1 fc0 sc0 ls0 ws0">way as in the NNLM.</div><div class="t m0 x0 h3 y99 ff1 fs1 fc0 sc0 ls0 ws0">3.2<span class="_ _d"> </span>Continuous Skip-gram Model</div><div class="t m0 x0 h3 y4e ff2 fs1 fc0 sc0 ls0 ws0">The second<span class="_ _4"> </span>architecture is<span class="_ _4"> </span>similar to CBO<span class="_ _1"></span>W<span class="_ _0"></span>, b<span class="_ _1"></span>ut instead of<span class="_ _4"> </span>predicting the<span class="_ _4"> </span>current w<span class="_ _1"></span>ord based<span class="_ _4"> </span>on the</div><div class="t m0 x0 h3 y4f ff2 fs1 fc0 sc0 ls0 ws0">context,<span class="_ _6"> </span>it<span class="_ _7"> </span>tries<span class="_ _7"> </span>to<span class="_ _6"> </span>maximize<span class="_ _7"> </span>classiﬁcation<span class="_ _6"> </span>of<span class="_ _7"> </span>a<span class="_ _7"> </span>word<span class="_ _6"> </span>based<span class="_ _7"> </span>on<span class="_ _6"> </span>another<span class="_ _7"> </span>word<span class="_ _6"> </span>in<span class="_ _7"> </span>the<span class="_ _6"> </span>same<span class="_ _7"> </span>sentence.</div><div class="t m0 x0 h3 y50 ff2 fs1 fc0 sc0 ls0 ws0">More<span class="_ _2"> </span>precisely<span class="_ _1"></span>,<span class="_ _2"> </span>we<span class="_ _2"> </span>use<span class="_ _2"> </span>each<span class="_ _2"> </span>current<span class="_ _2"> </span>word<span class="_ _2"> </span>as<span class="_ _2"> </span>an<span class="_ _2"> </span>input<span class="_ _2"> </span>to<span class="_ _a"> </span>a<span class="_ _2"> </span>log-linear<span class="_ _2"> </span>classiﬁer<span class="_ _2"> </span>with<span class="_ _2"> </span>continuous</div><div class="t m0 x0 h3 y51 ff2 fs1 fc0 sc0 ls0 ws0">projection<span class="_ _7"> </span>layer,<span class="_ _7"> </span>and<span class="_ _2"> </span>predict<span class="_ _7"> </span>words<span class="_ _7"> </span>within<span class="_ _7"> </span>a<span class="_ _2"> </span>certai<span class="_ _1"></span>n<span class="_ _7"> </span>range<span class="_ _2"> </span>before<span class="_ _7"> </span>and<span class="_ _7"> </span>after<span class="_ _2"> </span>the<span class="_ _6"> </span>current<span class="_ _2"> </span>w<span class="_ _1"></span>ord.<span class="_ _e"> </span>W<span class="_ _1"></span>e</div><div class="t m0 x0 h3 y9a ff2 fs1 fc0 sc0 ls0 ws0">found that<span class="_ _6"> </span>increasing<span class="_ _6"> </span>the range<span class="_ _6"> </span>improv<span class="_ _1"></span>es quality<span class="_ _6"> </span>of<span class="_ _6"> </span>the resulting<span class="_ _6"> </span>word vectors,<span class="_ _6"> </span>b<span class="_ _1"></span>ut it<span class="_ _6"> </span>also<span class="_ _6"> </span>increases</div><div class="t m0 x0 h3 y9b ff2 fs1 fc0 sc0 ls0 ws0">the<span class="_ _6"> </span>computational<span class="_ _7"> </span>complexity<span class="_ _3"></span>.<span class="_ _14"> </span>Since<span class="_ _6"> </span>the<span class="_ _7"> </span>more<span class="_ _6"> </span>distant<span class="_ _7"> </span>words<span class="_ _6"> </span>are<span class="_ _6"> </span>usually<span class="_ _7"> </span>less<span class="_ _6"> </span>related<span class="_ _6"> </span>to<span class="_ _7"> </span>the<span class="_ _6"> </span>current</div><div class="t m0 x0 h3 y9c ff2 fs1 fc0 sc0 ls0 ws0">word<span class="_ _7"> </span>than<span class="_ _7"> </span>those<span class="_ _7"> </span>close<span class="_ _6"> </span>to<span class="_ _7"> </span>it,<span class="_ _2"> </span>we<span class="_ _6"> </span>give<span class="_ _6"> </span>less<span class="_ _7"> </span>weight<span class="_ _7"> </span>to<span class="_ _7"> </span>the<span class="_ _7"> </span>distant<span class="_ _7"> </span>words<span class="_ _7"> </span>by<span class="_ _7"> </span>sampling<span class="_ _7"> </span>less<span class="_ _7"> </span>from<span class="_ _7"> </span>those</div><div class="t m0 x0 h3 y9d ff2 fs1 fc0 sc0 ls0 ws0">words in our training examples.</div><div class="t m0 x0 h3 y55 ff2 fs1 fc0 sc0 ls0 ws0">The training complexity of this architecture is proportional to</div><div class="t m0 x18 hb y9e ff6 fs1 fc0 sc0 ls0 ws0">Q<span class="_ _6"> </span><span class="ff7">=<span class="_ _6"> </span></span>C<span class="_ _6"> </span><span class="ff8">×<span class="_ _4"> </span><span class="ff7">(</span></span>D<span class="_ _4"> </span><span class="ff7">+<span class="_ _6"> </span></span>D<span class="_ _4"> </span><span class="ff8">×<span class="_ _4"> </span></span>log</div><div class="t m0 x8 h8 y9f ff9 fs4 fc0 sc0 ls0 ws0">2</div><div class="t m0 x10 hb y9e ff7 fs1 fc0 sc0 ls0 ws0">(<span class="ff6">V<span class="_ _4"> </span></span>))<span class="ff6">,<span class="_ _19"> </span><span class="ff2">(5)</span></span></div><div class="t m0 x0 hb y25 ff2 fs1 fc0 sc0 ls0 ws0">where <span class="ff6">C<span class="_ _2"> </span></span>is the maximum distance<span class="_ _6"> </span>of the<span class="_ _6"> </span>words.<span class="_ _2"> </span>Thus, if we<span class="_ _6"> </span>choose <span class="ff6">C<span class="_ _2"> </span><span class="ff7">=<span class="_ _7"> </span>5</span></span>, for<span class="_ _6"> </span>each training<span class="_ _6"> </span>word</div><div class="t m0 x0 hb y26 ff2 fs1 fc0 sc0 ls0 ws0">we<span class="_ _7"> </span>will<span class="_ _7"> </span>select<span class="_ _6"> </span>randomly<span class="_ _7"> </span>a<span class="_ _7"> </span>number<span class="_ _7"> </span><span class="ff6">R<span class="_ _7"> </span></span>in<span class="_ _7"> </span>range<span class="_ _7"> </span><span class="ff6">&lt;<span class="_ _2"> </span><span class="ff7">1;<span class="_ _5"> </span></span>C<span class="_ _9"> </span>&gt;</span>,<span class="_ _7"> </span>and<span class="_ _6"> </span>then<span class="_ _7"> </span>use<span class="_ _7"> </span><span class="ff6">R<span class="_ _7"> </span></span>words<span class="_ _7"> </span>from<span class="_ _7"> </span>history<span class="_ _7"> </span>and</div><div class="t m0 xe h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">4</div><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,419.755,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:270.984000px;bottom:667.058000px;width:5.973000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,392.758,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:316.512000px;bottom:623.287000px;width:5.974000px;height:7.722000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,213.374,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:413.895000px;bottom:477.595000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,187.13,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:430.051000px;bottom:477.735000px;width:10.955000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,213.374,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:398.037000px;bottom:433.760000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,494.806,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:362.352000px;bottom:422.801000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,366.514,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:377.296000px;bottom:422.801000px;width:5.974000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf5" data-dest-detail='[5,"XYZ",147.366,440.051,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:440.571000px;bottom:262.925000px;width:5.973000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf5" class="pf w0 h0" data-page-no="5"><div class="pc pc5 w0 h0"><img class="bi x13 y6d w4 hd" alt="" src="bg5.png"/><div class="c x23 ya0 w4 hd"><div class="t m0 x24 he y4e ffa fs7 fc2 sc0 ls0 ws0">w(t-2)</div><div class="t m0 x24 he ya1 ffa fs7 fc2 sc0 ls0 ws0">w(t+1)</div><div class="t m0 x24 he ya2 ffa fs7 fc2 sc0 ls0 ws0">w(t-1)</div><div class="t m0 x24 he ya3 ffa fs7 fc2 sc0 ls0 ws0">w(t+2)</div><div class="t m0 x5 he y22 ffa fs7 fc2 sc0 ls0 ws0">w(t)</div><div class="t m0 x25 he ya4 ffa fs7 fc2 sc0 ls0 ws0">SUM</div><div class="t m0 x26 he ya5 ffa fs7 fc2 sc0 ls0 ws0">       INPUT         PROJECTION     <span class="_ _f"></span>    OUTPUT</div><div class="t m0 x1a he y54 ffa fs7 fc2 sc0 ls0 ws0">w(t)</div><div class="t m0 x27 he ya5 ffa fs7 fc2 sc0 ls0 ws0">          I<span class="_ _f"></span>N<span class="_ _1"></span>PUT         PROJECTION      OUTPUT</div><div class="t m0 x28 he y79 ffa fs7 fc2 sc0 ls0 ws0">w(t-2)</div><div class="t m0 xb he ya2 ffa fs7 fc2 sc0 ls0 ws0">w(t-1)</div><div class="t m0 x28 he y58 ffa fs7 fc2 sc0 ls0 ws0">w(t+1)</div><div class="t m0 x28 he y5c ffa fs7 fc2 sc0 ls0 ws0">w(t+2)</div><div class="t m0 x29 hf ya6 ffa fs8 fc2 sc0 ls0 ws0">           <span class="_ _f"></span>        <span class="ffb">CBOW       <span class="_ _f"></span>                       <span class="_ _f"></span>                     Skip<span class="_ _f"></span>-gra<span class="_ _1"></span>m</span></div></div><div class="t m0 x0 h3 yb ff2 fs1 fc0 sc0 ls0 ws0">Figure 1:<span class="_ _7"> </span>Ne<span class="_ _1"></span>w model architectures.<span class="_ _6"> </span>The CBOW architecture predicts the current word based on the</div><div class="t m0 x0 h3 yc ff2 fs1 fc0 sc0 ls0 ws0">context, and the Skip-gram predicts surrounding words gi<span class="_ _1"></span>v<span class="_ _1"></span>en the current word.</div><div class="t m0 x0 hb ya7 ff6 fs1 fc0 sc0 ls0 ws0">R<span class="_ _2"> </span><span class="ff2">w<span class="_ _1"></span>ords<span class="_ _7"> </span>from<span class="_ _7"> </span>the<span class="_ _2"> </span>future<span class="_ _7"> </span>of<span class="_ _7"> </span>the<span class="_ _2"> </span>current<span class="_ _6"> </span>word<span class="_ _2"> </span>as<span class="_ _6"> </span>correct<span class="_ _2"> </span>labels.<span class="_ _e"> </span>This<span class="_ _7"> </span>will<span class="_ _2"> </span>require<span class="_ _7"> </span>us<span class="_ _7"> </span>to<span class="_ _2"> </span>do<span class="_ _7"> </span><span class="ff6">R<span class="_ _6"> </span><span class="ff8">×<span class="_ _6"> </span><span class="ff7">2</span></span></span></span></div><div class="t m0 x0 hb ya8 ff2 fs1 fc0 sc0 ls0 ws0">word classiﬁcations, with<span class="_ _6"> </span>the current<span class="_ _6"> </span>word as input,<span class="_ _6"> </span>and each of<span class="_ _6"> </span>the <span class="ff6">R<span class="_ _6"> </span><span class="ff7">+<span class="_ _4"> </span></span>R<span class="_ _6"> </span></span>w<span class="_ _1"></span>ords as output.<span class="_ _2"> </span>In the</div><div class="t m0 x0 hb ya9 ff2 fs1 fc0 sc0 ls0 ws0">following e<span class="_ _1"></span>xperiments, we use <span class="ff6">C<span class="_ _7"> </span><span class="ff7">=<span class="_ _6"> </span>10</span></span>.</div><div class="t m0 x0 h5 y12 ff1 fs2 fc0 sc0 ls0 ws0">4<span class="_ _8"> </span>Results</div><div class="t m0 x0 h3 yaa ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>o<span class="_ _6"> </span>compare<span class="_ _7"> </span>the<span class="_ _6"> </span>quality<span class="_ _6"> </span>of<span class="_ _6"> </span>different<span class="_ _6"> </span>versions<span class="_ _6"> </span>of<span class="_ _6"> </span>word<span class="_ _6"> </span>vectors,<span class="_ _6"> </span>previous<span class="_ _6"> </span>papers<span class="_ _6"> </span>typically<span class="_ _7"> </span>use<span class="_ _6"> </span>a<span class="_ _6"> </span>table</div><div class="t m0 x0 h3 yab ff2 fs1 fc0 sc0 ls0 ws0">showing<span class="_ _6"> </span>example<span class="_ _6"> </span>words<span class="_ _7"> </span>and<span class="_ _7"> </span>their<span class="_ _7"> </span>most<span class="_ _7"> </span>similar<span class="_ _6"> </span>words,<span class="_ _7"> </span>and<span class="_ _7"> </span>understand<span class="_ _7"> </span>them<span class="_ _7"> </span>intuitiv<span class="_ _1"></span>ely<span class="_ _3"></span>.<span class="_ _9"> </span>Although</div><div class="t m0 x0 h3 yac ff2 fs1 fc0 sc0 ls0 ws0">it<span class="_ _6"> </span>is<span class="_ _6"> </span>easy<span class="_ _6"> </span>to show<span class="_ _6"> </span>that<span class="_ _6"> </span>word <span class="ff5">F<span class="_ _1"></span>rance <span class="ff2">is<span class="_ _6"> </span>similar<span class="_ _6"> </span>to<span class="_ _6"> </span></span>Italy<span class="_ _6"> </span><span class="ff2">and<span class="_ _6"> </span>perhaps<span class="_ _6"> </span>some other<span class="_ _6"> </span>countries,<span class="_ _7"> </span>it is<span class="_ _6"> </span>much</span></span></div><div class="t m0 x0 h3 yad ff2 fs1 fc0 sc0 ls0 ws0">more<span class="_ _6"> </span>challenging when<span class="_ _6"> </span>subjecting<span class="_ _6"> </span>those<span class="_ _6"> </span>vectors in<span class="_ _6"> </span>a more<span class="_ _6"> </span>complex similarity<span class="_ _6"> </span>task,<span class="_ _6"> </span>as<span class="_ _6"> </span>follows.<span class="_ _2"> </span>W<span class="_ _3"></span>e</div><div class="t m0 x0 h3 yae ff2 fs1 fc0 sc0 ls0 ws0">follow<span class="_ _4"> </span>pre<span class="_ _1"></span>vious<span class="_ _4"> </span>observ<span class="_ _1"></span>ation<span class="_ _4"> </span>that<span class="_ _4"> </span>there<span class="_ _4"> </span>can<span class="_ _4"> </span>be<span class="_ _4"> </span>man<span class="_ _1"></span>y<span class="_ _4"> </span>dif<span class="_ _1"></span>ferent<span class="_ _4"> </span>types<span class="_ _4"> </span>of<span class="_ _4"> </span>similarities<span class="_ _4"> </span>between<span class="_ _4"> </span>words,<span class="_ _4"> </span>for</div><div class="t m0 x0 h3 yaf ff2 fs1 fc0 sc0 ls0 ws0">example,<span class="_ _6"> </span>word<span class="_ _6"> </span><span class="ff5">big<span class="_ _6"> </span></span>is<span class="_ _6"> </span>similar<span class="_ _6"> </span>to<span class="_ _6"> </span><span class="ff5">bigger<span class="_ _6"> </span></span>in<span class="_ _6"> </span>the<span class="_ _6"> </span>same<span class="_ _6"> </span>sense<span class="_ _6"> </span>that<span class="_ _6"> </span><span class="ff5">small<span class="_ _6"> </span></span>is<span class="_ _6"> </span>similar<span class="_ _6"> </span>to<span class="_ _6"> </span><span class="ff5">smaller</span>.<span class="_ _14"> </span>Example</div><div class="t m0 x0 h3 yb0 ff2 fs1 fc0 sc0 ls0 ws0">of another type of relationship<span class="_ _6"> </span>can be word pairs <span class="ff5">big - biggest </span>and <span class="ff5">small - smallest </span>[20].<span class="_ _2"> </span>W<span class="_ _0"></span>e further</div><div class="t m0 x0 h3 ya5 ff2 fs1 fc0 sc0 ls0 ws0">denote<span class="_ _7"> </span>two<span class="_ _7"> </span>pairs<span class="_ _7"> </span>of<span class="_ _2"> </span>w<span class="_ _1"></span>ords<span class="_ _7"> </span>with<span class="_ _7"> </span>the<span class="_ _7"> </span>same<span class="_ _2"> </span>relationship<span class="_ _6"> </span>as<span class="_ _2"> </span>a<span class="_ _6"> </span>question,<span class="_ _2"> </span>as<span class="_ _7"> </span>we<span class="_ _7"> </span>can<span class="_ _7"> </span>ask:<span class="_ _9"> </span>”What<span class="_ _6"> </span>is<span class="_ _7"> </span>the</div><div class="t m0 x0 h3 yb1 ff2 fs1 fc0 sc0 ls0 ws0">word that is similar to <span class="ff5">small </span>in the same sense as <span class="ff5">bigg<span class="_ _1"></span>est <span class="ff2">is similar to </span>big<span class="ff2">?”</span></span></div><div class="t m0 x0 h3 y4e ff2 fs1 fc0 sc0 ls0 ws0">Somewhat<span class="_ _4"> </span>surprisingly<span class="_ _3"></span>, these questions can be answered<span class="_ _4"> </span>by performing simple<span class="_ _4"> </span>algebraic operations</div><div class="t m0 x0 h3 y4f ff2 fs1 fc0 sc0 ls0 ws0">with the vector representation<span class="_ _6"> </span>of words.<span class="_ _7"> </span>T<span class="_ _1"></span>o ﬁnd a word that is<span class="_ _6"> </span>similar to <span class="ff5">small<span class="_ _6"> </span></span>in the same<span class="_ _6"> </span>sense as</div><div class="t m0 x0 hb y50 ff5 fs1 fc0 sc0 ls0 ws0">biggest<span class="_ _4"> </span><span class="ff2">is<span class="_ _4"> </span>similar<span class="_ _4"> </span>to </span>big<span class="ff2">,<span class="_ _4"> </span>we<span class="_ _4"> </span>can simply<span class="_ _4"> </span>compute<span class="_ _4"> </span>vector<span class="_ _4"> </span><span class="ff6">X<span class="_ _7"> </span><span class="ff7">=<span class="_ _6"> </span></span>v<span class="_ _f"></span>ector<span class="ff7">(”</span>big<span class="_ _f"></span>g<span class="_ _f"></span>est<span class="ff7">”)<span class="_ _5"> </span><span class="ff8">−<span class="_ _16"> </span></span></span>v<span class="_ _f"></span>ector<span class="ff7">(”</span>big<span class="_ _f"></span><span class="ff7">”)<span class="_ _5"> </span>+</span></span></span></div><div class="t m0 x0 hb y51 ff6 fs1 fc0 sc0 ls0 ws0">v<span class="_ _f"></span>ector<span class="ff7">(”</span>smal<span class="_ _f"></span>l<span class="ff7">”)<span class="ff2">.<span class="_ _6"> </span>Then, we<span class="_ _4"> </span>search<span class="_ _4"> </span>in<span class="_ _4"> </span>the<span class="_ _5"> </span>vector<span class="_ _4"> </span>space<span class="_ _4"> </span>for<span class="_ _4"> </span>the<span class="_ _4"> </span>w<span class="_ _1"></span>ord<span class="_ _4"> </span>closest<span class="_ _4"> </span>to<span class="_ _4"> </span><span class="ff6">X<span class="_ _6"> </span></span>measured<span class="_ _4"> </span>by<span class="_ _4"> </span>cosine</span></span></div><div class="t m0 x0 h3 yb2 ff2 fs1 fc0 sc0 ls0 ws0">distance,<span class="_ _6"> </span>and<span class="_ _7"> </span>use<span class="_ _6"> </span>it<span class="_ _6"> </span>as<span class="_ _7"> </span>the<span class="_ _6"> </span>answer<span class="_ _6"> </span>to<span class="_ _7"> </span>the<span class="_ _6"> </span>question<span class="_ _6"> </span>(we<span class="_ _6"> </span>discard<span class="_ _7"> </span>the<span class="_ _6"> </span>input<span class="_ _6"> </span>question<span class="_ _7"> </span>words<span class="_ _6"> </span>during<span class="_ _6"> </span>this</div><div class="t m0 x0 h3 yb3 ff2 fs1 fc0 sc0 ls0 ws0">search).<span class="_ _c"> </span>When<span class="_ _2"> </span>the<span class="_ _7"> </span>word<span class="_ _2"> </span>v<span class="_ _1"></span>ectors<span class="_ _7"> </span>are<span class="_ _2"> </span>well<span class="_ _2"> </span>trained,<span class="_ _2"> </span>it<span class="_ _7"> </span>is<span class="_ _2"> </span>possible<span class="_ _7"> </span>to<span class="_ _2"> </span>ﬁnd<span class="_ _7"> </span>the<span class="_ _2"> </span>correct<span class="_ _7"> </span>answer<span class="_ _2"> </span>(word</div><div class="t m0 x0 hc yb4 ff6 fs1 fc0 sc0 ls0 ws0">small<span class="_ _f"></span>est<span class="ff2">) using this method.</span></div><div class="t m0 x0 h3 yb5 ff2 fs1 fc0 sc0 ls0 ws0">Finally<span class="_ _1"></span>, we found<span class="_ _6"> </span>that when<span class="_ _6"> </span>we train<span class="_ _6"> </span>high dimensional<span class="_ _6"> </span>word vectors on<span class="_ _6"> </span>a large amount<span class="_ _6"> </span>of data,<span class="_ _6"> </span>the</div><div class="t m0 x0 h3 y55 ff2 fs1 fc0 sc0 ls0 ws0">resulting<span class="_ _6"> </span>vectors<span class="_ _6"> </span>can<span class="_ _6"> </span>be<span class="_ _6"> </span>used<span class="_ _6"> </span>to<span class="_ _6"> </span>answer<span class="_ _7"> </span>very subtle<span class="_ _6"> </span>semantic<span class="_ _7"> </span>relationships<span class="_ _6"> </span>between<span class="_ _6"> </span>words,<span class="_ _6"> </span>such<span class="_ _6"> </span>as</div><div class="t m0 x0 h3 y58 ff2 fs1 fc0 sc0 ls0 ws0">a<span class="_ _6"> </span>city<span class="_ _6"> </span>and<span class="_ _7"> </span>the<span class="_ _6"> </span>country<span class="_ _6"> </span>it<span class="_ _7"> </span>belongs<span class="_ _6"> </span>to,<span class="_ _7"> </span>e.g.<span class="_ _a"> </span>France<span class="_ _7"> </span>is<span class="_ _6"> </span>to<span class="_ _6"> </span>Paris<span class="_ _6"> </span>as<span class="_ _7"> </span>Germany<span class="_ _6"> </span>is<span class="_ _6"> </span>to<span class="_ _6"> </span>Berlin.<span class="_ _14"> </span>W<span class="_ _1"></span>ord vectors</div><div class="t m0 x0 h3 y24 ff2 fs1 fc0 sc0 ls0 ws0">with<span class="_ _7"> </span>such<span class="_ _7"> </span>semantic<span class="_ _7"> </span>relationships<span class="_ _7"> </span>could<span class="_ _7"> </span>be<span class="_ _7"> </span>used<span class="_ _7"> </span>to<span class="_ _7"> </span>improv<span class="_ _1"></span>e<span class="_ _7"> </span>many<span class="_ _6"> </span>existing<span class="_ _7"> </span>NLP<span class="_ _7"> </span>applications,<span class="_ _2"> </span>such</div><div class="t m0 x0 h3 y25 ff2 fs1 fc0 sc0 ls0 ws0">as machine translation,<span class="_ _4"> </span>information retrie<span class="_ _1"></span>v<span class="_ _1"></span>al and<span class="_ _4"> </span>question answering systems,<span class="_ _4"> </span>and may enable<span class="_ _4"> </span>other</div><div class="t m0 x0 h3 y26 ff2 fs1 fc0 sc0 ls0 ws0">future applications yet to be in<span class="_ _1"></span>vented.</div><div class="t m0 xe h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">5</div><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,648.206,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:440.792000px;bottom:236.841000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf6" class="pf w0 h0" data-page-no="6"><div class="pc pc6 w0 h0"><img class="bi x2a yb6 w5 h10" alt="" src="bg6.png"/><div class="t m0 x0 h3 yb7 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>able<span class="_ _6"> </span>1:<span class="_ _b"> </span><span class="ff5">Examples of<span class="_ _6"> </span>ﬁve<span class="_ _6"> </span>types<span class="_ _6"> </span>of semantic<span class="_ _6"> </span>and<span class="_ _6"> </span>nine<span class="_ _6"> </span>types<span class="_ _6"> </span>of<span class="_ _6"> </span>syntactic questions<span class="_ _6"> </span>in<span class="_ _6"> </span>the<span class="_ _6"> </span>Semantic-</span></div><div class="t m0 x0 h3 yb8 ff5 fs1 fc0 sc0 ls0 ws0">Syntactic W<span class="_ _3"></span>or<span class="_ _1"></span>d Relationship test set.</div><div class="t m0 xd h3 yb9 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>ype of relationship<span class="_ _1a"> </span>W<span class="_ _3"></span>ord Pair 1<span class="_ _1b"> </span>W<span class="_ _3"></span>ord Pair 2</div><div class="t m0 xd h3 y2 ff2 fs1 fc0 sc0 ls0 ws0">Common capital city<span class="_ _1c"> </span>Athens<span class="_ _1d"> </span>Greece<span class="_ _1e"> </span>Oslo<span class="_ _1f"> </span>Norway</div><div class="t m0 xd h3 yba ff2 fs1 fc0 sc0 ls0 ws0">All capital cities<span class="_ _20"> </span>Astana<span class="_ _21"> </span>Kazakhstan<span class="_ _21"> </span>Harare<span class="_ _22"> </span>Zimbabwe</div><div class="t m0 xd h3 ybb ff2 fs1 fc0 sc0 ls0 ws0">Currency<span class="_ _23"> </span>Angola<span class="_ _24"> </span>kwanza<span class="_ _1e"> </span>Iran<span class="_ _25"> </span>rial</div><div class="t m0 xd h3 ybc ff2 fs1 fc0 sc0 ls0 ws0">City-in-state<span class="_ _26"> </span>Chicago<span class="_ _27"> </span>Illinois<span class="_ _22"> </span>Stockton<span class="_ _1c"> </span>California</div><div class="t m0 xd h3 ybd ff2 fs1 fc0 sc0 ls0 ws0">Man-W<span class="_ _3"></span>oman<span class="_ _26"> </span>brother<span class="_ _28"> </span>sister<span class="_ _29"> </span>grandson<span class="_ _2a"> </span>granddaughter</div><div class="t m0 xd h3 ybe ff2 fs1 fc0 sc0 ls0 ws0">Adjectiv<span class="_ _1"></span>e to adverb<span class="_ _2b"> </span>apparent<span class="_ _2c"> </span>apparently<span class="_ _2d"> </span>rapid<span class="_ _2e"> </span>rapidly</div><div class="t m0 xd h3 ybf ff2 fs1 fc0 sc0 ls0 ws0">Opposite<span class="_ _2f"> </span>possibly<span class="_ _2c"> </span>impossibly<span class="_ _30"> </span>ethical<span class="_ _31"> </span>unethical</div><div class="t m0 xd h3 yc0 ff2 fs1 fc0 sc0 ls0 ws0">Comparativ<span class="_ _1"></span>e<span class="_ _32"> </span>great<span class="_ _33"> </span>greater<span class="_ _34"> </span>tough<span class="_ _33"> </span>tougher</div><div class="t m0 xd h3 yc1 ff2 fs1 fc0 sc0 ls0 ws0">Superlativ<span class="_ _1"></span>e<span class="_ _35"> </span>easy<span class="_ _36"> </span>easiest<span class="_ _28"> </span>lucky<span class="_ _33"> </span>luckiest</div><div class="t m0 xd h3 yc2 ff2 fs1 fc0 sc0 ls0 ws0">Present Participle<span class="_ _37"> </span>think<span class="_ _38"> </span>thinking<span class="_ _34"> </span>read<span class="_ _39"> </span>reading</div><div class="t m0 xd h3 yc3 ff2 fs1 fc0 sc0 ls0 ws0">Nationality adjectiv<span class="_ _1"></span>e<span class="_ _2a"> </span>Switzerland<span class="_ _1c"> </span>Swiss<span class="_ _2b"> </span>Cambodia<span class="_ _3a"> </span>Cambodian</div><div class="t m0 xd h3 yc4 ff2 fs1 fc0 sc0 ls0 ws0">Past tense<span class="_ _3b"> </span>walking<span class="_ _3c"> </span>walked<span class="_ _30"> </span>swimming<span class="_ _24"> </span>swam</div><div class="t m0 xd h3 yc5 ff2 fs1 fc0 sc0 ls0 ws0">Plural nouns<span class="_ _3d"> </span>mouse<span class="_ _1f"> </span>mice<span class="_ _36"> </span>dollar<span class="_ _1f"> </span>dollars</div><div class="t m0 xd h3 yc6 ff2 fs1 fc0 sc0 ls0 ws0">Plural verbs<span class="_ _3e"> </span>work<span class="_ _1f"> </span>works<span class="_ _1e"> </span>speak<span class="_ _2e"> </span>speaks</div><div class="t m0 x0 h3 yc7 ff1 fs1 fc0 sc0 ls0 ws0">4.1<span class="_ _d"> </span>T<span class="_ _3"></span>ask Description</div><div class="t m0 x0 h3 yc8 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>o<span class="_ _6"> </span>measure<span class="_ _6"> </span>quality of<span class="_ _6"> </span>the<span class="_ _6"> </span>word vectors,<span class="_ _6"> </span>we deﬁne<span class="_ _6"> </span>a<span class="_ _6"> </span>comprehensi<span class="_ _1"></span>ve test<span class="_ _6"> </span>set<span class="_ _6"> </span>that contains<span class="_ _6"> </span>ﬁv<span class="_ _1"></span>e types</div><div class="t m0 x0 h3 y40 ff2 fs1 fc0 sc0 ls0 ws0">of semantic questions,<span class="_ _6"> </span>and nine types of<span class="_ _6"> </span>syntactic questions.<span class="_ _7"> </span>T<span class="_ _1"></span>w<span class="_ _1"></span>o examples from each category are</div><div class="t m0 x0 h3 y41 ff2 fs1 fc0 sc0 ls0 ws0">shown<span class="_ _6"> </span>in<span class="_ _6"> </span>T<span class="_ _3"></span>able<span class="_ _7"> </span>1.<span class="_ _14"> </span>Overall,<span class="_ _6"> </span>there<span class="_ _7"> </span>are<span class="_ _6"> </span>8869<span class="_ _7"> </span>semantic<span class="_ _6"> </span>and<span class="_ _6"> </span>10675<span class="_ _7"> </span>syntactic<span class="_ _6"> </span>questions.<span class="_ _9"> </span>The questions</div><div class="t m0 x0 h3 y42 ff2 fs1 fc0 sc0 ls0 ws0">in<span class="_ _6"> </span>each<span class="_ _6"> </span>category were<span class="_ _6"> </span>created<span class="_ _6"> </span>in<span class="_ _6"> </span>two<span class="_ _6"> </span>steps:<span class="_ _2"> </span>ﬁrst, a<span class="_ _6"> </span>list<span class="_ _6"> </span>of<span class="_ _6"> </span>similar<span class="_ _6"> </span>word<span class="_ _6"> </span>pairs<span class="_ _6"> </span>was created<span class="_ _6"> </span>manually<span class="_ _1"></span>.</div><div class="t m0 x0 h3 y43 ff2 fs1 fc0 sc0 ls0 ws0">Then,<span class="_ _7"> </span>a<span class="_ _7"> </span>large<span class="_ _7"> </span>list<span class="_ _7"> </span>of<span class="_ _7"> </span>questions<span class="_ _7"> </span>is<span class="_ _7"> </span>formed<span class="_ _7"> </span>by<span class="_ _7"> </span>connecting<span class="_ _7"> </span>two<span class="_ _6"> </span>word<span class="_ _7"> </span>pairs.<span class="_ _9"> </span>For<span class="_ _7"> </span>example,<span class="_ _7"> </span>we<span class="_ _7"> </span>made<span class="_ _7"> </span>a</div><div class="t m0 x0 h3 y44 ff2 fs1 fc0 sc0 ls0 ws0">list<span class="_ _6"> </span>of<span class="_ _6"> </span>68<span class="_ _6"> </span>large<span class="_ _6"> </span>American<span class="_ _6"> </span>cities<span class="_ _7"> </span>and<span class="_ _6"> </span>the<span class="_ _6"> </span>states<span class="_ _6"> </span>they<span class="_ _6"> </span>belong<span class="_ _6"> </span>to,<span class="_ _7"> </span>and<span class="_ _6"> </span>formed<span class="_ _6"> </span>about<span class="_ _6"> </span>2.5K<span class="_ _7"> </span>questions<span class="_ _6"> </span>by</div><div class="t m0 x0 h3 yc9 ff2 fs1 fc0 sc0 ls0 ws0">picking<span class="_ _6"> </span>two<span class="_ _6"> </span>word<span class="_ _7"> </span>pairs<span class="_ _6"> </span>at<span class="_ _6"> </span>random.<span class="_ _14"> </span>W<span class="_ _1"></span>e<span class="_ _6"> </span>ha<span class="_ _1"></span>ve<span class="_ _6"> </span>included<span class="_ _6"> </span>in<span class="_ _7"> </span>our<span class="_ _6"> </span>test<span class="_ _7"> </span>set<span class="_ _6"> </span>only<span class="_ _6"> </span>single<span class="_ _7"> </span>token<span class="_ _6"> </span>words,<span class="_ _6"> </span>thus</div><div class="t m0 x0 h3 y12 ff2 fs1 fc0 sc0 ls0 ws0">multi-word entities are not present (such as <span class="ff5">Ne<span class="_ _1"></span>w Y<span class="_ _3"></span>ork<span class="ff2">).</span></span></div><div class="t m0 x0 h3 y47 ff2 fs1 fc0 sc0 ls0 ws0">W<span class="_ _3"></span>e<span class="_ _7"> </span>e<span class="_ _1"></span>valuate<span class="_ _6"> </span>the<span class="_ _6"> </span>overall<span class="_ _6"> </span>accuracy<span class="_ _6"> </span>for<span class="_ _7"> </span>all<span class="_ _6"> </span>question<span class="_ _7"> </span>types,<span class="_ _7"> </span>and<span class="_ _7"> </span>for<span class="_ _6"> </span>each<span class="_ _7"> </span>question<span class="_ _6"> </span>type<span class="_ _7"> </span>separately<span class="_ _6"> </span>(se-</div><div class="t m0 x0 h3 y48 ff2 fs1 fc0 sc0 ls0 ws0">mantic,<span class="_ _2"> </span>syntactic).<span class="_ _11"> </span>Question<span class="_ _7"> </span>is<span class="_ _2"> </span>assumed<span class="_ _7"> </span>to<span class="_ _2"> </span>be<span class="_ _6"> </span>correctly<span class="_ _2"> </span>answered<span class="_ _7"> </span>only<span class="_ _2"> </span>if<span class="_ _6"> </span>the<span class="_ _2"> </span>closest<span class="_ _7"> </span>word<span class="_ _7"> </span>to<span class="_ _2"> </span>the</div><div class="t m0 x0 h3 y49 ff2 fs1 fc0 sc0 ls0 ws0">vector<span class="_ _6"> </span>computed<span class="_ _2"> </span>using<span class="_ _6"> </span>the<span class="_ _7"> </span>abov<span class="_ _1"></span>e<span class="_ _7"> </span>method<span class="_ _7"> </span>is<span class="_ _7"> </span>exactly<span class="_ _7"> </span>the<span class="_ _7"> </span>same<span class="_ _7"> </span>as<span class="_ _7"> </span>the<span class="_ _7"> </span>correct<span class="_ _7"> </span>word<span class="_ _7"> </span>in<span class="_ _7"> </span>the<span class="_ _7"> </span>question;</div><div class="t m0 x0 h3 y4a ff2 fs1 fc0 sc0 ls0 ws0">synonyms<span class="_ _7"> </span>are<span class="_ _2"> </span>thus<span class="_ _2"> </span>counted<span class="_ _7"> </span>as<span class="_ _2"> </span>mistakes.<span class="_ _c"> </span>This<span class="_ _2"> </span>also<span class="_ _7"> </span>means<span class="_ _2"> </span>that<span class="_ _2"> </span>reaching<span class="_ _7"> </span>100%<span class="_ _2"> </span>accurac<span class="_ _1"></span>y<span class="_ _2"> </span>is<span class="_ _7"> </span>likely</div><div class="t m0 x0 h3 y4b ff2 fs1 fc0 sc0 ls0 ws0">to be<span class="_ _6"> </span>impossible, as<span class="_ _6"> </span>the current models<span class="_ _6"> </span>do not<span class="_ _6"> </span>hav<span class="_ _1"></span>e any input<span class="_ _6"> </span>information about word<span class="_ _6"> </span>morphology<span class="_ _3"></span>.</div><div class="t m0 x0 h3 yca ff2 fs1 fc0 sc0 ls0 ws0">Howe<span class="_ _1"></span>v<span class="_ _1"></span>er<span class="_ _1"></span>,<span class="_ _4"> </span>we<span class="_ _4"> </span>belie<span class="_ _1"></span>ve<span class="_ _4"> </span>that<span class="_ _4"> </span>usefulness<span class="_ _4"> </span>of<span class="_ _4"> </span>the<span class="_ _4"> </span>w<span class="_ _1"></span>ord<span class="_ _4"> </span>v<span class="_ _1"></span>ectors<span class="_ _4"> </span>for<span class="_ _4"> </span>certain<span class="_ _4"> </span>applications<span class="_ _4"> </span>should<span class="_ _4"> </span>be<span class="_ _4"> </span>positi<span class="_ _1"></span>vely</div><div class="t m0 x0 h3 ycb ff2 fs1 fc0 sc0 ls0 ws0">correlated with<span class="_ _4"> </span>this accurac<span class="_ _1"></span>y metric.<span class="_ _6"> </span>Further progress can<span class="_ _4"> </span>be achie<span class="_ _1"></span>v<span class="_ _1"></span>ed by<span class="_ _4"> </span>incorporating information</div><div class="t m0 x0 h3 y4c ff2 fs1 fc0 sc0 ls0 ws0">about structure of words, especially for the syntactic questions.</div><div class="t m0 x0 h3 ycc ff1 fs1 fc0 sc0 ls0 ws0">4.2<span class="_ _d"> </span>Maximization of Accuracy</div><div class="t m0 x0 h3 y4f ff2 fs1 fc0 sc0 ls0 ws0">W<span class="_ _3"></span>e<span class="_ _2"> </span>ha<span class="_ _1"></span>ve<span class="_ _2"> </span>used<span class="_ _7"> </span>a<span class="_ _2"> </span>Google<span class="_ _7"> </span>News<span class="_ _2"> </span>corpus<span class="_ _7"> </span>for<span class="_ _2"> </span>training<span class="_ _2"> </span>the<span class="_ _7"> </span>word<span class="_ _2"> </span>v<span class="_ _1"></span>ectors.<span class="_ _c"> </span>This<span class="_ _2"> </span>corpus<span class="_ _2"> </span>contains<span class="_ _7"> </span>about</div><div class="t m0 x0 h3 y50 ff2 fs1 fc0 sc0 ls0 ws0">6B<span class="_ _7"> </span>tokens.<span class="_ _9"> </span>W<span class="_ _3"></span>e<span class="_ _7"> </span>hav<span class="_ _1"></span>e<span class="_ _7"> </span>restricted<span class="_ _6"> </span>the<span class="_ _7"> </span>vocabulary<span class="_ _6"> </span>size<span class="_ _7"> </span>to<span class="_ _7"> </span>1<span class="_ _7"> </span>million<span class="_ _7"> </span>most<span class="_ _7"> </span>frequent<span class="_ _7"> </span>words.<span class="_ _9"> </span>Clearly<span class="_ _3"></span>,<span class="_ _2"> </span>we</div><div class="t m0 x0 h3 y51 ff2 fs1 fc0 sc0 ls0 ws0">are<span class="_ _6"> </span>facing<span class="_ _7"> </span>time<span class="_ _6"> </span>constrained<span class="_ _7"> </span>optimization<span class="_ _6"> </span>problem,<span class="_ _7"> </span>as<span class="_ _6"> </span>it<span class="_ _7"> </span>can<span class="_ _6"> </span>be<span class="_ _7"> </span>expected<span class="_ _6"> </span>that<span class="_ _6"> </span>both<span class="_ _7"> </span>using<span class="_ _6"> </span>more<span class="_ _7"> </span>data</div><div class="t m0 x0 h3 yb2 ff2 fs1 fc0 sc0 ls0 ws0">and<span class="_ _2"> </span>higher<span class="_ _2"> </span>dimensional<span class="_ _7"> </span>word<span class="_ _2"> </span>v<span class="_ _1"></span>ectors<span class="_ _2"> </span>will<span class="_ _2"> </span>impro<span class="_ _1"></span>v<span class="_ _1"></span>e<span class="_ _2"> </span>the<span class="_ _2"> </span>accurac<span class="_ _1"></span>y<span class="_ _3"></span>.<span class="_ _12"> </span>T<span class="_ _0"></span>o<span class="_ _2"> </span>estimate<span class="_ _2"> </span>the<span class="_ _2"> </span>best<span class="_ _7"> </span>choice<span class="_ _2"> </span>of</div><div class="t m0 x0 h3 yb3 ff2 fs1 fc0 sc0 ls0 ws0">model architecture for obtaining<span class="_ _6"> </span>as good as possible results<span class="_ _6"> </span>quickly<span class="_ _3"></span>,<span class="_ _6"> </span>we hav<span class="_ _1"></span>e ﬁrst ev<span class="_ _1"></span>aluated models</div><div class="t m0 x0 h3 yb4 ff2 fs1 fc0 sc0 ls0 ws0">trained<span class="_ _6"> </span>on<span class="_ _7"> </span>subsets<span class="_ _7"> </span>of<span class="_ _6"> </span>the<span class="_ _7"> </span>training<span class="_ _6"> </span>data,<span class="_ _7"> </span>with<span class="_ _7"> </span>vocab<span class="_ _1"></span>ulary<span class="_ _6"> </span>restricted<span class="_ _7"> </span>to<span class="_ _6"> </span>the<span class="_ _7"> </span>most<span class="_ _7"> </span>frequent<span class="_ _6"> </span>30k<span class="_ _7"> </span>words.</div><div class="t m0 x0 h3 ycd ff2 fs1 fc0 sc0 ls0 ws0">The<span class="_ _6"> </span>results<span class="_ _7"> </span>using<span class="_ _6"> </span>the<span class="_ _7"> </span>CBO<span class="_ _1"></span>W<span class="_ _6"> </span>architecture<span class="_ _7"> </span>with<span class="_ _6"> </span>different<span class="_ _6"> </span>choice<span class="_ _6"> </span>of<span class="_ _7"> </span>word<span class="_ _6"> </span>vector<span class="_ _6"> </span>dimensionality<span class="_ _7"> </span>and</div><div class="t m0 x0 h3 yce ff2 fs1 fc0 sc0 ls0 ws0">increasing amount of the training data are shown in T<span class="_ _0"></span>able 2.</div><div class="t m0 x0 h3 y58 ff2 fs1 fc0 sc0 ls0 ws0">It can be seen<span class="_ _6"> </span>that after some point,<span class="_ _6"> </span>adding more dimensions or<span class="_ _6"> </span>adding more training data provides</div><div class="t m0 x0 h3 y24 ff2 fs1 fc0 sc0 ls0 ws0">diminishing<span class="_ _2"> </span>impro<span class="_ _1"></span>vements.<span class="_ _c"> </span>So,<span class="_ _a"> </span>we<span class="_ _2"> </span>ha<span class="_ _1"></span>v<span class="_ _1"></span>e<span class="_ _2"> </span>to<span class="_ _2"> </span>increase<span class="_ _7"> </span>both<span class="_ _2"> </span>vector<span class="_ _7"> </span>dimensionality<span class="_ _2"> </span>and<span class="_ _2"> </span>the<span class="_ _2"> </span>amount</div><div class="t m0 x0 h3 y25 ff2 fs1 fc0 sc0 ls0 ws0">of<span class="_ _7"> </span>the<span class="_ _6"> </span>training<span class="_ _7"> </span>data<span class="_ _7"> </span>together<span class="_ _1"></span>.<span class="_ _14"> </span>While<span class="_ _7"> </span>this<span class="_ _7"> </span>observ<span class="_ _1"></span>ation<span class="_ _6"> </span>might<span class="_ _7"> </span>seem<span class="_ _7"> </span>trivial,<span class="_ _6"> </span>it<span class="_ _7"> </span>must<span class="_ _7"> </span>be<span class="_ _6"> </span>noted<span class="_ _7"> </span>that<span class="_ _7"> </span>it<span class="_ _7"> </span>is</div><div class="t m0 x0 h3 y26 ff2 fs1 fc0 sc0 ls0 ws0">currently popular to train<span class="_ _4"> </span>word v<span class="_ _1"></span>ectors on relati<span class="_ _1"></span>v<span class="_ _1"></span>ely large<span class="_ _4"> </span>amounts of data, b<span class="_ _1"></span>ut with<span class="_ _4"> </span>insufﬁcient<span class="_ _4"> </span>size</div><div class="t m0 xe h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">6</div><a class="l" href="#pf6" data-dest-detail='[6,"XYZ",144.122,704.315,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:170.931000px;bottom:380.762000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf7" data-dest-detail='[7,"XYZ",147.159,704.315,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:337.874000px;bottom:108.298000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf7" class="pf w0 h0" data-page-no="7"><div class="pc pc7 w0 h0"><img class="bi x0 y25 w6 h11" alt="" src="bg7.png"/><div class="t m0 x0 h3 yb7 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>able<span class="_ _2"> </span>2:<span class="_ _8"> </span><span class="ff5">Accur<span class="_ _1"></span>acy<span class="_ _2"> </span>on<span class="_ _7"> </span>subset<span class="_ _2"> </span>of<span class="_ _2"> </span>the<span class="_ _2"> </span>Semantic-Syntactic<span class="_ _2"> </span>W<span class="_ _3"></span>or<span class="_ _1"></span>d<span class="_ _2"> </span>Relationship<span class="_ _2"> </span>test<span class="_ _2"> </span>set,<span class="_ _2"> </span>using<span class="_ _2"> </span>wor<span class="_ _1"></span>d</span></div><div class="t m0 x0 h3 yb8 ff5 fs1 fc0 sc0 ls0 ws0">vectors<span class="_ _4"> </span>fr<span class="_ _1"></span>om<span class="_ _4"> </span>the<span class="_ _4"> </span>CBO<span class="_ _1"></span>W<span class="_ _4"> </span>ar<span class="_ _1"></span>c<span class="_ _1"></span>hitectur<span class="_ _1"></span>e<span class="_ _4"> </span>with<span class="_ _4"> </span>limited vocab<span class="_ _1"></span>ulary<span class="_ _1"></span>. Only questions<span class="_ _4"> </span>containing wor<span class="_ _1"></span>ds<span class="_ _4"> </span>fr<span class="_ _1"></span>om</div><div class="t m0 x0 h3 y2c ff5 fs1 fc0 sc0 ls0 ws0">the most fr<span class="_ _1"></span>equent 30k wor<span class="_ _1"></span>ds ar<span class="_ _1"></span>e used.</div><div class="t m0 x2b h3 ycf ff2 fs1 fc0 sc0 ls0 ws0">Dimensionality / T<span class="_ _1"></span>raining words<span class="_ _3f"> </span>24M<span class="_ _8"> </span>49M<span class="_ _8"> </span>98M<span class="_ _8"> </span>196M<span class="_ _8"> </span>391M<span class="_ _8"> </span>783M</div><div class="t m0 x2c h3 yd0 ff2 fs1 fc0 sc0 ls0 ws0">50<span class="_ _1b"> </span>13.4<span class="_ _40"> </span>15.7<span class="_ _3f"> </span>18.6<span class="_ _41"> </span>19.1<span class="_ _3a"> </span>22.5<span class="_ _3a"> </span>23.2</div><div class="t m0 x2d h3 yd1 ff2 fs1 fc0 sc0 ls0 ws0">100<span class="_ _42"> </span>19.4<span class="_ _3f"> </span>23.1<span class="_ _40"> </span>27.8<span class="_ _43"> </span>28.7<span class="_ _3a"> </span>33.4<span class="_ _3a"> </span>32.2</div><div class="t m0 x2d h3 yd2 ff2 fs1 fc0 sc0 ls0 ws0">300<span class="_ _42"> </span>23.2<span class="_ _3f"> </span>29.2<span class="_ _40"> </span>35.3<span class="_ _43"> </span>38.6<span class="_ _3a"> </span>43.7<span class="_ _3a"> </span>45.9</div><div class="t m0 x2d h3 yd3 ff2 fs1 fc0 sc0 ls0 ws0">600<span class="_ _42"> </span>24.0<span class="_ _3f"> </span>30.1<span class="_ _40"> </span>36.5<span class="_ _43"> </span>40.8<span class="_ _3a"> </span>46.6<span class="_ _3a"> </span>50.4</div><div class="t m0 x0 h3 yd4 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>able 3:<span class="_ _7"> </span><span class="ff5">Comparison of ar<span class="_ _1"></span>c<span class="_ _1"></span>hitectur<span class="_ _1"></span>es using<span class="_ _4"> </span>models trained<span class="_ _4"> </span>on the same data, with 640-dimensional</span></div><div class="t m0 x0 h3 yc0 ff5 fs1 fc0 sc0 ls0 ws0">wor<span class="_ _1"></span>d<span class="_ _7"> </span>vectors.<span class="_ _9"> </span>The<span class="_ _7"> </span>accuracies<span class="_ _7"> </span>ar<span class="_ _1"></span>e<span class="_ _6"> </span>reported<span class="_ _7"> </span>on<span class="_ _7"> </span>our<span class="_ _7"> </span>Semantic-Syntactic<span class="_ _7"> </span>W<span class="_ _3"></span>or<span class="_ _1"></span>d<span class="_ _7"> </span>Relationship<span class="_ _7"> </span>test<span class="_ _7"> </span>set,</div><div class="t m0 x0 h3 yd5 ff5 fs1 fc0 sc0 ls0 ws0">and on the syntactic r<span class="_ _1"></span>elationship test set of [20]</div><div class="t m0 x2e h3 yd6 ff2 fs1 fc0 sc0 ls0 ws0">Model<span class="_ _1e"> </span>Semantic-Syntactic W<span class="_ _3"></span>ord Relationship test set<span class="_ _30"> </span>MSR W<span class="_ _3"></span>ord Relatedness</div><div class="t m0 x13 h3 yd7 ff2 fs1 fc0 sc0 ls0 ws0">Architecture<span class="_ _3f"> </span>Semantic Accuracy [%]<span class="_ _8"> </span>Syntactic Accuracy [%]<span class="_ _1f"> </span>T<span class="_ _3"></span>est Set [20]</div><div class="t m0 x2f h3 yd8 ff2 fs1 fc0 sc0 ls0 ws0">RNNLM<span class="_ _3b"> </span>9<span class="_ _44"> </span>36<span class="_ _44"> </span>35</div><div class="t m0 x1 h3 yd9 ff2 fs1 fc0 sc0 ls0 ws0">NNLM<span class="_ _45"> </span>23<span class="_ _46"> </span>53<span class="_ _44"> </span>47</div><div class="t m0 x1 h3 yda ff2 fs1 fc0 sc0 ls0 ws0">CBO<span class="_ _1"></span>W<span class="_ _45"> </span>24<span class="_ _46"> </span>64<span class="_ _47"> </span>61</div><div class="t m0 x14 h3 ydb ff2 fs1 fc0 sc0 ls0 ws0">Skip-gram<span class="_ _32"> </span>55<span class="_ _46"> </span>59<span class="_ _44"> </span>56</div><div class="t m0 x0 h3 ydc ff2 fs1 fc0 sc0 ls0 ws0">(such<span class="_ _6"> </span>as 50<span class="_ _6"> </span>-<span class="_ _6"> </span>100).<span class="_ _2"> </span>Giv<span class="_ _1"></span>en<span class="_ _6"> </span>Equation 4,<span class="_ _6"> </span>increasing<span class="_ _6"> </span>amount<span class="_ _6"> </span>of<span class="_ _6"> </span>training data<span class="_ _6"> </span>twice<span class="_ _6"> </span>results<span class="_ _6"> </span>in<span class="_ _6"> </span>about the</div><div class="t m0 x0 h3 y92 ff2 fs1 fc0 sc0 ls0 ws0">same increase of computational complexity as increasing v<span class="_ _1"></span>ector size twice.</div><div class="t m0 x0 h3 ydd ff2 fs1 fc0 sc0 ls0 ws0">For the<span class="_ _4"> </span>experiments reported<span class="_ _4"> </span>in T<span class="_ _3"></span>ables 2 and 4, we used<span class="_ _4"> </span>three training epochs with<span class="_ _4"> </span>stochastic gradi-</div><div class="t m0 x0 h3 yde ff2 fs1 fc0 sc0 ls0 ws0">ent descent and backpropagation.<span class="_ _6"> </span>W<span class="_ _3"></span>e chose starting learning rate 0.025 and decreased<span class="_ _4"> </span>it linearly<span class="_ _3"></span>, so</div><div class="t m0 x0 h3 ydf ff2 fs1 fc0 sc0 ls0 ws0">that it approaches zero at the end of the last training epoch.</div><div class="t m0 x0 h3 ye0 ff1 fs1 fc0 sc0 ls0 ws0">4.3<span class="_ _d"> </span>Comparison of Model Architectur<span class="_ _1"></span>es</div><div class="t m0 x0 h3 ye1 ff2 fs1 fc0 sc0 ls0 ws0">First<span class="_ _4"> </span>we<span class="_ _4"> </span>compare dif<span class="_ _1"></span>ferent<span class="_ _4"> </span>model<span class="_ _4"> </span>architectures<span class="_ _4"> </span>for<span class="_ _4"> </span>deri<span class="_ _1"></span>ving<span class="_ _4"> </span>the<span class="_ _4"> </span>word<span class="_ _4"> </span>vectors<span class="_ _4"> </span>using<span class="_ _4"> </span>the<span class="_ _4"> </span>same<span class="_ _4"> </span>training</div><div class="t m0 x0 h3 ye2 ff2 fs1 fc0 sc0 ls0 ws0">data<span class="_ _6"> </span>and<span class="_ _6"> </span>using the<span class="_ _6"> </span>same<span class="_ _6"> </span>dimensionality<span class="_ _6"> </span>of<span class="_ _6"> </span>640<span class="_ _6"> </span>of<span class="_ _6"> </span>the word<span class="_ _6"> </span>vectors.<span class="_ _2"> </span>In<span class="_ _6"> </span>the<span class="_ _6"> </span>further<span class="_ _6"> </span>experiments, we</div><div class="t m0 x0 h3 y94 ff2 fs1 fc0 sc0 ls0 ws0">use<span class="_ _4"> </span>full<span class="_ _4"> </span>set<span class="_ _5"> </span>of<span class="_ _4"> </span>questions<span class="_ _4"> </span>in<span class="_ _4"> </span>the<span class="_ _5"> </span>new<span class="_ _4"> </span>Semantic-Syntactic<span class="_ _5"> </span>W<span class="_ _1"></span>ord<span class="_ _5"> </span>Relationship<span class="_ _4"> </span>test<span class="_ _4"> </span>set,<span class="_ _4"> </span>i.e. unrestricted to</div><div class="t m0 x0 h3 ye3 ff2 fs1 fc0 sc0 ls0 ws0">the 30k v<span class="_ _1"></span>ocabulary<span class="_ _3"></span>.<span class="_ _7"> </span>W<span class="_ _3"></span>e also include results on a test set introduced<span class="_ _4"> </span>in [20] that focuses on syntactic</div><div class="t m0 x0 h3 ye4 ff2 fs1 fc0 sc0 ls0 ws0">similarity between words</div><div class="t m0 x30 h8 yad ff2 fs4 fc0 sc0 ls0 ws0">3</div><div class="t m0 x31 h3 ye4 ff2 fs1 fc0 sc0 ls0 ws0">.</div><div class="t m0 x0 h3 y18 ff2 fs1 fc0 sc0 ls0 ws0">The<span class="_ _6"> </span>training<span class="_ _6"> </span>data<span class="_ _7"> </span>consists<span class="_ _6"> </span>of<span class="_ _6"> </span>several<span class="_ _6"> </span>LDC<span class="_ _6"> </span>corpora<span class="_ _6"> </span>and<span class="_ _7"> </span>is<span class="_ _6"> </span>described<span class="_ _6"> </span>in<span class="_ _7"> </span>detail<span class="_ _6"> </span>in<span class="_ _6"> </span>[18]<span class="_ _7"> </span>(320M<span class="_ _6"> </span>words,</div><div class="t m0 x0 h3 ye5 ff2 fs1 fc0 sc0 ls0 ws0">82K<span class="_ _2"> </span>v<span class="_ _1"></span>ocab<span class="_ _1"></span>ulary).<span class="_ _12"> </span>W<span class="_ _0"></span>e<span class="_ _2"> </span>used<span class="_ _2"> </span>these<span class="_ _7"> </span>data<span class="_ _2"> </span>to<span class="_ _2"> </span>pro<span class="_ _1"></span>vide<span class="_ _7"> </span>a<span class="_ _2"> </span>comparison<span class="_ _2"> </span>to<span class="_ _2"> </span>a<span class="_ _7"> </span>previously<span class="_ _7"> </span>trained<span class="_ _2"> </span>recurrent</div><div class="t m0 x0 h3 ye6 ff2 fs1 fc0 sc0 ls0 ws0">neural netw<span class="_ _1"></span>ork language<span class="_ _4"> </span>model<span class="_ _4"> </span>that took<span class="_ _4"> </span>about 8<span class="_ _4"> </span>weeks<span class="_ _4"> </span>to train<span class="_ _4"> </span>on<span class="_ _4"> </span>a single<span class="_ _4"> </span>CPU.<span class="_ _4"> </span>W<span class="_ _3"></span>e trained<span class="_ _4"> </span>a feed-</div><div class="t m0 x0 h3 ye7 ff2 fs1 fc0 sc0 ls0 ws0">forward<span class="_ _4"> </span>NNLM with<span class="_ _4"> </span>the same<span class="_ _4"> </span>number of<span class="_ _4"> </span>640 hidden<span class="_ _4"> </span>units using<span class="_ _4"> </span>the DistBelief<span class="_ _4"> </span>parallel training<span class="_ _4"> </span>[6],</div><div class="t m0 x0 h3 ye8 ff2 fs1 fc0 sc0 ls0 ws0">using a history of<span class="_ _4"> </span>8 pre<span class="_ _1"></span>vious words<span class="_ _4"> </span>(thus, the NNLM has<span class="_ _4"> </span>more parameters than the<span class="_ _4"> </span>RNNLM, as the</div><div class="t m0 x0 hb ye9 ff2 fs1 fc0 sc0 ls0 ws0">projection layer has size <span class="ff7">640<span class="_ _4"> </span><span class="ff8">×<span class="_ _4"> </span></span>8</span>).</div><div class="t m0 x0 h3 y1e ff2 fs1 fc0 sc0 ls0 ws0">In T<span class="_ _3"></span>able 3, it can be seen that the word v<span class="_ _1"></span>ectors from the RNN (as used<span class="_ _4"> </span>in [20]) perform well mostly</div><div class="t m0 x0 h3 yea ff2 fs1 fc0 sc0 ls0 ws0">on<span class="_ _6"> </span>the<span class="_ _6"> </span>syntactic<span class="_ _7"> </span>questions.<span class="_ _14"> </span>The<span class="_ _6"> </span>NNLM<span class="_ _6"> </span>vectors<span class="_ _6"> </span>perform<span class="_ _7"> </span>signiﬁcantly<span class="_ _6"> </span>better<span class="_ _6"> </span>than<span class="_ _7"> </span>the<span class="_ _6"> </span>RNN<span class="_ _6"> </span>-<span class="_ _7"> </span>this<span class="_ _6"> </span>is</div><div class="t m0 x0 h3 yeb ff2 fs1 fc0 sc0 ls0 ws0">not<span class="_ _7"> </span>surprising,<span class="_ _2"> </span>as<span class="_ _2"> </span>the<span class="_ _7"> </span>word<span class="_ _7"> </span>vectors<span class="_ _7"> </span>in<span class="_ _2"> </span>the<span class="_ _7"> </span>RNNLM<span class="_ _2"> </span>are<span class="_ _6"> </span>directly<span class="_ _2"> </span>connected<span class="_ _7"> </span>to<span class="_ _2"> </span>a<span class="_ _7"> </span>non-linear<span class="_ _7"> </span>hidden</div><div class="t m0 x0 h3 yec ff2 fs1 fc0 sc0 ls0 ws0">layer<span class="_ _1"></span>.<span class="_ _14"> </span>The<span class="_ _6"> </span>CBOW<span class="_ _6"> </span>architecture<span class="_ _7"> </span>works<span class="_ _6"> </span>better<span class="_ _6"> </span>than<span class="_ _7"> </span>the<span class="_ _7"> </span>NNLM<span class="_ _6"> </span>on<span class="_ _7"> </span>the<span class="_ _6"> </span>syntactic<span class="_ _7"> </span>tasks,<span class="_ _7"> </span>and<span class="_ _6"> </span>about<span class="_ _7"> </span>the</div><div class="t m0 x0 h3 yed ff2 fs1 fc0 sc0 ls0 ws0">same on the semantic<span class="_ _4"> </span>one.<span class="_ _6"> </span>Finally<span class="_ _1"></span>, the<span class="_ _4"> </span>Skip-gram architecture w<span class="_ _1"></span>orks slightly worse<span class="_ _4"> </span>on the syntactic</div><div class="t m0 x0 h3 yee ff2 fs1 fc0 sc0 ls0 ws0">task than<span class="_ _6"> </span>the<span class="_ _6"> </span>CBO<span class="_ _1"></span>W model<span class="_ _6"> </span>(but still better<span class="_ _6"> </span>than<span class="_ _6"> </span>the NNLM),<span class="_ _6"> </span>and much<span class="_ _6"> </span>better on<span class="_ _6"> </span>the<span class="_ _6"> </span>semantic part</div><div class="t m0 x0 h3 y29 ff2 fs1 fc0 sc0 ls0 ws0">of the test than all the other models.</div><div class="t m0 x0 h3 yef ff2 fs1 fc0 sc0 ls0 ws0">Next,<span class="_ _4"> </span>we<span class="_ _5"> </span>ev<span class="_ _1"></span>aluated<span class="_ _4"> </span>our<span class="_ _5"> </span>models<span class="_ _4"> </span>trained<span class="_ _5"> </span>using<span class="_ _4"> </span>one<span class="_ _4"> </span>CPU<span class="_ _5"> </span>only<span class="_ _4"> </span>and<span class="_ _4"> </span>compared<span class="_ _5"> </span>the<span class="_ _4"> </span>results<span class="_ _5"> </span>against<span class="_ _4"> </span>publicly</div><div class="t m0 x0 h3 yf0 ff2 fs1 fc0 sc0 ls0 ws0">av<span class="_ _1"></span>ailable<span class="_ _4"> </span>w<span class="_ _1"></span>ord<span class="_ _4"> </span>v<span class="_ _1"></span>ectors.<span class="_ _6"> </span>The<span class="_ _4"> </span>compari<span class="_ _1"></span>son<span class="_ _4"> </span>is<span class="_ _5"> </span>given<span class="_ _5"> </span>in<span class="_ _4"> </span>T<span class="_ _3"></span>able<span class="_ _4"> </span>4.<span class="_ _6"> </span>The<span class="_ _4"> </span>CBO<span class="_ _1"></span>W<span class="_ _5"> </span>model<span class="_ _4"> </span>was<span class="_ _4"> </span>trained<span class="_ _5"> </span>on<span class="_ _4"> </span>subset</div><div class="t m0 x13 h9 yf1 ff2 fs5 fc0 sc0 ls0 ws0">3</div><div class="t m0 x14 ha y26 ff2 fs6 fc0 sc0 ls0 ws0">W<span class="_ _3"></span>e thank Geoff Zweig for providing us the test set.</div><div class="t m0 xe h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">7</div><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,648.206,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:283.249000px;bottom:535.952000px;width:10.955000px;height:7.752000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,648.206,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:457.688000px;bottom:502.557000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf4" data-dest-detail='[4,"XYZ",246.205,302.482,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:250.885000px;bottom:406.494000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf7" data-dest-detail='[7,"XYZ",147.159,704.315,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:262.166000px;bottom:378.599000px;width:5.973000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf8" data-dest-detail='[8,"XYZ",143.492,704.315,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:286.380000px;bottom:378.599000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,648.206,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:389.217000px;bottom:276.986000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf7" data-dest-detail='[7,"XYZ",124.139,72.101,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:207.125000px;bottom:264.009000px;width:4.978000px;height:11.521000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,710.138,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:429.416000px;bottom:249.090000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,419.755,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:491.714000px;bottom:216.214000px;width:5.974000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf7" data-dest-detail='[7,"XYZ",143.131,570.409,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:141.616000px;bottom:175.342000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,648.206,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:402.145000px;bottom:177.359000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf8" data-dest-detail='[8,"XYZ",143.492,704.315,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:332.324000px;bottom:81.693000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf8" class="pf w0 h0" data-page-no="8"><div class="pc pc8 w0 h0"><img class="bi x32 yf2 w7 h12" alt="" src="bg8.png"/><div class="t m0 x0 h3 yb7 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>able 4:<span class="_ _12"> </span><span class="ff5">Comparison of publicly available wor<span class="_ _1"></span>d vectors on the Semantic-Syntactic<span class="_ _6"> </span>W<span class="_ _3"></span>or<span class="_ _1"></span>d Relation-</span></div><div class="t m0 x0 h3 yb8 ff5 fs1 fc0 sc0 ls0 ws0">ship test set, and wor<span class="_ _1"></span>d vectors fr<span class="_ _1"></span>om<span class="_ _4"> </span>our models.<span class="_ _6"> </span>Full vocabularies ar<span class="_ _1"></span>e used.</div><div class="t m0 x1 h3 yb9 ff2 fs1 fc0 sc0 ls0 ws0">Model<span class="_ _48"> </span>V<span class="_ _0"></span>ector<span class="_ _31"> </span>T<span class="_ _3"></span>raining<span class="_ _49"> </span>Accuracy [%]</div><div class="t m0 x22 h3 y2 ff2 fs1 fc0 sc0 ls0 ws0">Dimensionality<span class="_ _4a"> </span>words</div><div class="t m0 x33 h3 yba ff2 fs1 fc0 sc0 ls0 ws0">Semantic<span class="_ _8"> </span>Syntactic<span class="_ _8"> </span>T<span class="_ _3"></span>otal</div><div class="t m0 x1 h3 ybb ff2 fs1 fc0 sc0 ls0 ws0">Collobert-W<span class="_ _3"></span>eston NNLM<span class="_ _4b"> </span>50<span class="_ _4c"> </span>660M<span class="_ _4d"> </span>9.3<span class="_ _28"> </span>12.3<span class="_ _1c"> </span>11.0</div><div class="t m0 x1 h3 ybc ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _1"></span>urian NNLM<span class="_ _4e"> </span>50<span class="_ _49"> </span>37M<span class="_ _28"> </span>1.4<span class="_ _1f"> </span>2.6<span class="_ _27"> </span>2.1</div><div class="t m0 x1 h3 ybd ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _1"></span>urian NNLM<span class="_ _4f"> </span>200<span class="_ _4c"> </span>37M<span class="_ _28"> </span>1.4<span class="_ _1f"> </span>2.2<span class="_ _27"> </span>1.8</div><div class="t m0 x1 h3 ybe ff2 fs1 fc0 sc0 ls0 ws0">Mnih NNLM<span class="_ _50"> </span>50<span class="_ _49"> </span>37M<span class="_ _28"> </span>1.8<span class="_ _1f"> </span>9.1<span class="_ _27"> </span>5.8</div><div class="t m0 x1 h3 ybf ff2 fs1 fc0 sc0 ls0 ws0">Mnih NNLM<span class="_ _51"> </span>100<span class="_ _4c"> </span>37M<span class="_ _28"> </span>3.3<span class="_ _28"> </span>13.2<span class="_ _2b"> </span>8.8</div><div class="t m0 x1 h3 yc0 ff2 fs1 fc0 sc0 ls0 ws0">Mikolov RNNLM<span class="_ _23"> </span>80<span class="_ _4c"> </span>320M<span class="_ _4d"> </span>4.9<span class="_ _28"> </span>18.4<span class="_ _52"> </span>12.7</div><div class="t m0 x1 h3 yc1 ff2 fs1 fc0 sc0 ls0 ws0">Mikolov RNNLM<span class="_ _45"> </span>640<span class="_ _53"> </span>320M<span class="_ _4d"> </span>8.6<span class="_ _28"> </span>36.5<span class="_ _1c"> </span>24.6</div><div class="t m0 x1 h3 yc2 ff2 fs1 fc0 sc0 ls0 ws0">Huang NNLM<span class="_ _4e"> </span>50<span class="_ _4c"> </span>990M<span class="_ _3c"> </span>13.3<span class="_ _4d"> </span>11.6<span class="_ _1c"> </span>12.3</div><div class="t m0 x1 h3 yc3 ff2 fs1 fc0 sc0 ls0 ws0">Our NNLM<span class="_ _54"> </span>20<span class="_ _55"> </span>6B<span class="_ _1e"> </span>12.9<span class="_ _38"> </span>26.4<span class="_ _30"> </span>20.3</div><div class="t m0 x1 h3 yc4 ff2 fs1 fc0 sc0 ls0 ws0">Our NNLM<span class="_ _54"> </span>50<span class="_ _55"> </span>6B<span class="_ _1e"> </span>27.9<span class="_ _38"> </span>55.8<span class="_ _30"> </span>43.2</div><div class="t m0 x1 h3 yc5 ff2 fs1 fc0 sc0 ls0 ws0">Our NNLM<span class="_ _56"> </span>100<span class="_ _1a"> </span>6B<span class="_ _1e"> </span>34.2<span class="_ _4d"> </span><span class="ff1">64.5<span class="_ _52"> </span></span>50.8</div><div class="t m0 x1 h3 yc6 ff2 fs1 fc0 sc0 ls0 ws0">CBO<span class="_ _1"></span>W<span class="_ _57"> </span>300<span class="_ _53"> </span>783M<span class="_ _3c"> </span>15.5<span class="_ _38"> </span>53.1<span class="_ _30"> </span>36.1</div><div class="t m0 x1 h3 yf3 ff2 fs1 fc0 sc0 ls0 ws0">Skip-gram<span class="_ _58"> </span>300<span class="_ _53"> </span>783M<span class="_ _3c"> </span><span class="ff1">50.0<span class="_ _38"> </span></span>55.9<span class="_ _30"> </span><span class="ff1">53.3</span></div><div class="t m0 x0 h3 yf4 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>able<span class="_ _6"> </span>5:<span class="_ _b"> </span><span class="ff5">Comparison of<span class="_ _6"> </span>models<span class="_ _6"> </span>trained for<span class="_ _6"> </span>three epochs<span class="_ _6"> </span>on the<span class="_ _6"> </span>same<span class="_ _6"> </span>data<span class="_ _6"> </span>and<span class="_ _6"> </span>models<span class="_ _6"> </span>trained for</span></div><div class="t m0 x0 h3 yf5 ff5 fs1 fc0 sc0 ls0 ws0">one epoch.<span class="_ _6"> </span>Accuracy is r<span class="_ _1"></span>eported on the full Semantic-Syntactic data set.</div><div class="t m0 x0 h3 yf6 ff2 fs1 fc0 sc0 ls0 ws0">Model<span class="_ _4f"> </span>V<span class="_ _0"></span>ector<span class="_ _31"> </span>T<span class="_ _3"></span>raining<span class="_ _49"> </span>Accuracy [%]<span class="_ _49"> </span>T<span class="_ _3"></span>raining time</div><div class="t m0 x2d h3 yf7 ff2 fs1 fc0 sc0 ls0 ws0">Dimensionality<span class="_ _4a"> </span>words<span class="_ _59"> </span>[days]</div><div class="t m0 x34 h3 yf8 ff2 fs1 fc0 sc0 ls0 ws0">Semantic<span class="_ _8"> </span>Syntactic<span class="_ _8"> </span>T<span class="_ _3"></span>otal</div><div class="t m0 x0 h3 yc9 ff2 fs1 fc0 sc0 ls0 ws0">3 epoch CBO<span class="_ _1"></span>W<span class="_ _5a"> </span>300<span class="_ _53"> </span>783M<span class="_ _3c"> </span>15.5<span class="_ _38"> </span>53.1<span class="_ _30"> </span>36.1<span class="_ _4b"> </span>1</div><div class="t m0 x0 h3 yf9 ff2 fs1 fc0 sc0 ls0 ws0">3 epoch Skip-gram<span class="_ _2e"> </span>300<span class="_ _53"> </span>783M<span class="_ _27"> </span>50.0<span class="_ _38"> </span>55.9<span class="_ _30"> </span>53.3<span class="_ _4b"> </span>3</div><div class="t m0 x0 h3 ye1 ff2 fs1 fc0 sc0 ls0 ws0">1 epoch CBO<span class="_ _1"></span>W<span class="_ _5a"> </span>300<span class="_ _53"> </span>783M<span class="_ _3c"> </span>13.8<span class="_ _38"> </span>49.9<span class="_ _30"> </span>33.6<span class="_ _1f"> </span>0.3</div><div class="t m0 x0 h3 yfa ff2 fs1 fc0 sc0 ls0 ws0">1 epoch CBO<span class="_ _1"></span>W<span class="_ _5a"> </span>300<span class="_ _4c"> </span>1.6B<span class="_ _4d"> </span>16.1<span class="_ _4d"> </span>52.6<span class="_ _52"> </span>36.1<span class="_ _1f"> </span>0.6</div><div class="t m0 x0 h3 yfb ff2 fs1 fc0 sc0 ls0 ws0">1 epoch CBO<span class="_ _1"></span>W<span class="_ _5a"> </span>600<span class="_ _53"> </span>783M<span class="_ _3c"> </span>15.4<span class="_ _38"> </span>53.3<span class="_ _30"> </span>36.2<span class="_ _1f"> </span>0.7</div><div class="t m0 x0 h3 yfc ff2 fs1 fc0 sc0 ls0 ws0">1 epoch Skip-gram<span class="_ _2e"> </span>300<span class="_ _53"> </span>783M<span class="_ _27"> </span>45.6<span class="_ _38"> </span>52.2<span class="_ _30"> </span>49.2<span class="_ _4b"> </span>1</div><div class="t m0 x0 h3 yfd ff2 fs1 fc0 sc0 ls0 ws0">1 epoch Skip-gram<span class="_ _2e"> </span>300<span class="_ _20"> </span>1.6B<span class="_ _4d"> </span>52.2<span class="_ _38"> </span>55.1<span class="_ _30"> </span>53.8<span class="_ _4b"> </span>2</div><div class="t m0 x0 h3 ye5 ff2 fs1 fc0 sc0 ls0 ws0">1 epoch Skip-gram<span class="_ _2e"> </span>600<span class="_ _53"> </span>783M<span class="_ _27"> </span>56.7<span class="_ _38"> </span>54.5<span class="_ _30"> </span>55.5<span class="_ _1f"> </span>2.5</div><div class="t m0 x0 h3 yfe ff2 fs1 fc0 sc0 ls0 ws0">of<span class="_ _4"> </span>the<span class="_ _4"> </span>Google<span class="_ _4"> </span>Ne<span class="_ _1"></span>ws<span class="_ _5"> </span>data in<span class="_ _4"> </span>about<span class="_ _5"> </span>a day<span class="_ _3"></span>, while<span class="_ _4"> </span>training<span class="_ _5"> </span>time for<span class="_ _4"> </span>the<span class="_ _5"> </span>Skip-gram<span class="_ _4"> </span>model<span class="_ _4"> </span>was<span class="_ _4"> </span>about<span class="_ _5"> </span>three</div><div class="t m0 x0 h3 yff ff2 fs1 fc0 sc0 ls0 ws0">days.</div><div class="t m0 x0 h3 y100 ff2 fs1 fc0 sc0 ls0 ws0">For experiments<span class="_ _6"> </span>reported<span class="_ _6"> </span>further<span class="_ _1"></span>,<span class="_ _6"> </span>we<span class="_ _6"> </span>used<span class="_ _6"> </span>just<span class="_ _6"> </span>one<span class="_ _6"> </span>training<span class="_ _6"> </span>epoch<span class="_ _6"> </span>(again,<span class="_ _6"> </span>we<span class="_ _6"> </span>decrease<span class="_ _6"> </span>the<span class="_ _6"> </span>learning</div><div class="t m0 x0 h3 y101 ff2 fs1 fc0 sc0 ls0 ws0">rate<span class="_ _6"> </span>linearly<span class="_ _7"> </span>so<span class="_ _6"> </span>that<span class="_ _6"> </span>it<span class="_ _7"> </span>approaches<span class="_ _6"> </span>zero<span class="_ _6"> </span>at<span class="_ _7"> </span>the<span class="_ _6"> </span>end<span class="_ _6"> </span>of<span class="_ _7"> </span>training).<span class="_ _a"> </span>Training<span class="_ _6"> </span>a<span class="_ _6"> </span>model<span class="_ _7"> </span>on<span class="_ _6"> </span>twice<span class="_ _6"> </span>as<span class="_ _7"> </span>much</div><div class="t m0 x0 h3 y102 ff2 fs1 fc0 sc0 ls0 ws0">data<span class="_ _6"> </span>using<span class="_ _6"> </span>one<span class="_ _7"> </span>epoch<span class="_ _6"> </span>gi<span class="_ _1"></span>ves<span class="_ _6"> </span>comparable<span class="_ _6"> </span>or<span class="_ _6"> </span>better<span class="_ _6"> </span>results<span class="_ _7"> </span>than<span class="_ _6"> </span>iterating<span class="_ _6"> </span>over<span class="_ _6"> </span>the<span class="_ _6"> </span>same<span class="_ _6"> </span>data<span class="_ _6"> </span>for<span class="_ _7"> </span>three</div><div class="t m0 x0 h3 y103 ff2 fs1 fc0 sc0 ls0 ws0">epochs, as is shown in T<span class="_ _0"></span>able 5, and provides additional small speedup.</div><div class="t m0 x0 h3 yb5 ff1 fs1 fc0 sc0 ls0 ws0">4.4<span class="_ _d"> </span>Large Scale Parallel T<span class="_ _3"></span>raining of Models</div><div class="t m0 x0 h3 y58 ff2 fs1 fc0 sc0 ls0 ws0">As<span class="_ _6"> </span>mentioned<span class="_ _6"> </span>earlier<span class="_ _1"></span>, we<span class="_ _6"> </span>have implemented<span class="_ _6"> </span>various models<span class="_ _6"> </span>in<span class="_ _6"> </span>a<span class="_ _6"> </span>distributed framew<span class="_ _1"></span>ork<span class="_ _6"> </span>called<span class="_ _6"> </span>Dis-</div><div class="t m0 x0 h3 y24 ff2 fs1 fc0 sc0 ls0 ws0">tBelief.<span class="_ _c"> </span>Belo<span class="_ _1"></span>w<span class="_ _7"> </span>we<span class="_ _2"> </span>report<span class="_ _7"> </span>the<span class="_ _2"> </span>results<span class="_ _7"> </span>of<span class="_ _7"> </span>several<span class="_ _7"> </span>models<span class="_ _2"> </span>trained<span class="_ _7"> </span>on<span class="_ _2"> </span>the<span class="_ _7"> </span>Google<span class="_ _7"> </span>News<span class="_ _2"> </span>6B<span class="_ _7"> </span>data<span class="_ _7"> </span>set,</div><div class="t m0 x0 h3 y25 ff2 fs1 fc0 sc0 ls0 ws0">with<span class="_ _4"> </span>mini-batch asynchronous<span class="_ _4"> </span>gradient<span class="_ _4"> </span>descent<span class="_ _4"> </span>and<span class="_ _4"> </span>the<span class="_ _4"> </span>adapti<span class="_ _1"></span>ve<span class="_ _4"> </span>learning<span class="_ _4"> </span>rate<span class="_ _4"> </span>procedure called<span class="_ _4"> </span>Ada-</div><div class="t m0 x0 h3 y26 ff2 fs1 fc0 sc0 ls0 ws0">grad<span class="_ _7"> </span>[7].<span class="_ _9"> </span>W<span class="_ _3"></span>e<span class="_ _7"> </span>used<span class="_ _7"> </span>50<span class="_ _7"> </span>to<span class="_ _7"> </span>100<span class="_ _7"> </span>model<span class="_ _7"> </span>replicas<span class="_ _7"> </span>during<span class="_ _7"> </span>the<span class="_ _7"> </span>training.<span class="_ _9"> </span>The<span class="_ _7"> </span>number<span class="_ _7"> </span>of<span class="_ _7"> </span>CPU<span class="_ _7"> </span>cores<span class="_ _7"> </span>is<span class="_ _7"> </span>an</div><div class="t m0 xe h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">8</div><a class="l" href="#pf8" data-dest-detail='[8,"XYZ",144.158,424.557,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:221.950000px;bottom:139.098000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,392.758,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:130.742000px;bottom:60.567000px;width:5.974000px;height:7.723000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pf9" class="pf w0 h0" data-page-no="9"><div class="pc pc9 w0 h0"><img class="bi x0 y104 w8 h13" alt="" src="bg9.png"/><div class="t m0 x0 h3 yb7 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>able<span class="_ _2"> </span>6:<span class="_ _5b"> </span><span class="ff5">Comparison<span class="_ _2"> </span>of<span class="_ _2"> </span>models<span class="_ _2"> </span>tr<span class="_ _1"></span>ained<span class="_ _2"> </span>using<span class="_ _2"> </span>the<span class="_ _2"> </span>DistBelief<span class="_ _7"> </span>distributed<span class="_ _2"> </span>frame<span class="_ _1"></span>work.<span class="_ _12"> </span>Note<span class="_ _2"> </span>that</span></div><div class="t m0 x0 h3 yb8 ff5 fs1 fc0 sc0 ls0 ws0">training of NNLM with 1000-dimensional vectors would tak<span class="_ _1"></span>e too long to complete.</div><div class="t m0 x35 h3 yb9 ff2 fs1 fc0 sc0 ls0 ws0">Model<span class="_ _5c"> </span>V<span class="_ _0"></span>ector<span class="_ _31"> </span>T<span class="_ _3"></span>raining<span class="_ _49"> </span>Accuracy [%]<span class="_ _5d"> </span>T<span class="_ _1"></span>raining time</div><div class="t m0 x21 h3 y2 ff2 fs1 fc0 sc0 ls0 ws0">Dimensionality<span class="_ _4a"> </span>words<span class="_ _10"> </span>[days<span class="_ _4"> </span>x CPU cores]</div><div class="t m0 x36 h3 yba ff2 fs1 fc0 sc0 ls0 ws0">Semantic<span class="_ _8"> </span>Syntactic<span class="_ _8"> </span>T<span class="_ _3"></span>otal</div><div class="t m0 x35 h3 ybb ff2 fs1 fc0 sc0 ls0 ws0">NNLM<span class="_ _5a"> </span>100<span class="_ _1a"> </span>6B<span class="_ _1e"> </span>34.2<span class="_ _38"> </span>64.5<span class="_ _30"> </span>50.8<span class="_ _36"> </span>14 x 180</div><div class="t m0 x35 h3 ybc ff2 fs1 fc0 sc0 ls0 ws0">CBO<span class="_ _1"></span>W<span class="_ _25"> </span>1000<span class="_ _5e"> </span>6B<span class="_ _1e"> </span>57.3<span class="_ _38"> </span>68.9<span class="_ _30"> </span>63.7<span class="_ _4b"> </span>2<span class="_ _4"> </span>x 140</div><div class="t m0 x35 h3 ybd ff2 fs1 fc0 sc0 ls0 ws0">Skip-gram<span class="_ _1e"> </span>1000<span class="_ _5e"> </span>6B<span class="_ _33"> </span>66.1<span class="_ _4d"> </span>65.1<span class="_ _52"> </span>65.6<span class="_ _5f"> </span>2.5 x 125</div><div class="t m0 x0 h3 y105 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>able 7:<span class="_ _7"> </span><span class="ff5">Comparison and combination of models on the Micr<span class="_ _1"></span>osoft Sentence Completion Challeng<span class="_ _1"></span>e.</span></div><div class="t m0 x1a h3 y106 ff2 fs1 fc0 sc0 ls0 ws0">Architecture<span class="_ _60"> </span>Accuracy [%]</div><div class="t m0 x1a h3 y107 ff2 fs1 fc0 sc0 ls0 ws0">4-gram [32]<span class="_ _61"> </span>39</div><div class="t m0 x1a h3 y108 ff2 fs1 fc0 sc0 ls0 ws0">A<span class="_ _3"></span>verage LSA similarity [32]<span class="_ _36"> </span>49</div><div class="t m0 x1a h3 y109 ff2 fs1 fc0 sc0 ls0 ws0">Log-bilinear model [24]<span class="_ _62"> </span>54.8</div><div class="t m0 x1a h3 y10a ff2 fs1 fc0 sc0 ls0 ws0">RNNLMs [19]<span class="_ _63"> </span>55.4</div><div class="t m0 x1a h3 y10b ff2 fs1 fc0 sc0 ls0 ws0">Skip-gram<span class="_ _64"> </span>48.0</div><div class="t m0 x1a h3 y10c ff2 fs1 fc0 sc0 ls0 ws0">Skip-gram + RNNLMs<span class="_ _65"> </span><span class="ff1">58.9</span></div><div class="t m0 x0 h3 y10d ff2 fs1 fc0 sc0 ls0 ws0">estimate<span class="_ _6"> </span>since<span class="_ _7"> </span>the<span class="_ _7"> </span>data<span class="_ _6"> </span>center<span class="_ _7"> </span>machines<span class="_ _7"> </span>are<span class="_ _6"> </span>shared<span class="_ _7"> </span>with<span class="_ _7"> </span>other<span class="_ _6"> </span>production<span class="_ _7"> </span>tasks,<span class="_ _7"> </span>and<span class="_ _7"> </span>the<span class="_ _6"> </span>usage<span class="_ _7"> </span>can</div><div class="t m0 x0 h3 y10e ff2 fs1 fc0 sc0 ls0 ws0">ﬂuctuate<span class="_ _6"> </span>quite a<span class="_ _6"> </span>bit.<span class="_ _2"> </span>Note<span class="_ _6"> </span>that<span class="_ _6"> </span>due<span class="_ _6"> </span>to the<span class="_ _6"> </span>overhead of<span class="_ _6"> </span>the distributed<span class="_ _6"> </span>frame<span class="_ _1"></span>work,<span class="_ _6"> </span>the CPU<span class="_ _6"> </span>usage<span class="_ _6"> </span>of</div><div class="t m0 x0 h3 y10f ff2 fs1 fc0 sc0 ls0 ws0">the CBO<span class="_ _1"></span>W<span class="_ _4"> </span>model and<span class="_ _4"> </span>the Skip-gram<span class="_ _4"> </span>model are<span class="_ _4"> </span>much<span class="_ _4"> </span>closer to<span class="_ _4"> </span>each other<span class="_ _4"> </span>than their<span class="_ _4"> </span>single-machine</div><div class="t m0 x0 h3 ya7 ff2 fs1 fc0 sc0 ls0 ws0">implementations.<span class="_ _6"> </span>The result are reported in T<span class="_ _1"></span>able 6.</div><div class="t m0 x0 h3 y110 ff1 fs1 fc0 sc0 ls0 ws0">4.5<span class="_ _d"> </span>Microsoft Resear<span class="_ _1"></span>ch Sentence Completion Challenge</div><div class="t m0 x0 h3 y111 ff2 fs1 fc0 sc0 ls0 ws0">The Microsoft<span class="_ _4"> </span>Sentence<span class="_ _4"> </span>Completion Challenge<span class="_ _4"> </span>has been<span class="_ _4"> </span>recently<span class="_ _4"> </span>introduced as<span class="_ _4"> </span>a<span class="_ _4"> </span>task for<span class="_ _4"> </span>adv<span class="_ _1"></span>ancing</div><div class="t m0 x0 h3 yf9 ff2 fs1 fc0 sc0 ls0 ws0">language modeling and<span class="_ _4"> </span>other NLP techniques<span class="_ _4"> </span>[32].<span class="_ _6"> </span>This task consists of 1040<span class="_ _4"> </span>sentences, where one</div><div class="t m0 x0 h3 y112 ff2 fs1 fc0 sc0 ls0 ws0">word<span class="_ _6"> </span>is<span class="_ _7"> </span>missing<span class="_ _7"> </span>in<span class="_ _6"> </span>each<span class="_ _7"> </span>sentence<span class="_ _7"> </span>and<span class="_ _6"> </span>the<span class="_ _7"> </span>goal<span class="_ _7"> </span>is<span class="_ _7"> </span>to<span class="_ _6"> </span>select<span class="_ _7"> </span>word<span class="_ _6"> </span>that<span class="_ _7"> </span>is<span class="_ _7"> </span>the<span class="_ _7"> </span>most<span class="_ _6"> </span>coherent<span class="_ _7"> </span>with<span class="_ _7"> </span>the</div><div class="t m0 x0 h3 yaa ff2 fs1 fc0 sc0 ls0 ws0">rest<span class="_ _6"> </span>of<span class="_ _6"> </span>the<span class="_ _6"> </span>sentence,<span class="_ _7"> </span>gi<span class="_ _1"></span>ven<span class="_ _6"> </span>a<span class="_ _6"> </span>list<span class="_ _6"> </span>of<span class="_ _6"> </span>ﬁve reasonable<span class="_ _7"> </span>choices.<span class="_ _a"> </span>Performance<span class="_ _6"> </span>of<span class="_ _6"> </span>sev<span class="_ _1"></span>eral<span class="_ _6"> </span>techniques<span class="_ _6"> </span>has</div><div class="t m0 x0 h3 yab ff2 fs1 fc0 sc0 ls0 ws0">been<span class="_ _7"> </span>already<span class="_ _2"> </span>reported<span class="_ _7"> </span>on<span class="_ _7"> </span>this<span class="_ _2"> </span>s<span class="_ _1"></span>et,<span class="_ _2"> </span>including<span class="_ _6"> </span>N-gram<span class="_ _2"> </span>models,<span class="_ _7"> </span>LSA-based<span class="_ _2"> </span>model<span class="_ _7"> </span>[32],<span class="_ _2"> </span>log-bilinear</div><div class="t m0 x0 h3 yac ff2 fs1 fc0 sc0 ls0 ws0">model<span class="_ _6"> </span>[24]<span class="_ _6"> </span>and<span class="_ _6"> </span>a<span class="_ _6"> </span>combination<span class="_ _6"> </span>of<span class="_ _7"> </span>recurrent<span class="_ _6"> </span>neural<span class="_ _6"> </span>networks<span class="_ _6"> </span>that<span class="_ _6"> </span>currently<span class="_ _6"> </span>holds<span class="_ _6"> </span>the<span class="_ _6"> </span>state<span class="_ _6"> </span>of<span class="_ _7"> </span>the<span class="_ _6"> </span>art</div><div class="t m0 x0 h3 yad ff2 fs1 fc0 sc0 ls0 ws0">performance of 55.4% accuracy on this benchmark [19].</div><div class="t m0 x0 h3 y113 ff2 fs1 fc0 sc0 ls0 ws0">W<span class="_ _3"></span>e<span class="_ _6"> </span>have<span class="_ _6"> </span>explored<span class="_ _6"> </span>the<span class="_ _6"> </span>performance<span class="_ _6"> </span>of<span class="_ _6"> </span>Skip-gram<span class="_ _7"> </span>architecture<span class="_ _6"> </span>on<span class="_ _6"> </span>this<span class="_ _7"> </span>task.<span class="_ _a"> </span>First,<span class="_ _7"> </span>we<span class="_ _6"> </span>train<span class="_ _6"> </span>the<span class="_ _7"> </span>640-</div><div class="t m0 x0 h3 y114 ff2 fs1 fc0 sc0 ls0 ws0">dimensional<span class="_ _7"> </span>model<span class="_ _6"> </span>on<span class="_ _7"> </span>50M<span class="_ _7"> </span>words<span class="_ _6"> </span>provided<span class="_ _6"> </span>in<span class="_ _7"> </span>[32].<span class="_ _9"> </span>Then,<span class="_ _6"> </span>we<span class="_ _7"> </span>compute<span class="_ _7"> </span>score<span class="_ _7"> </span>of<span class="_ _6"> </span>each<span class="_ _7"> </span>sentence<span class="_ _7"> </span>in</div><div class="t m0 x0 h3 y115 ff2 fs1 fc0 sc0 ls0 ws0">the test<span class="_ _4"> </span>set by<span class="_ _4"> </span>using the<span class="_ _4"> </span>unknown<span class="_ _4"> </span>word<span class="_ _4"> </span>at the<span class="_ _4"> </span>input, and<span class="_ _4"> </span>predict all surrounding<span class="_ _4"> </span>words<span class="_ _4"> </span>in a<span class="_ _4"> </span>sentence.</div><div class="t m0 x0 h3 y116 ff2 fs1 fc0 sc0 ls0 ws0">The ﬁnal<span class="_ _6"> </span>sentence<span class="_ _6"> </span>score is<span class="_ _6"> </span>then the<span class="_ _6"> </span>sum of<span class="_ _6"> </span>these<span class="_ _6"> </span>indi<span class="_ _1"></span>vidual predictions.<span class="_ _2"> </span>Using<span class="_ _6"> </span>the sentence<span class="_ _6"> </span>scores,</div><div class="t m0 x0 h3 y4d ff2 fs1 fc0 sc0 ls0 ws0">we choose the most likely sentence.</div><div class="t m0 x0 h3 y117 ff2 fs1 fc0 sc0 ls0 ws0">A<span class="_ _7"> </span>short<span class="_ _2"> </span>summary<span class="_ _7"> </span>of<span class="_ _2"> </span>some<span class="_ _7"> </span>pre<span class="_ _1"></span>vious<span class="_ _7"> </span>results<span class="_ _2"> </span>together<span class="_ _7"> </span>with<span class="_ _2"> </span>the<span class="_ _7"> </span>ne<span class="_ _1"></span>w<span class="_ _7"> </span>results<span class="_ _2"> </span>is<span class="_ _7"> </span>presented<span class="_ _2"> </span>in<span class="_ _7"> </span>T<span class="_ _3"></span>able<span class="_ _2"> </span>7.</div><div class="t m0 x0 h3 y118 ff2 fs1 fc0 sc0 ls0 ws0">While<span class="_ _4"> </span>the<span class="_ _4"> </span>Skip-gram<span class="_ _4"> </span>model<span class="_ _5"> </span>itself<span class="_ _4"> </span>does<span class="_ _4"> </span>not<span class="_ _4"> </span>perform<span class="_ _4"> </span>on<span class="_ _5"> </span>this task<span class="_ _4"> </span>better<span class="_ _5"> </span>than LSA<span class="_ _4"> </span>similarity<span class="_ _3"></span>,<span class="_ _4"> </span>the<span class="_ _4"> </span>scores</div><div class="t m0 x0 h3 y119 ff2 fs1 fc0 sc0 ls0 ws0">from this model<span class="_ _4"> </span>are complementary to<span class="_ _4"> </span>scores obtained with<span class="_ _4"> </span>RNNLMs, and a<span class="_ _4"> </span>weighted combination</div><div class="t m0 x0 h3 y11a ff2 fs1 fc0 sc0 ls0 ws0">leads to<span class="_ _6"> </span>a new state<span class="_ _6"> </span>of the<span class="_ _6"> </span>art result<span class="_ _6"> </span>58.9% accuracy<span class="_ _6"> </span>(59.2% on<span class="_ _6"> </span>the dev<span class="_ _1"></span>elopment part<span class="_ _6"> </span>of<span class="_ _6"> </span>the set<span class="_ _6"> </span>and</div><div class="t m0 x0 h3 y52 ff2 fs1 fc0 sc0 ls0 ws0">58.7% on the test part of the set).</div><div class="t m0 x0 h5 y11b ff1 fs2 fc0 sc0 ls0 ws0">5<span class="_ _8"> </span>Examples of the Learned Relationships</div><div class="t m0 x0 h3 y58 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>able 8<span class="_ _4"> </span>sho<span class="_ _1"></span>ws w<span class="_ _1"></span>ords<span class="_ _4"> </span>that follo<span class="_ _1"></span>w<span class="_ _4"> </span>v<span class="_ _1"></span>arious<span class="_ _4"> </span>relationships.<span class="_ _6"> </span>W<span class="_ _1"></span>e<span class="_ _4"> </span>follo<span class="_ _1"></span>w<span class="_ _4"> </span>the<span class="_ _4"> </span>approach<span class="_ _4"> </span>described<span class="_ _4"> </span>abov<span class="_ _1"></span>e:<span class="_ _6"> </span>the</div><div class="t m0 x0 h3 y24 ff2 fs1 fc0 sc0 ls0 ws0">relationship<span class="_ _4"> </span>is<span class="_ _5"> </span>deﬁned<span class="_ _4"> </span>by<span class="_ _4"> </span>subtracting<span class="_ _5"> </span>two<span class="_ _4"> </span>word<span class="_ _5"> </span>vectors,<span class="_ _4"> </span>and<span class="_ _4"> </span>the<span class="_ _5"> </span>result<span class="_ _4"> </span>is<span class="_ _4"> </span>added<span class="_ _5"> </span>to<span class="_ _4"> </span>another<span class="_ _4"> </span>w<span class="_ _1"></span>ord.<span class="_ _6"> </span>Thus</div><div class="t m0 x0 h3 y25 ff2 fs1 fc0 sc0 ls0 ws0">for<span class="_ _7"> </span>example,<span class="_ _7"> </span><span class="ff5">P<span class="_ _3"></span>aris<span class="_ _7"> </span>-<span class="_ _7"> </span>F<span class="_ _1"></span>r<span class="_ _1"></span>ance<span class="_ _7"> </span>+<span class="_ _7"> </span>Italy<span class="_ _6"> </span>=<span class="_ _7"> </span>Rome<span class="ff2">.<span class="_ _9"> </span>As<span class="_ _7"> </span>it<span class="_ _7"> </span>can<span class="_ _7"> </span>be<span class="_ _7"> </span>seen,<span class="_ _7"> </span>accuracy<span class="_ _7"> </span>is<span class="_ _7"> </span>quite<span class="_ _7"> </span>good,<span class="_ _7"> </span>although</span></span></div><div class="t m0 x0 h3 y26 ff2 fs1 fc0 sc0 ls0 ws0">there<span class="_ _2"> </span>is<span class="_ _7"> </span>clearly<span class="_ _2"> </span>a<span class="_ _7"> </span>lot<span class="_ _2"> </span>of<span class="_ _7"> </span>room<span class="_ _2"> </span>for<span class="_ _7"> </span>further<span class="_ _2"> </span>impro<span class="_ _1"></span>v<span class="_ _1"></span>ements<span class="_ _2"> </span>(note<span class="_ _7"> </span>that<span class="_ _7"> </span>using<span class="_ _2"> </span>our<span class="_ _7"> </span>accuracy<span class="_ _2"> </span>metric<span class="_ _7"> </span>that</div><div class="t m0 xe h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">9</div><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,337.371,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:248.664000px;bottom:521.622000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,337.371,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:314.745000px;bottom:507.375000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,544.594,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:297.091000px;bottom:493.268000px;width:10.956000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,674.108,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:259.742000px;bottom:478.802000px;width:10.955000px;height:8.007000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf9" data-dest-detail='[9,"XYZ",147.108,704.315,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:306.073000px;bottom:379.669000px;width:5.974000px;height:9.943000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,337.371,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:294.028000px;bottom:323.832000px;width:10.955000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,337.371,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:436.613000px;bottom:290.955000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,544.594,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:137.627000px;bottom:280.136000px;width:10.955000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,674.108,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:315.498000px;bottom:268.958000px;width:10.956000px;height:8.006000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,337.371,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:301.291000px;bottom:241.142000px;width:10.956000px;height:7.926000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pf9" data-dest-detail='[9,"XYZ",143.391,567.956,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:495.032000px;bottom:189.311000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfa" data-dest-detail='[10,"XYZ",144.116,704.315,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:130.638000px;bottom:91.362000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pfa" class="pf w0 h0" data-page-no="a"><div class="pc pca w0 h0"><img class="bi x13 y11c w9 h14" alt="" src="bga.png"/><div class="t m0 x0 h3 yb7 ff2 fs1 fc0 sc0 ls0 ws0">T<span class="_ _3"></span>able<span class="_ _6"> </span>8:<span class="_ _b"> </span><span class="ff5">Examples of<span class="_ _6"> </span>the<span class="_ _6"> </span>wor<span class="_ _1"></span>d pair<span class="_ _6"> </span>relationships, using<span class="_ _6"> </span>the<span class="_ _6"> </span>best<span class="_ _6"> </span>wor<span class="_ _1"></span>d vectors<span class="_ _6"> </span>fr<span class="_ _1"></span>om T<span class="_ _3"></span>able<span class="_ _6"> </span>4<span class="_ _6"> </span>(Skip-</span></div><div class="t m0 x0 h3 yb8 ff5 fs1 fc0 sc0 ls0 ws0">gram model tr<span class="_ _1"></span>ained on 783M wor<span class="_ _1"></span>ds with 300 dimensionality).</div><div class="t m0 x37 h3 yb9 ff2 fs1 fc0 sc0 ls0 ws0">Relationship<span class="_ _66"> </span>Example 1<span class="_ _1a"> </span>Example 2<span class="_ _49"> </span>Example<span class="_ _4"> </span>3</div><div class="t m0 xd h3 y2 ff2 fs1 fc0 sc0 ls0 ws0">France - Paris<span class="_ _25"> </span>Italy:<span class="_ _6"> </span>Rome<span class="_ _67"> </span>Japan:<span class="_ _6"> </span>T<span class="_ _1"></span>ok<span class="_ _1"></span>yo<span class="_ _2c"> </span>Florida:<span class="_ _6"> </span>T<span class="_ _1"></span>allahassee</div><div class="t m0 x38 h3 yba ff2 fs1 fc0 sc0 ls0 ws0">big - bigger<span class="_ _62"> </span>small: larger<span class="_ _37"> </span>cold:<span class="_ _6"> </span>colder<span class="_ _1e"> </span>quick:<span class="_ _7"> </span>quicker</div><div class="t m0 x39 h3 ybb ff2 fs1 fc0 sc0 ls0 ws0">Miami - Florida<span class="_ _68"> </span>Baltimore:<span class="_ _6"> </span>Maryland<span class="_ _2c"> </span>Dallas:<span class="_ _6"> </span>T<span class="_ _1"></span>e<span class="_ _1"></span>xas<span class="_ _34"> </span>K<span class="_ _1"></span>ona:<span class="_ _6"> </span>Hawaii</div><div class="t m0 x3a h3 ybc ff2 fs1 fc0 sc0 ls0 ws0">Einstein - scientist<span class="_ _22"> </span>Messi:<span class="_ _7"> </span>midﬁelder<span class="_ _2c"> </span>Mozart:<span class="_ _7"> </span>violinist<span class="_ _30"> </span>Picasso:<span class="_ _6"> </span>painter</div><div class="t m0 x2a h3 ybd ff2 fs1 fc0 sc0 ls0 ws0">Sarkozy - France<span class="_ _1d"> </span>Berlusconi:<span class="_ _7"> </span>Italy<span class="_ _2c"> </span>Merk<span class="_ _1"></span>el:<span class="_ _7"> </span>German<span class="_ _1"></span>y<span class="_ _2c"> </span>K<span class="_ _1"></span>oizumi:<span class="_ _6"> </span>Japan</div><div class="t m0 x38 h3 y11d ff2 fs1 fc0 sc0 ls0 ws0">copper - Cu<span class="_ _3d"> </span>zinc:<span class="_ _7"> </span>Zn<span class="_ _69"> </span>gold:<span class="_ _6"> </span>Au<span class="_ _31"> </span>uranium:<span class="_ _6"> </span>plutonium</div><div class="t m0 x3a h3 y11e ff2 fs1 fc0 sc0 ls0 ws0">Berlusconi - Silvio<span class="_ _22"> </span>Sarkozy:<span class="_ _7"> </span>Nicolas<span class="_ _6a"> </span>Putin:<span class="_ _6"> </span>Medvedev<span class="_ _21"> </span>Obama:<span class="_ _7"> </span>Barack</div><div class="t m0 x1 h3 y11f ff2 fs1 fc0 sc0 ls0 ws0">Microsoft - W<span class="_ _1"></span>indo<span class="_ _1"></span>ws<span class="_ _21"> </span>Google:<span class="_ _7"> </span>Android<span class="_ _38"> </span>IBM:<span class="_ _4"> </span>Linux<span class="_ _1e"> </span>Apple:<span class="_ _7"> </span>iPhone</div><div class="t m0 x1 h3 y120 ff2 fs1 fc0 sc0 ls0 ws0">Microsoft - Ballmer<span class="_ _27"> </span>Google:<span class="_ _7"> </span>Y<span class="_ _3"></span>ahoo<span class="_ _24"> </span>IBM: McNealy<span class="_ _34"> </span>Apple:<span class="_ _7"> </span>Jobs</div><div class="t m0 xd h3 y121 ff2 fs1 fc0 sc0 ls0 ws0">Japan - sushi<span class="_ _34"> </span>German<span class="_ _1"></span>y:<span class="_ _6"> </span>bratwurst<span class="_ _52"> </span>France:<span class="_ _7"> </span>tapas<span class="_ _39"> </span>USA:<span class="_ _4"> </span>pizza</div><div class="t m0 x0 h3 y122 ff2 fs1 fc0 sc0 ls0 ws0">assumes<span class="_ _7"> </span>exact<span class="_ _7"> </span>match,<span class="_ _2"> </span>the<span class="_ _6"> </span>results<span class="_ _2"> </span>in<span class="_ _6"> </span>T<span class="_ _1"></span>able<span class="_ _6"> </span>8<span class="_ _2"> </span>w<span class="_ _1"></span>ould<span class="_ _7"> </span>score<span class="_ _7"> </span>only<span class="_ _7"> </span>about<span class="_ _7"> </span>60%).<span class="_ _11"> </span>W<span class="_ _3"></span>e<span class="_ _7"> </span>believe<span class="_ _7"> </span>that<span class="_ _7"> </span>word</div><div class="t m0 x0 h3 yc6 ff2 fs1 fc0 sc0 ls0 ws0">vectors trained<span class="_ _6"> </span>on ev<span class="_ _1"></span>en<span class="_ _6"> </span>lar<span class="_ _1"></span>ger data<span class="_ _6"> </span>sets<span class="_ _6"> </span>with larger dimensionality<span class="_ _6"> </span>will perform<span class="_ _6"> </span>signiﬁcantly<span class="_ _6"> </span>better<span class="_ _1"></span>,</div><div class="t m0 x0 h3 y10c ff2 fs1 fc0 sc0 ls0 ws0">and<span class="_ _4"> </span>will<span class="_ _5"> </span>enable<span class="_ _4"> </span>the<span class="_ _4"> </span>de<span class="_ _1"></span>v<span class="_ _1"></span>elopment<span class="_ _4"> </span>of<span class="_ _5"> </span>new<span class="_ _4"> </span>inno<span class="_ _1"></span>v<span class="_ _1"></span>ati<span class="_ _1"></span>ve<span class="_ _5"> </span>applications.<span class="_ _7"> </span>Another<span class="_ _4"> </span>w<span class="_ _1"></span>ay<span class="_ _4"> </span>to<span class="_ _5"> </span>improve<span class="_ _5"> </span>accuracy<span class="_ _4"> </span>is</div><div class="t m0 x0 h3 ya ff2 fs1 fc0 sc0 ls0 ws0">to provide<span class="_ _4"> </span>more than one<span class="_ _4"> </span>example of<span class="_ _4"> </span>the relationship.<span class="_ _6"> </span>By using ten e<span class="_ _1"></span>xamples instead of<span class="_ _4"> </span>one to form</div><div class="t m0 x0 h3 yb ff2 fs1 fc0 sc0 ls0 ws0">the relationship<span class="_ _4"> </span>vector (we<span class="_ _4"> </span>a<span class="_ _1"></span>verage the<span class="_ _4"> </span>indi<span class="_ _1"></span>vidual v<span class="_ _1"></span>ectors together), we<span class="_ _4"> </span>ha<span class="_ _1"></span>ve observ<span class="_ _1"></span>ed impro<span class="_ _1"></span>vement</div><div class="t m0 x0 h3 yc ff2 fs1 fc0 sc0 ls0 ws0">of accuracy of our best models by about 10% absolutely on the semantic-syntactic test.</div><div class="t m0 x0 h3 y123 ff2 fs1 fc0 sc0 ls0 ws0">It<span class="_ _2"> </span>is<span class="_ _7"> </span>also<span class="_ _2"> </span>possible<span class="_ _2"> </span>to<span class="_ _7"> </span>apply<span class="_ _2"> </span>the<span class="_ _7"> </span>vector<span class="_ _2"> </span>operations<span class="_ _2"> </span>to<span class="_ _7"> </span>solve<span class="_ _2"> </span>dif<span class="_ _1"></span>ferent<span class="_ _7"> </span>tasks.<span class="_ _12"> </span>F<span class="_ _1"></span>or<span class="_ _7"> </span>example,<span class="_ _2"> </span>we<span class="_ _2"> </span>ha<span class="_ _1"></span>v<span class="_ _1"></span>e</div><div class="t m0 x0 h3 y124 ff2 fs1 fc0 sc0 ls0 ws0">observed<span class="_ _4"> </span>good<span class="_ _4"> </span>accuracy<span class="_ _4"> </span>for<span class="_ _4"> </span>selecting<span class="_ _4"> </span>out-of-the-list<span class="_ _4"> </span>words,<span class="_ _4"> </span>by<span class="_ _4"> </span>computing a<span class="_ _1"></span>verage<span class="_ _4"> </span>v<span class="_ _1"></span>ector<span class="_ _4"> </span>for a<span class="_ _4"> </span>list<span class="_ _4"> </span>of</div><div class="t m0 x0 h3 y125 ff2 fs1 fc0 sc0 ls0 ws0">words,<span class="_ _4"> </span>and<span class="_ _4"> </span>ﬁnding<span class="_ _4"> </span>the<span class="_ _4"> </span>most<span class="_ _4"> </span>distant<span class="_ _4"> </span>w<span class="_ _1"></span>ord<span class="_ _4"> </span>vector<span class="_ _3"></span>.<span class="_ _7"> </span>This<span class="_ _4"> </span>is<span class="_ _4"> </span>a<span class="_ _4"> </span>popular<span class="_ _4"> </span>type<span class="_ _4"> </span>of<span class="_ _4"> </span>problems<span class="_ _4"> </span>in<span class="_ _4"> </span>certain<span class="_ _4"> </span>human</div><div class="t m0 x0 h3 yf7 ff2 fs1 fc0 sc0 ls0 ws0">intelligence tests.<span class="_ _6"> </span>Clearly<span class="_ _1"></span>, there is still a lot of discov<span class="_ _1"></span>eries to be made using these techniques.</div><div class="t m0 x0 h5 yf9 ff1 fs2 fc0 sc0 ls0 ws0">6<span class="_ _8"> </span>Conclusion</div><div class="t m0 x0 h3 yab ff2 fs1 fc0 sc0 ls0 ws0">In this<span class="_ _4"> </span>paper we studied<span class="_ _4"> </span>the quality<span class="_ _4"> </span>of v<span class="_ _1"></span>ector representations of<span class="_ _4"> </span>words<span class="_ _4"> </span>deri<span class="_ _1"></span>ved by<span class="_ _4"> </span>v<span class="_ _1"></span>arious models<span class="_ _4"> </span>on</div><div class="t m0 x0 h3 yac ff2 fs1 fc0 sc0 ls0 ws0">a<span class="_ _6"> </span>collection<span class="_ _6"> </span>of<span class="_ _6"> </span>syntactic<span class="_ _6"> </span>and<span class="_ _7"> </span>semantic<span class="_ _6"> </span>language<span class="_ _6"> </span>tasks.<span class="_ _a"> </span>W<span class="_ _1"></span>e observed<span class="_ _6"> </span>that<span class="_ _6"> </span>it<span class="_ _6"> </span>is<span class="_ _6"> </span>possible<span class="_ _7"> </span>to<span class="_ _6"> </span>train<span class="_ _6"> </span>high</div><div class="t m0 x0 h3 yad ff2 fs1 fc0 sc0 ls0 ws0">quality<span class="_ _4"> </span>word<span class="_ _4"> </span>v<span class="_ _1"></span>ectors<span class="_ _4"> </span>using<span class="_ _4"> </span>very<span class="_ _4"> </span>simple<span class="_ _4"> </span>model<span class="_ _4"> </span>architectures,<span class="_ _4"> </span>compared<span class="_ _4"> </span>to<span class="_ _4"> </span>the<span class="_ _4"> </span>popular<span class="_ _4"> </span>neural<span class="_ _4"> </span>netw<span class="_ _1"></span>ork</div><div class="t m0 x0 h3 yae ff2 fs1 fc0 sc0 ls0 ws0">models<span class="_ _6"> </span>(both feedforward<span class="_ _6"> </span>and recurrent).<span class="_ _2"> </span>Because<span class="_ _6"> </span>of<span class="_ _6"> </span>the<span class="_ _6"> </span>much lower computational<span class="_ _6"> </span>complexity<span class="_ _3"></span>,<span class="_ _6"> </span>it</div><div class="t m0 x0 h3 yaf ff2 fs1 fc0 sc0 ls0 ws0">is<span class="_ _7"> </span>possible<span class="_ _7"> </span>to<span class="_ _7"> </span>compute<span class="_ _7"> </span>very<span class="_ _7"> </span>accurate<span class="_ _7"> </span>high<span class="_ _2"> </span>dimensional<span class="_ _6"> </span>word<span class="_ _7"> </span>vectors<span class="_ _7"> </span>from<span class="_ _7"> </span>a<span class="_ _7"> </span>much<span class="_ _7"> </span>larger<span class="_ _7"> </span>data<span class="_ _7"> </span>set.</div><div class="t m0 x0 h3 yb0 ff2 fs1 fc0 sc0 ls0 ws0">Using the<span class="_ _4"> </span>DistBelief distrib<span class="_ _1"></span>uted<span class="_ _4"> </span>framew<span class="_ _1"></span>ork, it<span class="_ _4"> </span>should be<span class="_ _4"> </span>possible<span class="_ _4"> </span>to train<span class="_ _4"> </span>the CBO<span class="_ _1"></span>W<span class="_ _4"> </span>and<span class="_ _4"> </span>Skip-gram</div><div class="t m0 x0 h3 ya5 ff2 fs1 fc0 sc0 ls0 ws0">models e<span class="_ _1"></span>ven on corpora<span class="_ _4"> </span>with one trillion words,<span class="_ _4"> </span>for basically unlimited size of the v<span class="_ _1"></span>ocab<span class="_ _1"></span>ulary<span class="_ _3"></span>.<span class="_ _7"> </span>That</div><div class="t m0 x0 h3 yb1 ff2 fs1 fc0 sc0 ls0 ws0">is sev<span class="_ _1"></span>eral orders of magnitude lar<span class="_ _1"></span>ger than the best previously published results for similar models.</div><div class="t m0 x0 h3 y4e ff2 fs1 fc0 sc0 ls0 ws0">An<span class="_ _4"> </span>interesting task<span class="_ _4"> </span>where<span class="_ _4"> </span>the<span class="_ _4"> </span>w<span class="_ _1"></span>ord v<span class="_ _1"></span>ectors<span class="_ _4"> </span>hav<span class="_ _1"></span>e<span class="_ _4"> </span>recently<span class="_ _4"> </span>been<span class="_ _4"> </span>shown<span class="_ _4"> </span>to<span class="_ _4"> </span>signiﬁcantly<span class="_ _4"> </span>outperform<span class="_ _4"> </span>the</div><div class="t m0 x0 h3 y4f ff2 fs1 fc0 sc0 ls0 ws0">previous<span class="_ _4"> </span>state of the art is<span class="_ _4"> </span>the SemEv<span class="_ _1"></span>al-2012 T<span class="_ _3"></span>ask 2 [11].<span class="_ _6"> </span>The publicly av<span class="_ _1"></span>ailable RNN v<span class="_ _1"></span>ectors were</div><div class="t m0 x0 h3 y50 ff2 fs1 fc0 sc0 ls0 ws0">used<span class="_ _7"> </span>together<span class="_ _7"> </span>with<span class="_ _2"> </span>other<span class="_ _6"> </span>techniques<span class="_ _7"> </span>to<span class="_ _2"> </span>achie<span class="_ _1"></span>v<span class="_ _1"></span>e<span class="_ _7"> </span>ov<span class="_ _1"></span>er<span class="_ _7"> </span>50%<span class="_ _7"> </span>increase<span class="_ _7"> </span>in<span class="_ _2"> </span>Spearman’<span class="_ _3"></span>s<span class="_ _7"> </span>rank<span class="_ _7"> </span>correlation</div><div class="t m0 x0 h3 y51 ff2 fs1 fc0 sc0 ls0 ws0">ov<span class="_ _1"></span>er<span class="_ _6"> </span>the previous<span class="_ _6"> </span>best result<span class="_ _6"> </span>[31].<span class="_ _2"> </span>The<span class="_ _6"> </span>neural<span class="_ _6"> </span>network based<span class="_ _6"> </span>word<span class="_ _6"> </span>vectors were<span class="_ _6"> </span>pre<span class="_ _1"></span>viously<span class="_ _6"> </span>applied</div><div class="t m0 x0 h3 yb2 ff2 fs1 fc0 sc0 ls0 ws0">to many other NLP tasks, for example sentiment analysis [12] and<span class="_ _6"> </span>paraphrase detection [28].<span class="_ _7"> </span>It can</div><div class="t m0 x0 h3 yb3 ff2 fs1 fc0 sc0 ls0 ws0">be expected that these applications can beneﬁt from the model architectures described in this paper<span class="_ _3"></span>.</div><div class="t m0 x0 h3 y54 ff2 fs1 fc0 sc0 ls0 ws0">Our<span class="_ _6"> </span>ongoing<span class="_ _6"> </span>work<span class="_ _6"> </span>shows that<span class="_ _6"> </span>the<span class="_ _6"> </span>word<span class="_ _6"> </span>vectors<span class="_ _6"> </span>can<span class="_ _6"> </span>be<span class="_ _6"> </span>successfully<span class="_ _6"> </span>applied<span class="_ _6"> </span>to<span class="_ _6"> </span>automatic<span class="_ _6"> </span>extension</div><div class="t m0 x0 h3 yb5 ff2 fs1 fc0 sc0 ls0 ws0">of<span class="_ _2"> </span>facts<span class="_ _2"> </span>in<span class="_ _2"> </span>Knowledge<span class="_ _2"> </span>Bases,<span class="_ _a"> </span>and<span class="_ _2"> </span>also<span class="_ _2"> </span>for<span class="_ _a"> </span>veriﬁcation<span class="_ _2"> </span>of<span class="_ _2"> </span>correctness<span class="_ _2"> </span>of<span class="_ _2"> </span>existing<span class="_ _2"> </span>facts.<span class="_ _b"> </span>Results</div><div class="t m0 x0 h3 y55 ff2 fs1 fc0 sc0 ls0 ws0">from<span class="_ _2"> </span>machine<span class="_ _2"> </span>translation<span class="_ _7"> </span>experiments<span class="_ _2"> </span>also<span class="_ _2"> </span>look<span class="_ _7"> </span>very<span class="_ _2"> </span>promising.<span class="_ _12"> </span>In<span class="_ _7"> </span>the<span class="_ _2"> </span>future,<span class="_ _2"> </span>it<span class="_ _2"> </span>would<span class="_ _2"> </span>be<span class="_ _2"> </span>also</div><div class="t m0 x0 h3 y58 ff2 fs1 fc0 sc0 ls0 ws0">interesting to<span class="_ _4"> </span>compare<span class="_ _4"> </span>our techniques<span class="_ _4"> </span>to<span class="_ _4"> </span>Latent Relational<span class="_ _4"> </span>Analysis<span class="_ _4"> </span>[30] and<span class="_ _4"> </span>others.<span class="_ _6"> </span>W<span class="_ _3"></span>e belie<span class="_ _1"></span>ve<span class="_ _4"> </span>that</div><div class="t m0 x0 h3 y24 ff2 fs1 fc0 sc0 ls0 ws0">our comprehensiv<span class="_ _1"></span>e test set will<span class="_ _6"> </span>help the research community<span class="_ _6"> </span>to improv<span class="_ _1"></span>e the existing techniques for</div><div class="t m0 x0 h3 y25 ff2 fs1 fc0 sc0 ls0 ws0">estimating<span class="_ _4"> </span>the<span class="_ _4"> </span>w<span class="_ _1"></span>ord<span class="_ _4"> </span>v<span class="_ _1"></span>ectors.<span class="_ _6"> </span>W<span class="_ _3"></span>e also<span class="_ _4"> </span>e<span class="_ _1"></span>xpect<span class="_ _4"> </span>that<span class="_ _5"> </span>high quality<span class="_ _4"> </span>w<span class="_ _1"></span>ord<span class="_ _4"> </span>v<span class="_ _1"></span>ectors<span class="_ _4"> </span>will<span class="_ _4"> </span>become<span class="_ _5"> </span>an important</div><div class="t m0 x0 h3 y26 ff2 fs1 fc0 sc0 ls0 ws0">building block for future NLP applications.</div><div class="t m0 x3b h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">10</div><a class="l" href="#pf8" data-dest-detail='[8,"XYZ",143.492,704.315,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:470.942000px;bottom:689.803000px;width:5.973000px;height:9.869000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfa" data-dest-detail='[10,"XYZ",144.116,704.315,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:278.781000px;bottom:469.566000px;width:5.974000px;height:9.944000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,277.574,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:318.814000px;bottom:187.168000px;width:10.955000px;height:7.786000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,363.274,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:225.508000px;bottom:165.110000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",108,240.007,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:340.216000px;bottom:154.291000px;width:10.955000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,443,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:461.036000px;bottom:154.151000px;width:10.955000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,389.177,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:379.467000px;bottom:93.379000px;width:10.956000px;height:7.927000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pfb" class="pf w0 h0" data-page-no="b"><div class="pc pcb w0 h0"><img class="bi x0 y25 w2 h7" alt="" src="bgb.png"/><div class="t m0 x0 h5 y2a ff1 fs2 fc0 sc0 ls0 ws0">7<span class="_ _8"> </span>F<span class="_ _1"></span>ollow-Up W<span class="_ _3"></span>ork</div><div class="t m0 x0 h3 y126 ff2 fs1 fc0 sc0 ls0 ws0">After the initial v<span class="_ _1"></span>ersion of this paper w<span class="_ _1"></span>as written, we published single-machine<span class="_ _4"> </span>multi-threaded C++</div><div class="t m0 x0 h3 y127 ff2 fs1 fc0 sc0 ls0 ws0">code for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-</div><div class="t m0 x0 h3 y128 ff2 fs1 fc0 sc0 ls0 ws0">tectures</div><div class="t m0 x39 h8 y5f ff2 fs4 fc0 sc0 ls0 ws0">4</div><div class="t m0 xd h3 y128 ff2 fs1 fc0 sc0 ls0 ws0">.<span class="_ _6"> </span>The training speed is<span class="_ _4"> </span>signiﬁcantly higher<span class="_ _4"> </span>than reported earlier<span class="_ _4"> </span>in this<span class="_ _4"> </span>paper<span class="_ _1"></span>, i.e.<span class="_ _6"> </span>it is<span class="_ _4"> </span>in the</div><div class="t m0 x0 h3 y129 ff2 fs1 fc0 sc0 ls0 ws0">order<span class="_ _4"> </span>of billions<span class="_ _4"> </span>of<span class="_ _4"> </span>words<span class="_ _4"> </span>per<span class="_ _4"> </span>hour<span class="_ _4"> </span>for<span class="_ _4"> </span>typical<span class="_ _4"> </span>hyperparameter<span class="_ _4"> </span>choices.<span class="_ _6"> </span>W<span class="_ _3"></span>e also<span class="_ _4"> </span>published<span class="_ _4"> </span>more than</div><div class="t m0 x0 h3 y12a ff2 fs1 fc0 sc0 ls0 ws0">1.4<span class="_ _6"> </span>million vectors<span class="_ _6"> </span>that<span class="_ _6"> </span>represent named<span class="_ _6"> </span>entities,<span class="_ _6"> </span>trained<span class="_ _6"> </span>on<span class="_ _6"> </span>more<span class="_ _6"> </span>than 100<span class="_ _6"> </span>billion<span class="_ _6"> </span>words.<span class="_ _2"> </span>Some<span class="_ _6"> </span>of</div><div class="t m0 x0 h3 y12b ff2 fs1 fc0 sc0 ls0 ws0">our follow-up w<span class="_ _1"></span>ork will be published in an upcoming NIPS 2013 paper [21].</div><div class="t m0 x0 h5 y12c ff1 fs2 fc0 sc0 ls0 ws0">References</div><div class="t m0 x35 h3 y12d ff2 fs1 fc0 sc0 ls0 ws0">[1]<span class="_ _9"> </span>Y<span class="_ _0"></span>.<span class="_ _6"> </span>Bengio,<span class="_ _7"> </span>R.<span class="_ _7"> </span>Ducharme,<span class="_ _2"> </span>P<span class="_ _0"></span>.<span class="_ _6"> </span>V<span class="_ _1"></span>incent.<span class="_ _6"> </span>A<span class="_ _7"> </span>neural<span class="_ _7"> </span>probabilistic<span class="_ _7"> </span>language<span class="_ _7"> </span>model.<span class="_ _7"> </span>Journal<span class="_ _7"> </span>of<span class="_ _7"> </span>Ma-</div><div class="t m0 x1 h3 y12e ff2 fs1 fc0 sc0 ls0 ws0">chine Learning Research, 3:1137-1155, 2003.</div><div class="t m0 x35 h3 y12f ff2 fs1 fc0 sc0 ls0 ws0">[2]<span class="_ _9"> </span>Y<span class="_ _0"></span>.<span class="_ _7"> </span>Bengio,<span class="_ _2"> </span>Y<span class="_ _0"></span>.<span class="_ _2"> </span>LeCun.<span class="_ _7"> </span>Scaling<span class="_ _2"> </span>learning<span class="_ _7"> </span>algorithms<span class="_ _2"> </span>to<span class="_ _1"></span>wards<span class="_ _7"> </span>AI.<span class="_ _2"> </span>In:<span class="_ _9"> </span>Large-Scale<span class="_ _7"> </span>Kernel<span class="_ _7"> </span>Ma-</div><div class="t m0 x1 h3 y6 ff2 fs1 fc0 sc0 ls0 ws0">chines, MIT Press, 2007.</div><div class="t m0 x35 h3 yc2 ff2 fs1 fc0 sc0 ls0 ws0">[3]<span class="_ _9"> </span>T<span class="_ _3"></span>.<span class="_ _2"> </span>Brants,<span class="_ _2"> </span>A.<span class="_ _2"> </span>C.<span class="_ _2"> </span>Popat,<span class="_ _a"> </span>P<span class="_ _0"></span>.<span class="_ _2"> </span>Xu,<span class="_ _a"> </span>F<span class="_ _3"></span>.<span class="_ _2"> </span>J.<span class="_ _2"> </span>Och,<span class="_ _a"> </span>and<span class="_ _2"> </span>J.<span class="_ _2"> </span>Dean.<span class="_ _2"> </span>Lar<span class="_ _1"></span>ge<span class="_ _2"> </span>language<span class="_ _2"> </span>models<span class="_ _2"> </span>in<span class="_ _2"> </span>machine</div><div class="t m0 x1 h3 y108 ff2 fs1 fc0 sc0 ls0 ws0">translation.<span class="_ _4"> </span>In<span class="_ _4"> </span>Proceedings<span class="_ _4"> </span>of<span class="_ _4"> </span>the<span class="_ _4"> </span>Joint<span class="_ _4"> </span>Conference<span class="_ _4"> </span>on<span class="_ _4"> </span>Empirical<span class="_ _4"> </span>Methods<span class="_ _4"> </span>in<span class="_ _4"> </span>Natural<span class="_ _4"> </span>Language</div><div class="t m0 x1 h3 y130 ff2 fs1 fc0 sc0 ls0 ws0">Processing and Computational Language Learning, 2007.</div><div class="t m0 x35 h3 y131 ff2 fs1 fc0 sc0 ls0 ws0">[4]<span class="_ _9"> </span>R.<span class="_ _6"> </span>Collobert<span class="_ _7"> </span>and<span class="_ _7"> </span>J.<span class="_ _7"> </span>W<span class="_ _1"></span>eston.<span class="_ _6"> </span>A<span class="_ _7"> </span>Uniﬁed<span class="_ _7"> </span>Architecture<span class="_ _7"> </span>for<span class="_ _7"> </span>Natural<span class="_ _7"> </span>Language<span class="_ _7"> </span>Processing:<span class="_ _14"> </span>Deep</div><div class="t m0 x1 h3 y132 ff2 fs1 fc0 sc0 ls0 ws0">Neural Networks with Multitask<span class="_ _6"> </span>Learning. In International Conference<span class="_ _6"> </span>on Machine Learning,</div><div class="t m0 x1 h3 yda ff2 fs1 fc0 sc0 ls0 ws0">ICML, 2008.</div><div class="t m0 x35 h3 y3c ff2 fs1 fc0 sc0 ls0 ws0">[5]<span class="_ _9"> </span>R.<span class="_ _6"> </span>Collobert,<span class="_ _2"> </span>J.<span class="_ _6"> </span>W<span class="_ _1"></span>eston,<span class="_ _7"> </span>L.<span class="_ _7"> </span>Bottou,<span class="_ _7"> </span>M.<span class="_ _2"> </span>Karlen,<span class="_ _6"> </span>K.<span class="_ _2"> </span>Ka<span class="_ _1"></span>vukcuoglu<span class="_ _6"> </span>and<span class="_ _7"> </span>P<span class="_ _3"></span>.<span class="_ _7"> </span>Kuksa.<span class="_ _7"> </span>Natural<span class="_ _7"> </span>Lan-</div><div class="t m0 x1 h3 y3d ff2 fs1 fc0 sc0 ls0 ws0">guage<span class="_ _2"> </span>Processing<span class="_ _7"> </span>(Almost)<span class="_ _2"> </span>from<span class="_ _7"> </span>Scratch.<span class="_ _2"> </span>Journal<span class="_ _2"> </span>of<span class="_ _7"> </span>Machine<span class="_ _2"> </span>Learning<span class="_ _7"> </span>Research,<span class="_ _2"> </span>12:2493-</div><div class="t m0 x1 h3 y3e ff2 fs1 fc0 sc0 ls0 ws0">2537, 2011.</div><div class="t m0 x35 h3 y133 ff2 fs1 fc0 sc0 ls0 ws0">[6]<span class="_ _9"> </span>J. Dean, G.S.<span class="_ _6"> </span>Corrado, R.<span class="_ _6"> </span>Monga, K.<span class="_ _6"> </span>Chen,<span class="_ _6"> </span>M. Devin, Q.V<span class="_ _0"></span>.<span class="_ _6"> </span>Le, M.Z.<span class="_ _6"> </span>Mao, M.A.<span class="_ _6"> </span>Ranzato, A.</div><div class="t m0 x1 h3 y134 ff2 fs1 fc0 sc0 ls0 ws0">Senior<span class="_ _1"></span>, P<span class="_ _0"></span>. T<span class="_ _1"></span>ucker<span class="_ _1"></span>, K. Y<span class="_ _0"></span>ang, A. Y<span class="_ _0"></span>. Ng., Large Scale Distrib<span class="_ _1"></span>uted Deep Networks, NIPS, 2012.</div><div class="t m0 x35 h3 ydd ff2 fs1 fc0 sc0 ls0 ws0">[7]<span class="_ _9"> </span>J.C.<span class="_ _7"> </span>Duchi,<span class="_ _2"> </span>E.<span class="_ _7"> </span>Hazan,<span class="_ _2"> </span>and<span class="_ _7"> </span>Y<span class="_ _0"></span>.<span class="_ _7"> </span>Singer<span class="_ _1"></span>.<span class="_ _7"> </span>Adaptiv<span class="_ _1"></span>e<span class="_ _7"> </span>subgradient<span class="_ _2"> </span>methods<span class="_ _6"> </span>for<span class="_ _2"> </span>online<span class="_ _7"> </span>learning<span class="_ _7"> </span>and</div><div class="t m0 x1 h3 yde ff2 fs1 fc0 sc0 ls0 ws0">stochastic optimization. Journal of Machine Learning Research, 2011.</div><div class="t m0 x35 h3 y135 ff2 fs1 fc0 sc0 ls0 ws0">[8]<span class="_ _9"> </span>J.<span class="_ _4"> </span>Elman. Finding Structure in T<span class="_ _1"></span>ime. Cognitiv<span class="_ _1"></span>e Science, 14, 179-211, 1990.</div><div class="t m0 x35 h3 yc9 ff2 fs1 fc0 sc0 ls0 ws0">[9]<span class="_ _9"> </span>Eric<span class="_ _5"> </span>H. Huang,<span class="_ _4"> </span>R.<span class="_ _5"> </span>Socher<span class="_ _1"></span>,<span class="_ _4"> </span>C.<span class="_ _4"> </span>D.<span class="_ _4"> </span>Manning<span class="_ _5"> </span>and<span class="_ _4"> </span>Andre<span class="_ _1"></span>w<span class="_ _4"> </span>Y<span class="_ _0"></span>.<span class="_ _4"> </span>Ng.<span class="_ _5"> </span>Improving<span class="_ _4"> </span>W<span class="_ _3"></span>ord<span class="_ _4"> </span>Representations</div><div class="t m0 x1 h3 y12 ff2 fs1 fc0 sc0 ls0 ws0">via<span class="_ _7"> </span>Global<span class="_ _7"> </span>Context<span class="_ _7"> </span>and<span class="_ _2"> </span>Multiple<span class="_ _6"> </span>W<span class="_ _1"></span>ord<span class="_ _6"> </span>Prototypes.<span class="_ _2"> </span>In:<span class="_ _14"> </span>Proc.<span class="_ _7"> </span>Association<span class="_ _7"> </span>for<span class="_ _2"> </span>Computational</div><div class="t m0 x1 h3 y136 ff2 fs1 fc0 sc0 ls0 ws0">Linguistics, 2012.</div><div class="t m0 x0 h3 y137 ff2 fs1 fc0 sc0 ls0 ws0">[10]<span class="_ _9"> </span>G.E.<span class="_ _6"> </span>Hinton,<span class="_ _2"> </span>J.L.<span class="_ _7"> </span>McClelland,<span class="_ _7"> </span>D.E.<span class="_ _2"> </span>Rumelhart.<span class="_ _6"> </span>Distributed<span class="_ _7"> </span>representations.<span class="_ _7"> </span>In:<span class="_ _9"> </span>P<span class="_ _1"></span>arallel<span class="_ _6"> </span>dis-</div><div class="t m0 x1 h3 y138 ff2 fs1 fc0 sc0 ls0 ws0">tributed processing:<span class="_ _2"> </span>Explorations<span class="_ _6"> </span>in<span class="_ _6"> </span>the<span class="_ _6"> </span>microstructure<span class="_ _7"> </span>of<span class="_ _6"> </span>cognition.<span class="_ _6"> </span>V<span class="_ _0"></span>olume<span class="_ _6"> </span>1:<span class="_ _2"> </span>Foundations,</div><div class="t m0 x1 h3 y139 ff2 fs1 fc0 sc0 ls0 ws0">MIT Press, 1986.</div><div class="t m0 x0 h3 y96 ff2 fs1 fc0 sc0 ls0 ws0">[11]<span class="_ _9"> </span>D.A. Jur<span class="_ _1"></span>gens, S.M. Mohammad,<span class="_ _6"> </span>P<span class="_ _0"></span>.D.<span class="_ _6"> </span>T<span class="_ _1"></span>urne<span class="_ _1"></span>y<span class="_ _3"></span>,<span class="_ _6"> </span>K.J. Holyoak. Semev<span class="_ _1"></span>al-2012 task 2:<span class="_ _2"> </span>Measuring</div><div class="t m0 x1 h3 y97 ff2 fs1 fc0 sc0 ls0 ws0">degrees<span class="_ _4"> </span>of<span class="_ _4"> </span>relational<span class="_ _4"> </span>similarity<span class="_ _3"></span>.<span class="_ _4"> </span>In:<span class="_ _6"> </span>Proceedings<span class="_ _4"> </span>of<span class="_ _4"> </span>the<span class="_ _4"> </span>6th<span class="_ _4"> </span>International<span class="_ _4"> </span>W<span class="_ _3"></span>orkshop<span class="_ _4"> </span>on<span class="_ _4"> </span>Semantic</div><div class="t m0 x1 h3 y98 ff2 fs1 fc0 sc0 ls0 ws0">Evaluation (SemEv<span class="_ _1"></span>al 2012), 2012.</div><div class="t m0 x0 h3 ye6 ff2 fs1 fc0 sc0 ls0 ws0">[12]<span class="_ _9"> </span>A.L.<span class="_ _4"> </span>Maas, R.E. Daly<span class="_ _3"></span>, P<span class="_ _3"></span>.T<span class="_ _3"></span>. Pham, D. Huang, A.Y<span class="_ _0"></span>. Ng, and C. Potts. Learning w<span class="_ _1"></span>ord vectors<span class="_ _4"> </span>for</div><div class="t m0 x1 h3 ye7 ff2 fs1 fc0 sc0 ls0 ws0">sentiment analysis. In Proceedings of A<span class="_ _1"></span>CL, 2011.</div><div class="t m0 x0 h3 y13a ff2 fs1 fc0 sc0 ls0 ws0">[13]<span class="_ _9"> </span>T<span class="_ _3"></span>. Mikolov<span class="_ _3"></span>.<span class="_ _6"> </span>Language<span class="_ _6"> </span>Modeling<span class="_ _6"> </span>for<span class="_ _6"> </span>Speech Recognition<span class="_ _6"> </span>in<span class="_ _6"> </span>Czech,<span class="_ _6"> </span>Masters<span class="_ _6"> </span>thesis,<span class="_ _6"> </span>Brno<span class="_ _6"> </span>Uni-</div><div class="t m0 x1 h3 y13b ff2 fs1 fc0 sc0 ls0 ws0">versity of T<span class="_ _3"></span>echnology<span class="_ _1"></span>,<span class="_ _4"> </span>2007.</div><div class="t m0 x0 h3 y7b ff2 fs1 fc0 sc0 ls0 ws0">[14]<span class="_ _9"> </span>T<span class="_ _3"></span>.<span class="_ _6"> </span>Mikolov<span class="_ _3"></span>,<span class="_ _7"> </span>J.<span class="_ _6"> </span>K<span class="_ _1"></span>opeck</div><div class="t m0 x3c h3 y7b ff2 fs1 fc0 sc0 ls0 ws0">´</div><div class="t m0 x3c h3 y7b ff2 fs1 fc0 sc0 ls0 ws0">y,<span class="_ _7"> </span>L.<span class="_ _6"> </span>Burget,<span class="_ _6"> </span>O.<span class="_ _6"> </span>Glembek<span class="_ _7"> </span>and<span class="_ _6"> </span>J.</div><div class="t m0 x3d h3 y50 ff2 fs1 fc0 sc0 ls0 ws0">ˇ</div><div class="t m0 x3d h3 y7b ff2 fs1 fc0 sc0 ls0 ws0">Cernock</div><div class="t m0 x3e h3 y7b ff2 fs1 fc0 sc0 ls0 ws0">´</div><div class="t m0 x3e h3 y7b ff2 fs1 fc0 sc0 ls0 ws0">y.<span class="_ _6"> </span>Neural<span class="_ _7"> </span>network<span class="_ _6"> </span>based<span class="_ _6"> </span>lan-</div><div class="t m0 x1 h3 y7c ff2 fs1 fc0 sc0 ls0 ws0">guage models for higly inﬂectiv<span class="_ _1"></span>e languages, In:<span class="_ _6"> </span>Proc. ICASSP 2009.</div><div class="t m0 x0 h3 y13c ff2 fs1 fc0 sc0 ls0 ws0">[15]<span class="_ _9"> </span>T<span class="_ _3"></span>.<span class="_ _2"> </span>Mikolo<span class="_ _1"></span>v<span class="_ _3"></span>,<span class="_ _a"> </span>M.<span class="_ _2"> </span>Karaﬁ</div><div class="t m0 x3f h3 y13d ff2 fs1 fc0 sc0 ls0 ws0">´</div><div class="t m0 x3f h3 y13c ff2 fs1 fc0 sc0 ls0 ws0">at,<span class="_ _a"> </span>L.<span class="_ _2"> </span>Burget,<span class="_ _2"> </span>J.</div><div class="t m0 x40 h3 y52 ff2 fs1 fc0 sc0 ls0 ws0">ˇ</div><div class="t m0 x36 h3 y13c ff2 fs1 fc0 sc0 ls0 ws0">Cernock</div><div class="t m0 x41 h3 y13d ff2 fs1 fc0 sc0 ls0 ws0">´</div><div class="t m0 x41 h3 y13c ff2 fs1 fc0 sc0 ls0 ws0">y,<span class="_ _a"> </span>S.<span class="_ _2"> </span>Khudanpur<span class="_ _1"></span>.<span class="_ _2"> </span>Recurrent<span class="_ _2"> </span>neural<span class="_ _2"> </span>netw<span class="_ _1"></span>ork</div><div class="t m0 x1 h3 y13e ff2 fs1 fc0 sc0 ls0 ws0">based language model, In:<span class="_ _6"> </span>Proceedings of Interspeech, 2010.</div><div class="t m0 x0 h3 y13f ff2 fs1 fc0 sc0 ls0 ws0">[16]<span class="_ _9"> </span>T<span class="_ _3"></span>.<span class="_ _4"> </span>Mik<span class="_ _1"></span>olov<span class="_ _3"></span>,<span class="_ _4"> </span>S.<span class="_ _4"> </span>K<span class="_ _1"></span>ombrink,<span class="_ _5"> </span>L.<span class="_ _4"> </span>Burget,<span class="_ _4"> </span>J.</div><div class="t m0 xc h3 y9d ff2 fs1 fc0 sc0 ls0 ws0">ˇ</div><div class="t m0 xc h3 y13f ff2 fs1 fc0 sc0 ls0 ws0">Cernock</div><div class="t m0 x34 h3 y140 ff2 fs1 fc0 sc0 ls0 ws0">´</div><div class="t m0 x42 h3 y13f ff2 fs1 fc0 sc0 ls0 ws0">y,<span class="_ _4"> </span>S.<span class="_ _4"> </span>Khudanpur<span class="_ _3"></span>.<span class="_ _4"> </span>Extensions<span class="_ _4"> </span>of<span class="_ _5"> </span>recurrent<span class="_ _4"> </span>neural</div><div class="t m0 x1 h3 y141 ff2 fs1 fc0 sc0 ls0 ws0">network language model, In:<span class="_ _6"> </span>Proceedings of ICASSP 2011.</div><div class="t m0 x0 h3 ya1 ff2 fs1 fc0 sc0 ls0 ws0">[17]<span class="_ _9"> </span>T<span class="_ _3"></span>. Mikolov<span class="_ _3"></span>, A.<span class="_ _6"> </span>Deoras, S.<span class="_ _6"> </span>K<span class="_ _1"></span>ombrink, L. Burget, J.</div><div class="t m0 x43 h3 y58 ff2 fs1 fc0 sc0 ls0 ws0">ˇ</div><div class="t m0 x43 h3 ya1 ff2 fs1 fc0 sc0 ls0 ws0">Cernock</div><div class="t m0 x33 h3 ya1 ff2 fs1 fc0 sc0 ls0 ws0">´</div><div class="t m0 x33 h3 ya1 ff2 fs1 fc0 sc0 ls0 ws0">y. Empirical<span class="_ _6"> </span>Ev<span class="_ _1"></span>aluation and Com-</div><div class="t m0 x1 h3 y5a ff2 fs1 fc0 sc0 ls0 ws0">bination of Advanced Language Modeling T<span class="_ _3"></span>echniques, In:<span class="_ _6"> </span>Proceedings of Interspeech, 2011.</div><div class="t m0 x13 h9 yf1 ff2 fs5 fc0 sc0 ls0 ws0">4</div><div class="t m0 x14 ha y26 ff2 fs6 fc0 sc0 ls0 ws0">The code is av<span class="_ _1"></span>ailable at <span class="ff3">https://code.google.com/p/word2vec/</span></div><div class="t m0 x3b h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">11</div><a class="l" href="#pfb" data-dest-detail='[11,"XYZ",124.139,72.101,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(255,0,0);position:absolute;left:137.487000px;bottom:650.713000px;width:4.978000px;height:11.521000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="#pfc" data-dest-detail='[12,"XYZ",108,624.32,null]'><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,0);position:absolute;left:396.703000px;bottom:619.993000px;width:10.956000px;height:7.787000px;background-color:rgba(255,255,255,0.000001);"></div></a><a class="l" href="https://code.google.com/p/word2vec/"><div class="d m2" style="border-width:1.000000px;border-style:solid;border-color:rgb(0,255,255);position:absolute;left:209.141000px;bottom:57.653000px;width:189.286000px;height:11.867000px;background-color:rgba(255,255,255,0.000001);"></div></a></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
<div id="pfc" class="pf w0 h0" data-page-no="c"><div class="pc pcc w0 h0"><div class="t m0 x0 h3 y2a ff2 fs1 fc0 sc0 ls0 ws0">[18]<span class="_ _9"> </span>T<span class="_ _3"></span>. Mikolov<span class="_ _3"></span>,<span class="_ _7"> </span>A. Deoras,<span class="_ _6"> </span>D.<span class="_ _6"> </span>Pove<span class="_ _1"></span>y<span class="_ _1"></span>, L.<span class="_ _6"> </span>Burget, J.</div><div class="t m0 x44 h3 y142 ff2 fs1 fc0 sc0 ls0 ws0">ˇ</div><div class="t m0 x34 h3 y2a ff2 fs1 fc0 sc0 ls0 ws0">Cernock</div><div class="t m0 x3d h3 y143 ff2 fs1 fc0 sc0 ls0 ws0">´</div><div class="t m0 x3d h3 y2a ff2 fs1 fc0 sc0 ls0 ws0">y.<span class="_ _6"> </span>Strategies for<span class="_ _6"> </span>T<span class="_ _1"></span>raining<span class="_ _6"> </span>Large Scale</div><div class="t m0 x1 h3 y2b ff2 fs1 fc0 sc0 ls0 ws0">Neural Network Language Models, In:<span class="_ _7"> </span>Proc. Automatic Speech Recognition and Understand-</div><div class="t m0 x1 h3 y144 ff2 fs1 fc0 sc0 ls0 ws0">ing, 2011.</div><div class="t m0 x0 h3 y145 ff2 fs1 fc0 sc0 ls0 ws0">[19]<span class="_ _9"> </span>T<span class="_ _3"></span>. Mik<span class="_ _1"></span>olov<span class="_ _3"></span>. Statistical Language<span class="_ _4"> </span>Models based<span class="_ _4"> </span>on Neural<span class="_ _4"> </span>Networks.<span class="_ _4"> </span>PhD thesis,<span class="_ _4"> </span>Brno Uni<span class="_ _1"></span>ver<span class="_ _1"></span>-</div><div class="t m0 x1 h3 y146 ff2 fs1 fc0 sc0 ls0 ws0">sity of T<span class="_ _3"></span>echnology<span class="_ _1"></span>, 2012.</div><div class="t m0 x0 h3 yd0 ff2 fs1 fc0 sc0 ls0 ws0">[20]<span class="_ _9"> </span>T<span class="_ _3"></span>. Mikolo<span class="_ _1"></span>v<span class="_ _3"></span>, W<span class="_ _3"></span>.T<span class="_ _1"></span>.<span class="_ _4"> </span>Y<span class="_ _1"></span>ih,<span class="_ _4"> </span>G. Zweig. Linguistic Re<span class="_ _1"></span>gularities in Continuous<span class="_ _4"> </span>Space W<span class="_ _3"></span>ord Represen-</div><div class="t m0 x1 h3 y147 ff2 fs1 fc0 sc0 ls0 ws0">tations. N<span class="_ _1"></span>AA<span class="_ _1"></span>CL HL<span class="_ _0"></span>T 2013.</div><div class="t m0 x0 h3 y148 ff2 fs1 fc0 sc0 ls0 ws0">[21]<span class="_ _9"> </span>T<span class="_ _3"></span>.<span class="_ _7"> </span>Mikolov<span class="_ _3"></span>,<span class="_ _2"> </span>I.<span class="_ _7"> </span>Sutske<span class="_ _1"></span>ver<span class="_ _1"></span>,<span class="_ _7"> </span>K.<span class="_ _2"> </span>Chen,<span class="_ _7"> </span>G.<span class="_ _2"> </span>Corrado,<span class="_ _7"> </span>and<span class="_ _7"> </span>J.<span class="_ _2"> </span>Dean.<span class="_ _7"> </span>Distrib<span class="_ _1"></span>uted<span class="_ _7"> </span>Representations<span class="_ _2"> </span>of</div><div class="t m0 x1 h3 y32 ff2 fs1 fc0 sc0 ls0 ws0">W<span class="_ _3"></span>ords and Phrases and their Compositionality<span class="_ _1"></span>. Accepted to NIPS 2013.</div><div class="t m0 x0 h3 y63 ff2 fs1 fc0 sc0 ls0 ws0">[22]<span class="_ _9"> </span>A.<span class="_ _6"> </span>Mnih,<span class="_ _2"> </span>G.<span class="_ _7"> </span>Hinton.<span class="_ _7"> </span>Three<span class="_ _7"> </span>new<span class="_ _7"> </span>graphical<span class="_ _7"> </span>models<span class="_ _7"> </span>for<span class="_ _7"> </span>statistical<span class="_ _7"> </span>language<span class="_ _2"> </span>modelling.<span class="_ _6"> </span>ICML,</div><div class="t m0 x1 h3 y64 ff2 fs1 fc0 sc0 ls0 ws0">2007.</div><div class="t m0 x0 h3 y149 ff2 fs1 fc0 sc0 ls0 ws0">[23]<span class="_ _9"> </span>A.<span class="_ _5"> </span>Mnih, G.<span class="_ _4"> </span>Hinton.<span class="_ _5"> </span>A<span class="_ _4"> </span>Scalable<span class="_ _4"> </span>Hierarchical<span class="_ _5"> </span>Distributed<span class="_ _4"> </span>Language<span class="_ _5"> </span>Model. Adv<span class="_ _1"></span>ances<span class="_ _4"> </span>in<span class="_ _5"> </span>Neural</div><div class="t m0 x1 h3 y11f ff2 fs1 fc0 sc0 ls0 ws0">Information Processing Systems 21, MIT Press, 2009.</div><div class="t m0 x0 h3 yc1 ff2 fs1 fc0 sc0 ls0 ws0">[24]<span class="_ _9"> </span>A.<span class="_ _2"> </span>Mnih,<span class="_ _2"> </span>Y<span class="_ _0"></span>.W<span class="_ _3"></span>.<span class="_ _2"> </span>T<span class="_ _3"></span>eh.<span class="_ _2"> </span>A<span class="_ _2"> </span>fast<span class="_ _2"> </span>and<span class="_ _7"> </span>simple<span class="_ _2"> </span>algorithm<span class="_ _2"> </span>for<span class="_ _2"> </span>training<span class="_ _2"> </span>neural<span class="_ _2"> </span>probabilistic<span class="_ _2"> </span>language</div><div class="t m0 x1 h3 y107 ff2 fs1 fc0 sc0 ls0 ws0">models. ICML, 2012.</div><div class="t m0 x0 h3 y14a ff2 fs1 fc0 sc0 ls0 ws0">[25]<span class="_ _9"> </span>F<span class="_ _0"></span>.<span class="_ _7"> </span>Morin,<span class="_ _6"> </span>Y<span class="_ _0"></span>.<span class="_ _6"> </span>Bengio.<span class="_ _7"> </span>Hierarchical<span class="_ _6"> </span>Probabilistic<span class="_ _6"> </span>Neural<span class="_ _6"> </span>Network<span class="_ _6"> </span>Language<span class="_ _7"> </span>Model.<span class="_ _6"> </span>AIST<span class="_ _3"></span>A<span class="_ _0"></span>TS,</div><div class="t m0 x1 h3 y130 ff2 fs1 fc0 sc0 ls0 ws0">2005.</div><div class="t m0 x0 h3 y131 ff2 fs1 fc0 sc0 ls0 ws0">[26]<span class="_ _9"> </span>D.<span class="_ _2"> </span>E.<span class="_ _2"> </span>Rumelhart,<span class="_ _2"> </span>G.<span class="_ _2"> </span>E.<span class="_ _2"> </span>Hinton,<span class="_ _2"> </span>R.<span class="_ _2"> </span>J.<span class="_ _2"> </span>W<span class="_ _1"></span>illiams.<span class="_ _2"> </span>Learning<span class="_ _2"> </span>internal<span class="_ _2"> </span>representations<span class="_ _2"> </span>by<span class="_ _7"> </span>back-</div><div class="t m0 x1 h3 y132 ff2 fs1 fc0 sc0 ls0 ws0">propagating errors. Nature, 323:533.536, 1986.</div><div class="t m0 x0 h3 y3b ff2 fs1 fc0 sc0 ls0 ws0">[27]<span class="_ _9"> </span>H.<span class="_ _7"> </span>Schwenk.<span class="_ _7"> </span>Continuous<span class="_ _2"> </span>space<span class="_ _7"> </span>language<span class="_ _7"> </span>models.<span class="_ _2"> </span>Computer<span class="_ _7"> </span>Speech<span class="_ _7"> </span>and<span class="_ _2"> </span>Language,<span class="_ _7"> </span>vol.<span class="_ _2"> </span>21,</div><div class="t m0 x1 h3 y3c ff2 fs1 fc0 sc0 ls0 ws0">2007.</div><div class="t m0 x0 h3 y14b ff2 fs1 fc0 sc0 ls0 ws0">[28]<span class="_ _9"> </span>R.<span class="_ _7"> </span>Socher<span class="_ _1"></span>,<span class="_ _2"> </span>E.H.<span class="_ _2"> </span>Huang,<span class="_ _2"> </span>J.<span class="_ _7"> </span>Pennington,<span class="_ _2"> </span>A.Y<span class="_ _0"></span>.<span class="_ _2"> </span>Ng,<span class="_ _2"> </span>and<span class="_ _7"> </span>C.D.<span class="_ _2"> </span>Manning.<span class="_ _7"> </span>Dynamic<span class="_ _2"> </span>Pooling<span class="_ _7"> </span>and</div><div class="t m0 x1 h3 y14c ff2 fs1 fc0 sc0 ls0 ws0">Unfolding Recursiv<span class="_ _1"></span>e Autoencoders for Paraphrase Detection. In NIPS, 2011.</div><div class="t m0 x0 h3 y10e ff2 fs1 fc0 sc0 ls0 ws0">[29]<span class="_ _9"> </span>J.<span class="_ _7"> </span>T<span class="_ _1"></span>urian,<span class="_ _7"> </span>L.<span class="_ _7"> </span>Ratinov<span class="_ _1"></span>,<span class="_ _7"> </span>Y<span class="_ _0"></span>.<span class="_ _2"> </span>Bengio.<span class="_ _6"> </span>W<span class="_ _1"></span>ord<span class="_ _6"> </span>Representations:<span class="_ _9"> </span>A<span class="_ _7"> </span>Simple<span class="_ _7"> </span>and<span class="_ _7"> </span>General<span class="_ _2"> </span>Method<span class="_ _6"> </span>for</div><div class="t m0 x1 h3 y10f ff2 fs1 fc0 sc0 ls0 ws0">Semi-Supervised Learning. In:<span class="_ _6"> </span>Proc. Association for Computational Linguistics, 2010.</div><div class="t m0 x0 h3 y93 ff2 fs1 fc0 sc0 ls0 ws0">[30]<span class="_ _9"> </span>P<span class="_ _0"></span>. D. T<span class="_ _1"></span>urne<span class="_ _1"></span>y<span class="_ _1"></span>. Measuring Semantic Similarity by Latent Relational Analysis. In:<span class="_ _7"> </span>Proc. Interna-</div><div class="t m0 x1 h3 y6e ff2 fs1 fc0 sc0 ls0 ws0">tional Joint Conference on Artiﬁcial Intelligence, 2005.</div><div class="t m0 x0 h3 y14d ff2 fs1 fc0 sc0 ls0 ws0">[31]<span class="_ _9"> </span>A.<span class="_ _6"> </span>Zhila,<span class="_ _2"> </span>W<span class="_ _0"></span>.T<span class="_ _1"></span>.<span class="_ _6"> </span>Y<span class="_ _1"></span>ih,<span class="_ _2"> </span>C.<span class="_ _6"> </span>Meek,<span class="_ _2"> </span>G.<span class="_ _6"> </span>Zweig,<span class="_ _2"> </span>T<span class="_ _3"></span>.<span class="_ _7"> </span>Mikolov<span class="_ _3"></span>.<span class="_ _7"> </span>Combining<span class="_ _7"> </span>Heterogeneous<span class="_ _2"> </span>Models<span class="_ _6"> </span>for</div><div class="t m0 x1 h3 y14e ff2 fs1 fc0 sc0 ls0 ws0">Measuring Relational Similarity<span class="_ _1"></span>.<span class="_ _4"> </span>N<span class="_ _1"></span>AA<span class="_ _1"></span>CL HL<span class="_ _0"></span>T 2013.</div><div class="t m0 x0 h3 y14f ff2 fs1 fc0 sc0 ls0 ws0">[32]<span class="_ _9"> </span>G. Zweig, C.J.C. Bur<span class="_ _1"></span>ges. The Microsoft Research Sentence Completion Challenge, Microsoft</div><div class="t m0 x1 h3 y150 ff2 fs1 fc0 sc0 ls0 ws0">Research T<span class="_ _3"></span>echnical Report MSR-TR-2011-129, 2011.</div><div class="t m0 x3b h3 y27 ff2 fs1 fc0 sc0 ls0 ws0">12</div></div><div class="pi" data-data='{"ctm":[1.000000,0.000000,0.000000,1.000000,0.000000,0.000000]}'></div></div>
</div>
<div class="loading-indicator">
<img alt="" src="pdf2htmlEX-64x64.png">
</div>
</body>
</html>
